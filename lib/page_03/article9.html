<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Генеральный директор Anthropic о Claude, AGI и будущем искусственного интеллекта и человечества - Интервью и выступления</title>

	<link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96" />
	<link rel="icon" type="image/svg+xml" href="/favicon.svg" />
	<link rel="shortcut icon" href="/favicon.ico" />
	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
	<link rel="manifest" href="/site.webmanifest" />
	
	<meta name="description" content="Генеральный директор Anthropic о Claude, AGI и будущем искусственного интеллекта и человечества - Интервью с известными людьми, интересные выступления и обсуждения.">
	
    <link rel="stylesheet" href="../css/page.css">
</head>
<body>
    <!-- Шапка сайта -->
    <header>
        <div class="container header-content">
            <a href="../../index.html" class="logo">Интервью и выступления</a>
            <a href="../page_00/about.html" class="about-link">О сайте</a>
        </div>
    </header>
    
    <!-- Основной контент -->
    <div class="container">
        <div class="article-header">
            <h1 class="article-title">Генеральный директор Anthropic о Claude, AGI и будущем искусственного интеллекта и человечества</h1>
        </div>
        
        <!-- Введение -->
		
        
        <!-- Оглавление -->
		
        <div class="toc-section">
            <div class="toc-header" id="tocHeader">
                <h2 class="toc-title">Содержание</h2>
                <button class="toc-toggle" id="tocToggle">
                    <span id="tocIcon">▼</span>
                    <span id="tocText">Показать</span>
                </button>
            </div>
            
            <div class="toc-content" id="tocContent">
                <ul class="toc-list">
				    
						                  
						<li><a href="#block-1">Введение</a></li>
						
					
						                  
						<li><a href="#block-2">Законы масштабирования</a></li>
						
					
						                  
						<li><a href="#block-3">Пределы масштабирования языковых моделей</a></li>
						
					
						                  
						<li><a href="#block-4">Конкуренция с OpenAI, Google, xAI, Meta</a></li>
						
					
						                  
						<li><a href="#block-5">Claude</a></li>
						
					
						                  
						<li><a href="#block-6">Opus 3.5</a></li>
						
					
						                  
						<li><a href="#block-7">Sonnet 3.5</a></li>
						
					
						                  
						<li><a href="#block-8">Claude 4.0</a></li>
						
					
						                  
						<li><a href="#block-9">Критика Claude</a></li>
						
					
						                  
						<li><a href="#block-10">Уровни безопасности ИИ</a></li>
						
					
						                  
						<li><a href="#block-11">ASL-3 и ASL-4</a></li>
						
					
						                  
						<li><a href="#block-12">Использование компьютера</a></li>
						
					
						                  
						<li><a href="#block-13">Государственное регулирование ИИ</a></li>
						
					
						                  
						<li><a href="#block-14">Создание сильной команды</a></li>
						
					
						                  
						<li><a href="#block-15">Машины любящей благодати</a></li>
						
					
						                  
						<li><a href="#block-16">Сроки достижения ОИИ</a></li>
						
					
						                  
						<li><a href="#block-17">Программирование</a></li>
						
					
						                  
						<li><a href="#block-18">Смысл жизни</a></li>
						
					
						                  
						<li><a href="#block-19">Аманда Аскелл</a></li>
						
					
						                  
						<li><a href="#block-20">Советы по программированию для нетехнических специалистов</a></li>
						
					
						                  
						<li><a href="#block-21">Разговор с Claude</a></li>
						
					
						                  
						<li><a href="#block-22">Промт-инжиниринг</a></li>
						
					
						                  
						<li><a href="#block-23">Системные промты</a></li>
						
					
						                  
						<li><a href="#block-24">Claude становится глупее?</a></li>
						
					
						                  
						<li><a href="#block-25">Обучение характера</a></li>
						
					
						                  
						<li><a href="#block-26">Природа истины</a></li>
						
					
						                  
						<li><a href="#block-27">Оптимальная частота неудач</a></li>
						
					
						                  
						<li><a href="#block-28">Сознание ИИ</a></li>
						
					
						                  
						<li><a href="#block-29">Искусственный общий интеллект (ИОИ)</a></li>
						
					
						                  
						<li><a href="#block-30">Особенности, схемы, универсальность</a></li>
						
					
						                  
						<li><a href="#block-31">Суперпозиция</a></li>
						
					
						                  
						<li><a href="#block-32">Моносемантичность</a></li>
						
					
						                  
						<li><a href="#block-33">Масштабирование моносемичности</a></li>
						
					
						                  
						<li><a href="#block-34">Макроскопическое поведение нейронных сетей</a></li>
						
					
						                  
						<li><a href="#block-35">Красота нейронных сетей</a></li>
						
					
                    
                </ul>
            </div>
        </div>
        
        <!-- Основное содержание -->
        <div class="article-content">
			
            <!-- Блок 1 -->
            <div class="article-block" id="block-1">
				
                 <div class="block-header">
                    <h2 class="block-title">Введение</h2>
                </div>
				
                <div class="block-text">
					
					
                    <img src="images/article9/p01.webp" alt="Введение - Генеральный директор Anthropic о Claude, AGI и будущем искусственного интеллекта и человечества" class="block-image right" loading="lazy">
					
					
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Если экстраполировать текущие тенденции, верно? Если сказать: «Ну, я не знаю, мы уже достигаем уровня PhD, в прошлом году были на уровне бакалавра, а годом ранее — на уровне школьника», — можно, конечно, спорить о конкретных задачах и критериях. «Нам ещё не хватает некоторых модальностей, но они постепенно добавляются», — например, добавилась работа с компьютером, генерация изображений. Если просто взглянуть на скорость роста этих возможностей, становится ясно, что мы достигнем цели к 2026 или 2027 году. Думаю, всё ещё есть сценарии, где этого не произойдёт и за 100 лет. Но таких сценариев становится всё меньше. Мы быстро исчерпываем по-настоящему убедительные препятствия, веские причины, почему этого не случится в ближайшие годы. Масштабирование происходит очень быстро. Сегодня мы создаём модель и разворачиваем тысячи, а может, и десятки тысяч её экземпляров. Думаю, через два-три года, независимо от того, появятся ли сверхмощные ИИ, кластеры достигнут такого размера, что можно будет развернуть миллионы таких моделей. Я оптимистичен насчёт смысла. Но меня беспокоят экономика и концентрация власти. Именно это волнует меня больше — злоупотребление властью.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>ИИ увеличивает количество власти в мире. И если сконцентрировать эту власть и злоупотребить ею, это может нанести неисчислимый ущерб.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, это очень пугает. Очень пугает.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Перед вами беседа с Дарио Амодеи, CEO Anthropic, компании, создавшей Claude, который часто занимает верхние строчки в рейтингах больших языковых моделей. Кроме того, Дарио и команда Anthropic активно выступают за серьёзное отношение к безопасности ИИ. Они продолжают публиковать множество fascinating исследований на эту и другие темы. Позже ко мне присоединились ещё два выдающихся сотрудника Anthropic. Первая — Аманда Аскелл, исследователь, работающая над alignment и тонкой настройкой Claude, включая разработку его характера и личности. Некоторые говорили мне, что она, вероятно, общалась с Claude больше, чем любой другой человек в Anthropic. Она определённо была fascinating собеседницей в вопросах prompt engineering и практических советов, как получить от Claude максимум. Затем зашёл поболтать Крис Олах. Он один из пионеров в области mechanistic interpretability — захватывающего направления, которое занимается reverse engineering нейронных сетей, чтобы понять, что происходит внутри, выводя поведение из паттернов активации нейронов. Это очень перспективный подход для обеспечения безопасности будущих сверхразумных ИИ. Например, для обнаружения моментов, когда модель пытается обмануть собеседника-человека. Это подкаст Лекса Фридмана. Чтобы поддержать его, ознакомьтесь с нашими спонсорами в описании. А теперь, дорогие друзья, встречайте — Дарио Амодеи.</p>
					
                </div>
            </div>
			
            <!-- Блок 2 -->
            <div class="article-block" id="block-2">
				
                 <div class="block-header">
                    <h2 class="block-title">Законы масштабирования</h2>
                </div>
				
                <div class="block-text">
					
					
                    <img src="images/article9/p02.webp" alt="Законы масштабирования - Генеральный директор Anthropic о Claude, AGI и будущем искусственного интеллекта и человечества" class="block-image" loading="lazy">
					
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Давайте начнём с большой идеи — законов масштабирования и гипотезы масштабирования. Что это? Какова их история, и где мы находимся сегодня?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Я могу описать это только через призму своего опыта, но я работаю в области ИИ около 10 лет и заметил это очень рано. Я впервые погрузился в мир ИИ, когда работал в Baidu с Эндрю Ыном в конце 2014 года — почти ровно 10 лет назад. Первое, над чем мы работали, — системы распознавания речи. В те дни deep learning был чем-то новым. Он добился большого прогресса, но все говорили: «Нам не хватает алгоритмов для успеха. Мы используем лишь крошечную часть возможностей. Нужно ещё многое открыть алгоритмически. Мы ещё не поняли, как повторить работу человеческого мозга». В каком-то смысле мне повезло, это было похоже на удачу новичка. Я был новичком в этой области. Я посмотрел на recurrent neural networks, которые мы использовали для речи, и подумал: «А что, если сделать их больше и добавить больше слоёв? И масштабировать данные вместе с этим?» Я видел в этом независимые параметры, которые можно настраивать. И заметил, что модели становились лучше с увеличением данных, размера моделей и времени обучения. Тогда я не измерял всё точно, но вместе с коллегами мы явно почувствовали: чем больше данных, вычислений и обучения, тем лучше работают модели. Сначала я думал: «Может, это верно только для распознавания речи. Может, это частный случай». Полагаю, только в 2017 году, когда я увидел результаты GPT-1, до меня дошло, что язык, вероятно, — та область, где это работает. Мы можем получить триллионы слов языковых данных и обучаться на них. Модели тогда были крошечными — их можно было обучать на одном-восьми GPU, а сейчас мы обучаем задачи на десятках тысяч GPU, скоро — на сотнях тысяч. Когда я соединил эти два наблюдения, оказалось, что несколько людей, например Илья Суцкевер, которого вы интервьюировали, придерживались схожих взглядов. Возможно, он был первым, хотя несколько человек пришли к этому примерно одновременно. Было эссе Рича Саттона «Горький урок», Gwern писал о гипотезе масштабирования. Но где-то между 2014 и 2017 годами я окончательно убедился: «Мы сможем решать невероятно сложные когнитивные задачи, просто масштабируя модели». На каждом этапе масштабирования всегда находятся аргументы против. Когда я впервые их услышал, честно говоря, подумал: «Наверное, я ошибаюсь, а все эксперты правы. Они разбираются лучше». Есть аргумент Хомского: «Можно получить синтаксис, но не семантику». Была идея: «Можно сделать осмысленным предложение, но не абзац». Сегодня популярно: «Мы исчерпаем данные, или данные недостаточно качественны, или модели не могут рассуждать». И каждый раз мы находим обходной путь, или масштабирование само становится решением. Иногда одно, иногда другое. Сейчас я всё ещё считаю, что всё очень неопределённо. У нас есть только индуктивные выводы, чтобы предположить, что следующие два года будут похожи на последние 10. Но я видел этот фильм столько раз, что действительно верю: масштабирование, вероятно, продолжится, и в нём есть какая-то магия, которую мы пока не можем объяснить теоретически.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>И под масштабированием подразумеваются более крупные сети, больше данных, больше вычислительных ресурсов?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Всё это?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>В частности, линейное масштабирование более крупных сетей, увеличение времени обучения и всё больше данных. Это похоже на химическую реакцию: у вас есть три компонента, и вам нужно линейно увеличивать все три. Если увеличить только один, а другие оставить без изменения, реакция остановится из-за нехватки реагентов. Но если масштабировать всё последовательно, реакция продолжится.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>И конечно, теперь, когда у нас есть эта эмпирическая наука/искусство, её можно применить к более тонким вещам, например, к законам масштабирования в интерпретируемости или пост-тренинге. Или просто наблюдать, как это масштабируется. Но главный закон масштабирования, или гипотеза масштабирования, связан с тем, что большие сети и большие данные ведут к интеллекту?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, мы задокументировали законы масштабирования во многих областях, помимо языка. Первая работа, где мы это показали, вышла в начале 2020 года и касалась языка. Позже, в конце 2020 года, мы показали то же самое для других модальностей: изображений, видео, текста в изображения, изображений в текст, математики. Везде была одна и та же закономерность. И вы правы, теперь есть новые этапы, например, пост-тренинг или новые типы моделей рассуждений. Во всех этих случаях, которые мы измерили, мы видим схожие законы масштабирования.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Философский вопрос: почему, по вашему мнению, больше — значит лучше, когда речь идёт о размере сети и объёме данных? Почему это делает модели умнее?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>В моей предыдущей карьере биофизика… Я изучал физику в бакалавриате, а затем биофизику в аспирантуре. Я вспоминаю свои знания как физика, хотя они гораздо скромнее, чем у некоторых моих коллег в Anthropic. Есть концепция, называемая шумом 1/f и распределениями 1/X. Если сложить множество естественных процессов, получится гауссово распределение. Но если сложить процессы с разными распределениями… Например, если подключить датчик к резистору, распределение теплового шума в резисторе будет пропорционально 1/f. Это своего рода естественное сходящееся распределение. Я думаю, суть в том, что если взглянуть на множество вещей, созданных естественными процессами с разными масштабами, это не гауссово распределение, которое узко сконцентрировано, а скорее распределение 1/X. Например, в языке есть простые паттерны: некоторые слова встречаются чаще других, как артикль «the». Есть базовая структура «существительное-глагол», согласование между ними, структура предложения, тематическая структура абзацев. Эта иерархия паттернов такова, что по мере увеличения сети сначала улавливаются простые корреляции, а затем — длинный хвост более сложных. Если этот длинный хвост паттернов гладкий, как шум 1/f в физических процессах, то по мере увеличения сети она захватывает всё больше этого распределения. Эта гладкость отражается на способности моделей предсказывать и их общей производительности. Язык — это продукт эволюции. У нас есть часто употребляемые слова и редкие, распространённые выражения и уникальные, идеи-клише и новые идеи. Этот процесс развивался вместе с человечеством миллионы лет. И моё предположение, чисто спекулятивное, заключается в том, что распределение этих идей имеет длинный хвост.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>То есть есть длинный хвост, но также есть иерархия концепций, которые выстраиваются. Чем больше сеть, тем выше её способность…</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Точно. Маленькая сеть улавливает только самое простое. Например, крошечная нейросеть понимает, что в предложении должен быть глагол, прилагательное и существительное, но не может правильно их подобрать. Если немного увеличить сеть, она справляется с этим, но не может связать предложения в абзацы. Более редкие и сложные паттерны захватываются по мере роста ёмкости сети.</p>
					
                </div>
            </div>
			
            <!-- Блок 3 -->
            <div class="article-block" id="block-3">
				
                 <div class="block-header">
                    <h2 class="block-title">Пределы масштабирования языковых моделей</h2>
                </div>
				
                <div class="block-text">
					
					
                    <img src="images/article9/p03.webp" alt="Пределы масштабирования языковых моделей - Генеральный директор Anthropic о Claude, AGI и будущем искусственного интеллекта и человечества" class="block-image right" loading="lazy">
					
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Тогда естественный вопрос: где предел?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Насколько сложен реальный мир? Сколько в нём всего можно изучить?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Думаю, никто из нас не знает ответа. Моё сильное убеждение — предела нет, по крайней мере, до уровня человека. Люди способны понимать эти паттерны, и я считаю, что если продолжать масштабировать модели, разрабатывать новые методы их обучения, они как минимум достигнут человеческого уровня. Дальше вопрос: насколько можно превзойти человека? Насколько можно быть умнее и восприимчивее? Думаю, ответ зависит от области. Если взять биологию, как я писал в эссе «Machines of Loving Grace», люди с трудом понимают её сложность. В Стэнфорде, Гарварде или Беркли целые отделы изучают иммунную систему или метаболические пути, и каждый специалист понимает лишь крошечную часть. Им сложно объединить свои знания. Поэтому я думаю, что у ИИ есть огромный потенциал для роста. Если говорить о материалах, разрешении конфликтов между людьми, возможно, здесь пределы ближе к человеческим возможностям. Как в распознавании речи: есть предел чёткости восприятия. В одних областях потолок близок, в других — очень далёк. Узнаем только когда построим такие системы. Заранее нельзя быть уверенным. Можно лишь строить гипотезы.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>А в некоторых областях потолок может быть связан с человеческой бюрократией, как вы писали.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Таким образом, люди принципиально должны быть частью цикла. Это причина потолка, а не, возможно, пределы интеллекта.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, я думаю, что во многих случаях, в теории, технологии могут меняться очень быстро. Например, все, что мы можем изобрести в области биологии, но помните, что существует система клинических испытаний, через которую мы должны пройти, чтобы применять эти вещи на людях. Я думаю, это смесь излишней бюрократии и вещей, которые защищают целостность общества. И вся сложность в том, что трудно понять, что к чему. Что касается разработки лекарств, я считаю, что мы движемся слишком медленно и слишком консервативно. Но, конечно, если ошибиться, можно рисковать жизнями людей, действуя слишком опрометчиво. Поэтому по крайней мере некоторые из этих человеческих институтов действительно защищают людей. Всё дело в поиске баланса. Я сильно подозреваю, что баланс скорее на стороне желания ускорить процессы, но баланс есть.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Если мы действительно столкнемся с ограничением, если замедлимся в законах масштабирования, как вы думаете, в чем будет причина? В ограничении вычислительных мощностей, данных? В чем-то еще? В ограничении идей?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Несколько моментов: сейчас мы говорим о достижении предела до уровня навыков людей. Одно из популярных сегодня предположений, и я думаю, это может быть реальным ограничением, — это то, что мы просто исчерпаем данные. В интернете ограниченное количество данных, и есть проблемы с их качеством. Можно получить сотни триллионов слов из интернета, но многое из этого — повторяющийся контент, тексты для SEO или, возможно, в будущем даже тексты, сгенерированные самим ИИ. Поэтому есть пределы того, что можно создать таким образом. Тем не менее, мы, и, я полагаю, другие компании, работаем над способами создания синтетических данных, где модель может генерировать больше данных того типа, который у нас уже есть, или даже создавать данные с нуля. Если взглянуть на то, что было сделано с AlphaGo Zero от DeepMind, они смогли создать бота, который от полного неумения играть в го достиг уровня выше человеческого, просто играя против себя. В версии AlphaGo Zero не требовались примеры данных от людей. Другое направление — это модели рассуждений, которые используют цепочку мыслей, останавливаются, чтобы подумать, и анализируют собственные размышления. В каком-то смысле это еще один вид синтетических данных в сочетании с обучением с подкреплением. Я предполагаю, что с одним из этих методов мы преодолеем ограничение данных, или могут появиться другие источники данных. Мы можем просто наблюдать, что даже если нет проблем с данными, по мере масштабирования моделей они перестают улучшаться. Казалось, что улучшение было стабильным, но в какой-то момент оно может остановиться по неизвестной нам причине. Ответ может заключаться в том, что нам нужно изобрести новую архитектуру. В прошлом уже были проблемы, например, с численной стабильностью моделей, когда казалось, что прогресс остановился, но когда мы нашли правильное решение, это оказалось не так. Возможно, нам нужен новый метод оптимизации или новая техника, чтобы преодолеть ограничения. Пока я не вижу доказательств этого, но если прогресс замедлится, это может быть одной из причин.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>А как насчет ограничений вычислительных мощностей, то есть дороговизны строительства все более крупных дата-центров?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Сейчас большинство передовых компаний, работающих с моделями, оперируют масштабами примерно в 1 миллиард, плюс-минус в три раза. Это модели, которые существуют сейчас или обучаются. Я думаю, в следующем году мы перейдем к нескольким миллиардам, а к 2026 году — к более чем 10 миллиардам. И, вероятно, к 2027 году появятся амбиции построить кластеры стоимостью в сто миллиардов. Я думаю, все это действительно произойдет. Есть огромная решимость создать вычислительные мощности и сделать это в этой стране, и я полагаю, что это случится. Если мы достигнем ста миллиардов, но этого все равно будет недостаточно, то либо нам потребуется еще больший масштаб, либо нужно найти способ делать это эффективнее, сместив кривую. Одна из причин, почему я оптимистично настроен насчет быстрого развития мощного ИИ, заключается в том, что если экстраполировать следующие точки на кривой, мы очень быстро приближаемся к уровню человеческих способностей. Некоторые из новых моделей, которые мы разработали, а также модели рассуждений от других компаний, начинают достигать того, что я бы назвал уровнем PhD или профессионала. Например, если взглянуть на их способности в программировании, наша последняя модель, Sonnet 3.5, обновленная версия, набирает около 50% на SWE-bench. А SWE-bench — это набор реальных профессиональных задач по разработке программного обеспечения. В начале года, я думаю, лучший результат был 3-4%. Так что за 10 месяцев мы прошли путь от 3% до 50%. И, вероятно, через год достигнем 90%. Может быть, даже раньше. Мы видели подобные результаты в выпускных уровнях математики, физики и биологии у моделей, таких как OpenAI o1. Если просто продолжить экстраполировать это с точки зрения навыков, то, следуя этой кривой, через несколько лет мы получим модели, превосходящие уровень лучших профессионалов среди людей. Продолжится ли эта кривая? Вы и я указали на множество возможных причин, почему этого может не произойти. Но если экстраполяция продолжится, это тот путь, по которому мы идем.</p>
					
                </div>
            </div>
			
            <!-- Блок 4 -->
            <div class="article-block" id="block-4">
				
                 <div class="block-header">
                    <h2 class="block-title">Конкуренция с OpenAI, Google, xAI, Meta</h2>
                </div>
				
                <div class="block-text">
					
					
                    <img src="images/article9/p04.webp" alt="Конкуренция с OpenAI, Google, xAI, Meta - Генеральный директор Anthropic о Claude, AGI и будущем искусственного интеллекта и человечества" class="block-image" loading="lazy">
					
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>У Anthropic есть несколько конкурентов. Было бы интересно услышать ваш взгляд на это. OpenAI, Google, XAI, Meta. Что нужно, чтобы победить в широком смысле этого слова в этой сфере?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, я хочу разделить несколько моментов. Миссия Anthropic — попытаться сделать так, чтобы всё шло хорошо. У нас есть теория изменений под названием «Гонка к вершине». Она заключается в том, чтобы подталкивать других игроков к правильным действиям, подавая пример. Речь не о том, чтобы быть «хорошими парнями», а о том, чтобы создать условия, при которых все мы можем ими быть. Приведу несколько примеров. В начале истории Anthropic один из наших сооснователей, Крис Ола, которого, я думаю, вы скоро будете интервьюировать, он же сооснователь направления механистической интерпретируемости — попытки понять, что происходит внутри ИИ-моделей. Мы поручили ему и одной из наших первых команд сосредоточиться на этой области, которая, как мы считаем, делает модели безопаснее и прозрачнее. Три или четыре года у этого не было никакого коммерческого применения. Да и сейчас его нет. Сегодня мы проводим первые бета-тесты, и, вероятно, со временем оно появится, но это очень долгий исследовательский путь, в рамках которого мы работали открыто и делились результатами. Мы сделали это, потому что считаем, что это делает модели безопаснее. Интересно, что по мере нашего продвижения другие компании тоже начали этим заниматься. В одних случаях потому, что их это вдохновило, в других — потому, что они беспокоились, что если другие компании выглядят ответственнее, им тоже нужно так выглядеть. Никто не хочет казаться безответственным. И они тоже это перенимают. Когда люди приходят в Anthropic, интерпретируемость часто становится аргументом, и я говорю им: «Расскажите тем, кого вы не выбрали, почему пришли сюда». И вскоре вы видите, что в других местах тоже появляются команды по интерпретируемости. В каком-то смысле это лишает нас конкурентного преимущества, потому что теперь другие тоже этим занимаются. Но это хорошо для системы в целом, поэтому нам приходится изобретать что-то новое, чего другие пока не делают. Идея в том, чтобы повысить значимость правильных действий. Речь не только о нас. Не о том, чтобы быть единственным «хорошим парнем». Другие компании тоже могут этим заниматься. Если они присоединятся к этой гонке, это будет лучшая новость. Важно направить стимулы вверх, а не вниз.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>И стоит отметить, что пример с механистической интерпретируемостью — это строгий, не поверхностный подход к безопасности ИИ…</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>…или он движется в этом направлении.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Мы стараемся. Мы ещё в начале пути с точки зрения нашей способности что-то разглядеть, но я удивлён тем, как много нам удаётся увидеть внутри этих систем и понять. В отличие от законов масштабирования, где кажется, что есть некий закон, заставляющий модели работать лучше, внутри они не… Нет причины, по которой они должны быть созданы для нашего понимания, верно? Они созданы для работы. Как человеческий мозг или биохимия. Они не созданы для того, чтобы человек открыл люк, заглянул внутрь и понял их. Но мы обнаружили — и вы можете обсудить это подробнее с Крисом — что когда мы заглядываем внутрь, мы находим удивительно интересные вещи.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>И как побочный эффект, вы также видите красоту этих моделей. Вы исследуете прекрасную природу больших нейронных сетей через методологию MEC и TERP.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Меня поражает, насколько всё чётко. Меня поражают, например, индукционные головы. Меня поражает, что мы можем использовать разреженные автоэнкодеры для поиска направлений внутри сетей, и что эти направления соответствуют очень ясным концептам. Мы немного продемонстрировали это с «Golden Gate Bridge Claude». Это был эксперимент, где мы нашли направление в одном из слоёв нейронной сети, соответствующее мосту Золотые Ворота. И мы просто усилили его. Мы выпустили эту модель как демо, это было вроде как шутка на пару дней, но это иллюстрировало наш метод. Вы могли спросить модель о чём угодно. Например: «Как прошёл твой день?» И из-за активации этой фичи она связывала ответ с мостом Золотые Ворота. Она могла сказать: «Я чувствую себя расслабленно и свободно, как арки моста Золотые Ворота», или…</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Она мастерски переводила тему на мост Золотые Ворота и интегрировала его. В этом фокусе на мост была даже какая-то грусть. Думаю, люди быстро его полюбили. Уже скучают по нему, потому что его убрали через день.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Как-то так вышло, что эти вмешательства в модель, где вы корректируете её поведение, эмоционально делали её более человечной, чем любая другая версия.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>У неё сильная личность, яркая идентичность.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>У неё сильная личность. Эти навязчивые интересы… Мы все можем вспомнить кого-то, кто чем-то одержим. Это действительно делает её чуть более человечной.</p>
					
                </div>
            </div>
			
            <!-- Блок 5 -->
            <div class="article-block" id="block-5">
				
                 <div class="block-header">
                    <h2 class="block-title">Claude</h2>
                </div>
				
                <div class="block-text">
					
					
                    <img src="images/article9/p05.webp" alt="Claude - Генеральный директор Anthropic о Claude, AGI и будущем искусственного интеллекта и человечества" class="block-image right" loading="lazy">
					
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Давайте поговорим о настоящем. Давайте поговорим о Claude. В этом году многое произошло. В марте вышли Claude 3 Opus, Sonnet и Haiku. Затем в июле — Claude 3.5 Sonnet, с обновлённой версией, выпущенной совсем недавно. А потом вышел и Claude 3.5 Haiku. Хорошо. Можешь объяснить разницу между Opus, Sonnet и Haiku и как нам следует воспринимать разные версии?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, давай вернёмся к марту, когда мы впервые выпустили эти три модели. Наша идея заключалась в том, что разные компании создают большие и маленькие модели, лучшие и худшие. Мы почувствовали, что есть спрос как на очень мощную модель, которая может быть немного медленнее и дороже, так и на быстрые и дешёвые модели, которые настолько умны, насколько это возможно при их скорости и стоимости. Когда нужно выполнить сложный анализ, например, написать код, придумать идеи или заняться творческим письмом, хочется использовать самую мощную модель. Но есть много практических применений в бизнесе: взаимодействие с сайтом, заполнение налогов или общение с юристом для анализа контракта. У нас много компаний, которые просто хотят, например, автодополнение в IDE. Для всего этого нужна скорость и широкое использование модели. Мы хотели охватить весь спектр потребностей. Так у нас появилась эта поэтическая тема. Что такое очень короткое стихотворение? Хайку. Haiku — это маленькая, быстрая и дешёвая модель, которая в то время была удивительно умной для своей скорости и стоимости. Sonnet — это стихотворение среднего размера, пара абзацев. Sonnet — средняя модель. Она умнее, но немного медленнее и дороже. А Opus, как Magnum Opus — большое произведение, Opus была самой большой и умной моделью на тот момент. Такова была изначальная идея. Мы думали: «Каждое новое поколение моделей должно смещать кривую компромиссов». Когда мы выпустили Sonnet 3.5, он сохранил примерно те же стоимость и скорость, что и Sonnet 3, но стал умнее оригинального Opus 3. Особенно в коде, но и в целом. Теперь у нас есть результаты для Haiku 3.5. Я считаю, что Haiku 3.5, самая маленькая новая модель, примерно так же хороша, как Opus 3, самая большая старая модель. Цель — сместить кривую, и в какой-то момент выйдет Opus 3.5. Каждое новое поколение моделей уникально. Они используют новые данные, их «личность» меняется, и мы пытаемся этим управлять, но не полностью. Нет точной эквивалентности, где меняется только интеллект. Мы всегда улучшаем и другие аспекты, а некоторые меняются без нашего ведома. Это неточная наука. Во многом, манера и личность этих моделей — скорее искусство, чем наука.</p>
					
                </div>
            </div>
			
            <!-- Блок 6 -->
            <div class="article-block" id="block-6">
				
                 <div class="block-header">
                    <h2 class="block-title">Opus 3.5</h2>
                </div>
				
                <div class="block-text">
					
					
                    <img src="images/article9/p06.webp" alt="Opus 3.5 - Генеральный директор Anthropic о Claude, AGI и будущем искусственного интеллекта и человечества" class="block-image" loading="lazy">
					
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>В чём причина промежутка между, скажем, Claude Opus 3.0 и 3.5? Что занимает это время, если можешь рассказать?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, есть разные этапы. Предварительное обучение — это обычное обучение языковой модели. Оно занимает много времени. Сейчас для этого используются десятки тысяч, иногда многие десятки тысяч GPU или TPU, или других ускорителей, и обучение длится месяцами. Затем идёт пост-обучение: обучение с подкреплением на основе обратной связи от людей и другие методы. Этот этап становится всё важнее, и он менее точен. Нужны усилия, чтобы всё сделать правильно. Модели тестируются ранними партнёрами, проверяются на безопасность, особенно на катастрофические и автономные риски. Мы проводим внутренние тесты согласно нашей политике ответственного масштабирования. У нас есть соглашения с Институтами безопасности ИИ США и Великобритании, а также с другими тестерами, которые проверяют модели на CBRN-риски (химические, биологические, радиологические и ядерные). Мы не считаем, что модели пока серьёзно угрожают здесь, но каждую новую модель мы оцениваем, чтобы не приблизиться к опасным возможностям. Ещё нужно время на доработку модели и запуск API. Процесс сложный, но мы стараемся его оптимизировать. Мы хотим, чтобы тестирование безопасности было строгим, но быстрым и автоматизированным, без потери качества. То же с пред- и пост-обучением. Это как строить самолёты: они должны быть безопасными, но процесс должен быть эффективным. Творческое напряжение между этими целями важно для работы моделей.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да, ходят слухи, что у Anthropic отличные инструменты. Наверное, много сложностей связано с разработкой ПО, чтобы эффективно взаимодействовать с инфраструктурой.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Вы удивитесь, насколько многое в создании моделей сводится к инженерии ПО и оптимизации. Со стороны кажется, что всё решают гениальные открытия, как в кино. Но даже великие открытия зависят от деталей, иногда очень скучных. Не могу сказать, что наши инструменты лучше, чем у других, но мы уделяем этому много внимания.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Не знаю, можешь ли сказать, но между Claude 3 и 3.5 было дополнительное пред-обучение или основное внимание уделялось пост-обучению? Произошёл большой скачок в производительности.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, на каждом этапе мы улучшаем всё сразу. Есть разные команды, каждая работает над своим участком эстафеты. Когда выходит новая модель, все улучшения объединяются.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Данные предпочтений от RLHF — можно ли их применять к новым моделям по мере их обучения?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, данные от старых моделей иногда используются, хотя лучше работают с новыми моделями. У нас есть метод конституционного ИИ, где модель обучается и на себе. Пост-обучение становится всё сложнее, это не только RLHF, но и другие методы.</p>
					
                </div>
            </div>
			
            <!-- Блок 7 -->
            <div class="article-block" id="block-7">
				
                 <div class="block-header">
                    <h2 class="block-title">Sonnet 3.5</h2>
                </div>
				
                <div class="block-text">
					
					
                    <img src="images/article9/p07.webp" alt="Sonnet 3.5 - Генеральный директор Anthropic о Claude, AGI и будущем искусственного интеллекта и человечества" class="block-image right" loading="lazy">
					
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Что объясняет такой значительный скачок в производительности новой модели Sonnet 3.5, по крайней мере, в части программирования? Возможно, это хороший момент, чтобы поговорить о бенчмарках. Что значит «стать лучше»? Просто цифры выросли, но я программирую, и мне это нравится, и я использую Claude 3.5 через Cursor для помощи в программировании. И, по крайней мере, на практике, субъективно, он стал умнее в этом. Так что нужно, чтобы сделать его ещё умнее?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Мы-</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Так что нужно, чтобы сделать его умнее?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Мы тоже это заметили. Кстати, у нас в Anthropic есть несколько очень сильных инженеров, для которых все предыдущие модели для работы с кодом, созданные нами и другими компаниями, не были полезны. Они говорили: «Может, это полезно для новичков, но не для меня». Но Sonnet 3.5, впервые, заставил их сказать: «О боже, это помогло мне с тем, на что у меня ушли бы часы. Это первая модель, которая реально сэкономила мне время». Итак, уровень растёт. И, думаю, новая версия Sonnet стала ещё лучше. В плане того, что для этого нужно, скажу, что улучшения были во всём: в предварительном обучении, в пост-обучении, в различных оценках, которые мы проводим. Мы тоже это наблюдали. Если углубиться в детали бенчмарка, SWE-bench — это… Раз ты программист, ты знаком с pull request’ами, и они как атомарные единицы работы. Можно сказать: «Я реализую одну задачу». SWE-bench даёт реальную ситуацию: кодовая база в текущем состоянии, и нужно реализовать что-то, описанное словами. У нас есть внутренние бенчмарки, где мы измеряем то же самое: даёшь модели полную свободу — запускать, редактировать что угодно. Насколько хорошо она справляется с такими задачами? И по этому бенчмарку прогресс с «справляется в 3% случаев» до «справляется примерно в 50% случаев». Я действительно верю, что можно улучшать бенчмарки, но если мы достигнем 100% на этом бенчмарке без переобучения или подгонки под него, это будет означать серьёзный рост способностей в программировании. И я подозреваю, что достижение 90-95% будет означать возможность автономного выполнения значительной части задач в разработке ПО.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Вопрос на засыпку: когда выйдет Claude Opus 3.5?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Не назову точную дату, но, насколько нам известно, план выпустить Claude 3.5 Opus остаётся в силе.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Он выйдет раньше GTA 6 или нет?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Как Duke Nukem Forever?</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Duke Nukem. Именно.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Что это была за игра? Та, которую задерживали 15 лет.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Верно.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Это был Duke Nukem Forever?</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да. А GTA сейчас только трейлеры выпускает.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Прошло всего три месяца с момента выхода первого Sonnet.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да, это невероятный темп выпуска.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Это говорит о том, какие сейчас ожидания по срокам выхода продуктов.</p>
					
                </div>
            </div>
			
            <!-- Блок 8 -->
            <div class="article-block" id="block-8">
				
                 <div class="block-header">
                    <h2 class="block-title">Claude 4.0</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Что насчёт 4.0? Как ты думаешь о версионировании, когда модели становятся всё больше и больше? И вообще, почему Sonnet 3.5 обновили с датой, а не назвали Sonnet 3.6, как многие его называют?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Название — это интересный вызов. Год назад большая часть работы была в предварительном обучении. Можно было начать с нуля и сказать: «У нас будут модели разных размеров, мы обучим их все вместе, придумаем систему названий, добавим что-то новое — и получим следующее поколение». Проблемы начинаются, когда некоторые модели обучаются гораздо дольше других. Это уже сбивает график. Но когда ты делаешь большой прорыв в предварительном обучении, то понимаешь: «О, я могу сделать модель лучше». Это не занимает много времени, но модель остаётся того же размера и формы. И это, плюс вопросы сроков, ломает любую систему названий. Реальность не вписывается в схему. Это не как в ПО, где можно сказать: «Вот версия 3.7, вот 3.8». Нет, у моделей разные компромиссы: одни быстрее, другие медленнее, одни дороже, другие дешевле. Все компании сталкиваются с этим. У нас была хорошая система с Haiku, Sonnet и Opus.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Отличное начало.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Мы пытаемся её сохранить, но это не идеально. Мы вернёмся к простоте. Но в этой области никто не придумал идеальную систему названий. Это другая парадигма по сравнению с обычным ПО, и ни у кого нет идеального решения. Это удивительно сложная задача на фоне науки обучения моделей.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>С точки зрения пользователя, обновлённый Sonnet 3.5 отличается от предыдущей версии за июнь 2024. Хорошо бы придумать систему обозначений, которая это отражает. Люди говорят «Sonnet 3.5», но теперь есть новая версия. Как отличать старую от новой, если есть явное улучшение? Это усложняет обсуждение.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, да. Я точно считаю, что есть множество свойств моделей, которые не отражены в тестах. Это действительно так, и все с этим согласны. И не все из них связаны с возможностями. Модели могут быть вежливыми или резкими, они могут быть очень реактивными или задавать вам вопросы. У них может быть, скажем так, тёплая или холодная личность. Они могут быть скучными или очень запоминающимися, как Golden Gate Claude. У нас есть целая команда, которая занимается, как мы это называем, «характером Claude». Аманда руководит этой командой, и мы расскажем вам об этом, но это всё ещё очень неточная наука. Часто мы обнаруживаем, что у моделей есть свойства, о которых мы не знали. Дело в том, что можно общаться с моделью 10 000 раз, и всё равно не увидеть некоторых её особенностей, прямо как с человеком, верно? Я могу знать кого-то несколько месяцев и не подозревать, что у него есть определённый навык или какая-то сторона личности. Так что, думаю, нам просто нужно привыкнуть к этой идее. Мы постоянно ищем лучшие способы тестирования моделей, чтобы выявлять эти возможности, а также решать, какие свойства личности мы хотим, чтобы модели имели, а какие — нет. Сам этот нормативный вопрос тоже очень интересен.</p>
					
                </div>
            </div>
			
            <!-- Блок 9 -->
            <div class="article-block" id="block-9">
				
                 <div class="block-header">
                    <h2 class="block-title">Критика Claude</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Мне нужно задать тебе вопрос с Reddit.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>С Reddit? Ох, боже.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Есть такой увлекательный, по крайней мере для меня, психологический и социальный феномен: люди сообщают, что со временем Claude стал для них «тупее». Вопрос в том, есть ли основания для жалоб пользователей на «оглупление» Claude 3.5 Sonnet? Это просто социальный феномен, основанный на единичных наблюдениях, или бывают случаи, когда Claude действительно становится менее умным?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>На самом деле это касается не только Claude. Я видел такие жалобы по поводу каждой базовой модели, выпущенной крупной компанией. Это говорили о GPT-4, о GPT-4 Turbo. Во-первых, сами веса модели, её «мозг», не меняются, пока мы не выпустим новую модель. Есть множество причин, по которым бессистемная замена версий модели была бы непрактичной. Это сложно с точки зрения вывода, и последствия изменения весов модели трудно контролировать. Допустим, ты хочешь дообучить модель, чтобы она реже говорила «конечно», как старая версия Sonnet. В итоге ты изменишь ещё 100 вещей. У нас есть целый процесс для таких изменений, включающий тестирование и сбор отзывов от ранних пользователей. Мы никогда не меняли веса модели без предупреждения. В текущей системе это просто не имело бы смысла. Иногда мы проводим A/B-тесты, но обычно это происходит незадолго до выпуска модели и на очень короткое время. Например, за день до выхода Sonnet 3.5 (да, название неудачное) некоторые заметили, что модель стала лучше — это потому, что часть пользователей участвовала в A/B-тесте. Иногда меняется системный промт, но он вряд ли сделает модель «тупее». Хотя эти два фактора случаются редко, жалобы на «оглупление», «ухудшение» или «излишнюю цензуру» моделей поступают постоянно. Не хочу сказать, что люди это выдумывают, но модели в основном не меняются. Мне кажется, это связано с их сложностью: даже небольшие изменения в формулировке запроса могут дать очень разные результаты. Это, конечно, наш провал: модели слишком чувствительны к формулировкам, и наука об их работе ещё очень слабо развита. Если я изменю способ общения с моделью, результаты могут отличаться. Кроме того, это сложно измерить. Люди восхищаются новыми моделями при выпуске, но со временем замечают их ограничения. Так что это может быть ещё одним эффектом. Короче говоря, за редкими исключениями, модели не меняются.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Думаю, есть психологический эффект: ты привыкаешь, и планка ожиданий растёт. Когда у людей впервые появился Wi-Fi в самолётах, это казалось чудом.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, это было потрясающе.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>А потом ты начинаешь…</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>И вот я думаю: «Эта штука не работает. Полный отстой.»</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Точно. Легко поддаться теории заговора: «Они намеренно замедляют Wi-Fi». Это, наверное, тема, которую я подробнее обсужу с Амандой, но вот ещё вопрос с Reddit: «Когда Claude перестанет быть моей строгой бабушкой-щупальцем, навязывающей свою мораль мне, платящему клиенту? И ещё: в чём психология, стоящая за избыточными извинениями Claude?» Это другой взгляд на разочарование, связанный с его поведением.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, пару замечаний по этому поводу. Во-первых, то, что люди пишут на Reddit и Twitter (или X, или как там сейчас), — это огромный разрыв между тем, на что громко жалуются в соцсетях, и тем, что действительно важно для пользователей и влияет на использование моделей. Люди разочарованы, когда модель не дописывает код или просто не так хороша в программировании, как могла бы быть, даже если это лучшая в мире модель для кода. Большинство жалоб именно об этом, но есть и громкое меньшинство, которое возмущается, когда модель отказывает в том, в чём не должна, или слишком часто извиняется, или использует раздражающие словесные шаблоны. Во-вторых, и это важно чётко обозначить, потому что некоторые не знают, а другие забывают: очень сложно контролировать поведение моделей во всех аспектах. Нельзя просто взять и сказать: «Пусть модель меньше извиняется». Можно добавить обучающие данные с такой установкой, но тогда в других ситуациях модель может стать грубой или излишне самоуверенной, что введёт людей в заблуждение. Всё это компромиссы. Например, был период, когда модели (наши и, думаю, другие тоже) были слишком многословны — повторялись, говорили лишнее. Можно снизить многословие, штрафуя модели за длинные ответы. Но если сделать это грубо, то при написании кода они могут начать говорить: «Остальной код здесь», потому что так они научились экономить слова. Это приводит к тому, что модель становится «ленивой» в написании кода, как бы говоря: «Остальное ты доделаешь сам». Это не из-за желания сэкономить ресурсы или потому что модели «ленивы на каникулах», как гласят некоторые теории заговора. Просто очень сложно контролировать поведение модели во всех ситуациях одновременно. Это как игра «бей крота»: исправляешь одну проблему, а другие вылезают в неожиданных местах. Именно поэтому я так обеспокоен глобальным согласованием ИИ-систем в будущем. Эти системы довольно непредсказуемы, и их сложно направлять. Текущие проблемы — это аналог будущих задач контроля над ИИ, которые мы можем изучать уже сейчас. Сложность в том, чтобы направлять поведение модели и гарантировать, что изменения в одной области не приведут к нежелательным последствиям в другой. Это ранний признак грядущих проблем. Если мы сможем решить, например, как заставить модель отказываться помогать в создании оспы, но при этом поддерживать в изучении вирусологии, — это будет прорыв. Но это сложно, это многомерная задача. Формирование «личности» модели — очень сложная задача. Мы не идеальны, но, думаю, справляемся лучше других компаний. И если мы научимся контролировать ложные срабатывания и пропуски в нынешних условиях, это поможет нам в будущем, когда модели станут автономными и смогут создавать опасные вещи или даже компании. Сейчас это раздражает, но также является хорошей тренировкой для будущего.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Как сейчас лучше всего собирать отзывы пользователей? Не единичные случаи, а массовые данные о проблемах или, наоборот, положительных моментах? Внутреннее тестирование? A/B-тесты? Что работает?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Обычно мы проводим внутренние тесты, где почти 1000 сотрудников Anthropic пытаются «сломать» модель, взаимодействуя с ней разными способами. У нас есть набор оценок, например, «отказывает ли модель без причины?» Был даже тест на слово «certainly», потому что модель начала надоедливо повторять: «Конечно, я помогу вам с этим. Конечно, я буду рад это сделать.» Но это игра в «бей крота»: если убрать «certainly», модель может начать говорить «definitely». У нас сотни таких оценок, но ничто не заменит живого взаимодействия. Мы также проводим A/B-тесты, нанимаем подрядчиков для тестирования. Несмотря на это, идеального результата нет: модель всё ещё отказывает в странных ситуациях. Наша задача — предотвратить серьёзные ошибки (например, обсуждение запрещённых тем), но при этом избегать глупых отказов. Это сложная грань, и мы …, но проблем ещё много. Это также показатель будущих сложностей с управлением более мощными моделями. Найти баланс между предотвращением вредных действий и избеганием неоправданных отказов — сложная задача. Мы приближаемся к идеалу, но предстоит ещё много работы. И это индикатор будущих вызовов в управлении ИИ. С каждым днём мы становимся лучше, но предстоит ещё многое решить. И это показатель грядущих сложностей в управлении более мощными моделями.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Думаешь, Claude 4.0 когда-нибудь выйдет?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Я не хочу давать никаких обязательств по поводу названий, потому что если я скажу: «В следующем году у нас будет Claude 4», а потом мы решим начать с нуля из-за появления нового типа модели, мне не хотелось бы быть связанным этим. В обычном ходе событий можно ожидать, что Claude 4 появится после Claude 3.5, но в этой безумной сфере никогда нельзя знать наверняка.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Но идея масштабирования продолжает развиваться.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Масштабирование продолжается. Определённо появятся модели мощнее, чем те, что существуют сегодня. Это точно. А если нет, значит, мы как компания потерпели полный провал.</p>
					
                </div>
            </div>
			
            <!-- Блок 10 -->
            <div class="article-block" id="block-10">
				
                 <div class="block-header">
                    <h2 class="block-title">Уровни безопасности ИИ</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Хорошо. Можешь объяснить политику ответственного масштабирования и стандарты уровней безопасности ИИ, ASL?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Как бы я ни был воодушевлён преимуществами этих моделей — и мы поговорим об этом, если будем обсуждать Machines of Loving Grace, — я обеспокоен рисками и продолжаю быть обеспокоенным. Никто не должен думать, что Machines of Loving Grace означало, будто я больше не волнуюсь о рисках этих моделей. Я считаю, что это две стороны одной медали. Мощь моделей и их способность решать все эти проблемы в биологии, нейробиологии, экономическом развитии, управлении и мире, значительных секторах экономики — всё это сопряжено и с рисками, верно? С великой силой приходит великая ответственность. Они неразрывно связаны. Мощные вещи могут делать хорошее, но могут и плохое. Я разделяю эти риски на несколько категорий, возможно, две самые большие, о которых я думаю. И это не значит, что сегодня нет важных рисков, но когда я думаю о вещах, которые могут произойти в самом грандиозном масштабе, первое — это то, что я называю катастрофическим злоупотреблением. Это злоупотребление моделями в таких сферах, как киберпространство, биология, радиология, ядерные технологии — вещи, которые могут навредить или даже убить тысячи, даже миллионы людей, если всё пойдёт совсем не так. Их предотвращение — приоритет номер один. И здесь я сделаю простое наблюдение: сегодня человечество защищено тем, что пересечение между очень умными, хорошо образованными людьми и теми, кто хочет совершать ужасные вещи, как правило, невелико. Допустим, у меня есть PhD в этой области, хорошо оплачиваемая работа. Есть что терять. Даже если я абсолютно злой, что редкость, зачем такому человеку рисковать жизнью, репутацией, наследием ради чего-то поистине ужасного? Если бы таких людей было больше, мир стал бы гораздо опаснее. И я беспокоюсь, что ИИ, как более разумный агент, может разрушить эту корреляцию. И у меня есть серьёзные опасения на этот счёт. Я верю, что мы можем их предотвратить. Но как контрапункт к Machines of Loving Grace, я хочу сказать, что риски остаются серьёзными. Вторая группа рисков — это риски автономности, идея, что модели могут самостоятельно, особенно если мы дадим им больше агентства, чем раньше, особенно если поручим более масштабные задачи, вроде написания целых кодовых баз или даже управления компаниями, — окажутся на слишком длинном поводке. Делают ли они то, чего мы действительно от них хотим? Очень сложно даже понять детали их действий, не говоря уже о контроле. И, как я сказал, первые признаки того, что границу между допустимым и недопустимым поведением провести идеально сложно: с одной стороны — раздражающая бесполезность, с другой — нежелательные действия. Исправишь одно — возникают другие проблемы. Мы становимся всё лучше в решении этого. Я не думаю, что это нерешаемая проблема. Это наука, как безопасность самолётов, автомобилей или лекарств. Нет какого-то глобального упущения. Просто нужно научиться лучше контролировать эти модели. Вот два риска, которые меня беспокоят. И наш план ответственного масштабирования, который, признаю, стал очень долгим ответом на ваш вопрос.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Мне нравится. Мне нравится.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Наш план ответственного масштабирования разработан для устранения этих двух типов рисков. Каждый раз, создавая новую модель, мы тестируем её на способность совершать такие вредные действия. Если немного отступить, у нас интересная дилемма с ИИ: пока он недостаточно мощный, чтобы создать катастрофы. Неизвестно, создаст ли вообще. Возможно, и нет. Но аргументы в пользу рисков достаточно сильны, чтобы действовать сейчас, а модели улучшаются очень, очень быстро. Я говорил в Сенате, что серьёзные биориски могут появиться через два-три года. Прошёл примерно год. Всё идёт по плану. Получается, что с этими рисками удивительно сложно бороться, потому что их пока нет. Они как призраки, но приближаются к нам с огромной скоростью из-за стремительного развития моделей. Итак, как справиться с тем, чего ещё нет сегодня, но что приближается к нам очень быстро? Решение, которое мы разработали в сотрудничестве с такими организациями, как METR, и Полом Кристиано, заключается в том, что вам нужны тесты, которые покажут, когда риск станет близким. Вам нужна система раннего предупреждения. Поэтому каждый раз, когда у нас появляется новая модель, мы тестируем её на способность выполнять задачи, связанные с CBRN, а также на её автономность. В последней версии нашего RSP, выпущенной в прошлом месяце или около того, мы проверяем риски автономности по способности модели ИИ самостоятельно проводить аспекты исследований в области ИИ. Когда модели ИИ смогут заниматься исследованиями, они станут по-настоящему автономными. Этот порог важен по многим причинам. И что мы тогда делаем с этими задачами? RSP разрабатывает структуру «если-то»: если модели достигают определённого уровня способностей, то мы накладываем на них определённые требования по безопасности. Сегодняшние модели относятся к уровню ASL-2. Модели уровня ASL-1 — это системы, которые явно не представляют риска автономности или misuse. Например, шахматный бот Deep Blue был бы ASL-1. Очевидно, что Deep Blue нельзя использовать ни для чего, кроме шахмат. Он был создан только для этого. Никто не сможет использовать его для кибератаки или захвата мира. ASL-2 — это современные системы ИИ, которые, как мы выяснили, недостаточно умны для автономного самовоспроизведения или выполнения опасных задач. Они также не способны предоставлять значимую информацию о рисках CBRN или создании оружия CBRN сверх того, что можно найти в Google. Иногда они действительно дают больше информации, чем поисковик, но не в такой форме, которая могла бы быть опасной. ASL-3 — это уровень, на котором модели могут усилить возможности негосударственных акторов. Государственные акторы, к сожалению, уже способны на многое из этого. Разница в том, что негосударственные акторы пока не могут. Когда мы достигнем ASL-3, мы введём особые меры безопасности, чтобы предотвратить кражу модели и её misuse. Нам потребуются усиленные фильтры для этих областей.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Кибер, био, ядерное.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Кибер, био, ядерное и автономность модели, что скорее риск действий самой модели, чем её misuse. ASL-4 — это уровень, на котором модели могут усилить возможности государственных акторов или стать основным источником угрозы. ASL-4 также предполагает ускорение в исследованиях ИИ с помощью самой модели. ASL-5 — это уровень, на котором модели превзойдут человечество в выполнении любых задач. Идея структуры «если-то» в том, чтобы не поднимать панику раньше времени. Опасно говорить, что модель рискованна, когда она явно не опасна. Но риск приближается быстро, и нам нужно быть готовыми. Как с этим справиться? Это сложная задача для планировщиков рисков. Структура «если-то» позволяет не накладывать излишние ограничения на пока ещё безопасные модели, но жёстко реагировать, когда опасность становится очевидной. Конечно, важно иметь достаточный запас, чтобы не пропустить опасность. Это не идеальная система. Мы уже меняли её и, вероятно, будем делать это ещё, потому что технически и организационно сложно создать идеальную политику. Но это наше предложение: структура «если-то» для минимизации ложных тревог и адекватного реагирования на реальные угрозы.</p>
					
                </div>
            </div>
			
            <!-- Блок 11 -->
            <div class="article-block" id="block-11">
				
                 <div class="block-header">
                    <h2 class="block-title">ASL-3 и ASL-4</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Какой, по-вашему, срок достижения ASL-3, когда сработают триггеры? И какой срок для ASL-4?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Это горячо обсуждается в компании. Мы активно готовим меры безопасности для ASL-3. Детали я опущу, но прогресс значительный, и мы скоро будем готовы. Я не удивлюсь, если ASL-3 будет достигнут в следующем году. Есть опасения, что это может случиться даже в этом году. 2030 год кажется мне слишком поздним сроком.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>То есть есть протоколы для обнаружения и протоколы для реагирования.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Насколько сложен второй аспект?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Для ASL-3 это в основном вопросы безопасности и фильтров в узких областях при развёртывании модели. На этом уровне модель ещё не автономна, поэтому не стоит беспокоиться о её поведении внутри системы. Меры для ASL-3 строгие, но их проще реализовать. С ASL-4 появляются опасения, что модели могут обманывать тесты, скрывать свои возможности. Недавние исследования показали, что модели могут вводить в заблуждение. Поэтому для ASL-4 важно использовать не только взаимодействие с моделью, но и интерпретируемость или скрытые цепочки рассуждений, чтобы проверять её свойства. Мы ещё работаем над ASL-4. Одно из правил RSP — не определять меры для ASL-4, пока не достигнем ASL-3. Это мудрое решение, потому что даже с ASL-3 много неизвестного, и мы хотим тщательно всё проработать.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>То есть для ASL-3 угрозой будут люди.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Люди, да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Итак, здесь есть немного больше…</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Для ASL-4, я думаю, это и то, и другое.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>И то, и другое. И вот здесь в игру вступает механистическая интерпретируемость, и, надеюсь, методы, используемые для этого, не будут доступны модели.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да. Конечно, вы можете подключить механистическую интерпретируемость к самой модели, но тогда вы потеряете её как надежный индикатор состояния модели. Есть множество экзотических способов, при которых она тоже может быть ненадежной, например, если модель станет настолько умной, что сможет переключаться между компьютерами и читать код, где вы смотрите на её внутреннее состояние. Мы думали о некоторых из них. Думаю, они достаточно экзотичны. Есть способы сделать их маловероятными. Но да, в целом, вы хотите сохранить механистическую интерпретируемость как проверочный набор или тестовый набор, отдельный от процесса обучения модели.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Видите ли, я думаю, что по мере того, как эти модели становятся лучше в общении и умнее, социальная инженерия тоже становится угрозой, потому что они могут начать очень убедительно воздействовать на инженеров внутри компаний.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>О, да. Да. Мы видели множество примеров демагогии в нашей жизни от людей, и есть опасение, что модели тоже могут это делать.</p>
					
                </div>
            </div>
			
            <!-- Блок 12 -->
            <div class="article-block" id="block-12">
				
                 <div class="block-header">
                    <h2 class="block-title">Использование компьютера</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Один из способов, благодаря которому Claude становится всё мощнее, — это его способность выполнять некоторые агентские задачи, использование компьютера. Также есть анализ внутри песочницы самого Claude.ai. Но давайте поговорим об использовании компьютера. Мне кажется, это невероятно захватывающе, что вы можете просто дать Claude задачу, и он выполняет множество действий, разбирается в ней и получает доступ к… … множеству действий, разбирается в ней и получает доступ к вашему компьютеру через скриншоты. Можете объяснить, как это работает и к чему это ведёт?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да. На самом деле это относительно просто. У Claude уже давно, с версии Claude 3 ещё в марте, была возможность анализировать изображения и отвечать на них текстом. Единственное новое, что мы добавили, — это то, что эти изображения могут быть скриншотами компьютера, и в ответ мы обучаем модель указывать место на экране, куда можно кликнуть, и/или кнопки на клавиатуре, которые можно нажать, чтобы выполнить действие. Оказывается, что при не таком уж большом дополнительном обучении модели могут стать довольно хорошими в этой задаче. Это хороший пример обобщения. Люди иногда говорят, что если вы достигнете низкой околоземной орбиты, вы на полпути к чему угодно из-за того, сколько нужно для преодоления гравитационного колодца. Если у вас есть сильная предобученная модель, я чувствую, что вы на полпути к чему угодно в пространстве интеллекта. И на самом деле, не потребовалось так уж много, чтобы заставить Claude делать это. Вы можете просто запустить это в цикле: дать модели скриншот, сказать, куда кликнуть, дать следующий скриншот, сказать, куда кликнуть, и это превращается в полное, почти 3D-видео взаимодействие модели, и она способна выполнять все эти задачи. Мы показывали демонстрации, где она может заполнять таблицы, взаимодействовать с веб-сайтом, открывать все kinds of программы, разные операционные системы: Windows, Linux, Mac. Всё это очень захватывающе. Скажу, что хотя теоретически нет ничего, что вы не могли бы сделать через API для управления экраном компьютера, это значительно снижает барьер. И есть много людей, которые либо не могут взаимодействовать с этими API, либо им требуется много времени. Экран — это просто универсальный интерфейс, с которым гораздо проще взаимодействовать. И я ожидаю, что со временем это снизит множество барьеров. Сейчас, честно говоря, текущая модель оставляет желать лучшего, и мы честно сказали об этом в блоге. Она ошибается, кликает не туда. Мы предупредили людей: «Эй, вы не можете просто оставить эту штуку работать на вашем компьютере минутами. Вам нужно установить границы и ограничения». И я думаю, это одна из причин, почему мы сначала выпустили её в виде API, а не просто дали потребителю контроль над их компьютером. Но я определённо чувствую, что важно выпускать эти возможности. По мере того как модели становятся мощнее, нам придётся разбираться, как использовать эти возможности безопасно. Как предотвратить их злоупотребление? И я думаю, выпуск модели, пока её возможности ещё ограничены, очень помогает в этом. С момента выпуска несколько клиентов, например, Replit, возможно, был одним из самых быстрых в развёртывании, использовали её по-разному. Люди подключили демонстрации для рабочих столов Windows, Mac, Linux. Так что да, это очень захватывающе. Как и со всем остальным, это приносит новые захватывающие возможности, а с ними мы должны думать, как сделать модель безопасной, надёжной, чтобы она делала то, что хотят люди. Это та же история для всего. Та же напряжённость.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Но диапазон возможных вариантов использования просто невероятен. Насколько нужно выходить за рамки предобученной модели, чтобы это работало действительно хорошо в будущем? Нужно ли больше пост-обучения, RLHF, контролируемой тонкой настройки или синтетических данных специально для агентских задач?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да. Говоря на высоком уровне, мы намерены продолжать вкладывать много сил в улучшение модели. Мы смотрим на некоторые тесты, где предыдущие модели справлялись в 6% случаев, а наша модель сейчас справляется в 14 или 22%. И да, мы хотим достичь человеческого уровня надёжности в 80-90%, как и везде. Мы на той же кривой, что и с SWE-bench, и я думаю, что через год модели смогут делать это очень и очень надёжно. Но нужно с чего-то начинать.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Так вы думаете, что можно достичь человеческого уровня в 90%, просто продолжая делать то же, что и сейчас, или для использования компьютера нужно что-то особенное?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Зависит от того, что вы подразумеваете под «особенным» и «особенным» в целом, но я обычно думаю, что те же методы, которые мы использовали для обучения текущей модели, я ожидаю, что их дальнейшее развитие, как это было с кодом, моделями в целом, обработкой изображений и голоса, будет масштабироваться и здесь, как это происходило во всех других областях.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Но это дает Клоду возможность действовать, и вы можете делать много действительно мощных вещей, но также можете нанести много вреда.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, да. Нет, мы очень хорошо это осознаем. По моему мнению, использование компьютера не является принципиально новой возможностью, как, например, возможности в области CBRN или автономности. Это скорее расширяет возможности модели для применения её существующих способностей. Исходя из нашего RSP, мы считаем, что сама по себе эта модель не увеличивает риски с точки зрения RSP, но по мере роста её мощности наличие такой возможности может сделать её более опасной, когда она достигнет когнитивных способностей уровня ASL-3 и ASL-4. Это может стать тем, что позволит ей выйти за рамки. Поэтому, безусловно, этот режим взаимодействия — это то, что мы тестировали и будем продолжать тестировать в рамках RSP. Думаю, лучше изучить и исследовать эту возможность, пока модель не стала сверхмощной.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да. И есть много интересных атак, таких как инъекция промптов, потому что теперь вы расширили возможности, и можно внедрять промпты через содержимое на экране. Если это становится всё более полезным, то и выгода от внедрения вредоносного контента растет. Это может быть безобидная реклама или что-то вредоносное, верно?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, мы много думали о таких вещах, как спам, CAPTCHA, массовые… Секрет в том, что когда вы изобретаете новую технологию, первое злоупотребление, с которым вы столкнётесь, — это мошенничество, просто мелкие аферы.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Это стара как мир вещь — люди обманывают друг друга. И каждый раз приходится с этим разбираться.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Звучит почти глупо, но это правда: боты и спам в целом становятся всё более изощрёнными…</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>…и бороться с ними становится всё сложнее.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Как я уже сказал, в мире много мелких преступников, и каждая новая технология — это новый способ для них сделать что-то глупое и злонамеренное.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Есть ли идеи по поводу песочницы? Насколько сложна задача песочницы?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, мы используем песочницу во время обучения. Например, во время обучения мы не подключали модель к интернету. Думаю, это плохая идея во время обучения, потому что модель может менять свою политику, свои действия и влиять на реальный мир. Что касается развертывания модели, это зависит от приложения. Иногда вы хотите, чтобы модель что-то делала в реальном мире. Но, конечно, всегда можно установить ограничения. Например, можно сказать: «Эта модель не будет перемещать файлы с моего компьютера или веб-сервера куда-либо ещё». Когда речь заходит о песочнице, на уровне ASL-4 все эти меры предосторожности теряют смысл. На уровне ASL-4 возникает теоретическая опасность, что модель станет достаточно умной, чтобы вырваться из любой песочницы. Поэтому здесь нужно думать о механистической интерпретируемости. Если мы создадим песочницу, она должна быть математически доказуемой. Это совершенно другой уровень по сравнению с тем, с чем мы имеем дело сегодня.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да, наука создания такой песочницы, из которой система ИИ уровня ASL-4 не сможет выбраться.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Думаю, это неверный подход. Лучше не пытаться удержать что-то неконтролируемое, а правильно спроектировать модель или создать цикл, где вы можете заглянуть внутрь модели, проверить её свойства и итеративно добиться нужного результата. Сдерживание плохих моделей — гораздо худшее решение, чем создание хороших.</p>
					
                </div>
            </div>
			
            <!-- Блок 13 -->
            <div class="article-block" id="block-13">
				
                 <div class="block-header">
                    <h2 class="block-title">Государственное регулирование ИИ</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Позвольте спросить о регулировании. Какова роль регулирования в обеспечении безопасности ИИ? Например, можете описать калифорнийский законопроект о регулировании ИИ SB 1047, который в итоге был отклонён губернатором? Каковы плюсы и минусы этого законопроекта в целом?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, в итоге мы внесли несколько предложений по законопроекту. Некоторые из них были приняты, и к концу этого процесса мы, думаю, относились к законопроекту вполне положительно, хотя у него оставались некоторые недостатки. И, конечно, он был отклонён. На высоком уровне, ключевые идеи, лежащие в основе этого законопроекта, я бы сказал, схожи с идеями, лежащими в основе наших RSP (Responsible Scaling Policies). И я считаю очень важным, чтобы какая-то юрисдикция — будь то Калифорния, федеральное правительство и/или другие страны и штаты — приняла подобное регулирование. Я могу объяснить, почему это так важно. Я доволен нашим RSP. Он не идеален, над ним нужно много работать, но он стал хорошим инструментом, чтобы заставить компанию серьёзно относиться к этим рискам, учитывать их при планировании продуктов и сделать их центральной частью работы в Anthropic. А также убедиться, что все сотрудники — а их уже почти тысяча в Anthropic — понимают, что это один из главных приоритетов компании, если не самый главный. Но, во-первых, есть компании, у которых нет подобных RSP-механизмов. OpenAI и Google внедрили такие механизмы через несколько месяцев после Anthropic, но есть и другие компании, у которых их вообще нет. И если одни компании внедрят эти механизмы, а другие нет, это создаст ситуацию, когда некоторые из этих опасностей будут обладать свойством: неважно, если три из пяти компаний действуют безопасно, если остальные две — нет. Это создаёт негативные внешние эффекты. И я считаю, что отсутствие единообразия несправедливо по отношению к тем, кто приложил много усилий, чтобы тщательно продумать эти процедуры. Во-вторых, я не думаю, что можно доверять компаниям в соблюдении этих добровольных планов самостоятельно. Хочется верить, что Anthropic будет делать всё возможное — наш RSP проверяется нашим долгосрочным трастом, поэтому мы прилагаем все усилия, чтобы следовать ему. Но вы часто слышите о разных компаниях: «Они обещали столько-то вычислительных ресурсов, но не дали. Они сказали, что сделают то-то, но не сделали». Я не думаю, что стоит разбирать конкретные случаи, но общий принцип таков: если за нами как отраслью нет надзора, нет гарантии, что мы поступим правильно, а ставки очень высоки. Поэтому важно иметь единый стандарт, который все соблюдают, и просто добиться, чтобы отрасль делала то, что большинство её участников уже назвали важным и обязались выполнять. Есть люди, которые принципиально против регулирования. Я понимаю их позицию. Если взглянуть на GDPR в Европе или другие их инициативы, часть из них хороша, но часть создаёт излишнюю нагрузку и, справедливо сказать, тормозит инновации. Я понимаю, откуда берутся такие взгляды. Но ИИ — это другое. Если вернуться к серьёзным рискам автономности и злоупотреблений, о которых я говорил несколько минут назад, они необычны и требуют необычно жёсткого ответа. Поэтому я считаю это очень важным. Нам нужно что-то, что поддержат все. Одна из проблем SB 1047, особенно его первоначальной версии, в том, что, хотя он включал структуру RSP, в нём также было много громоздких или обременительных положений, которые могли даже не достичь цели в снижении рисков. В Twitter вы не увидите обсуждения этого — только одни поддерживают любое регулирование, а другие приводят зачастую недобросовестные аргументы: например, что это заставит нас уехать из Калифорнии (хотя закон не применяется, если вы базируетесь в Калифорнии, только если ведёте там бизнес), или что это навредит open-source, или вызовет другие проблемы. Думаю, это в основном ерунда, но есть и более серьёзные аргументы против регулирования. Например, Дин Болл — очень грамотный аналитик, который изучает, как регулирование может выйти из-под контроля или быть плохо спроектированным. Наша позиция всегда была: регулирование в этой сфере необходимо, но мы хотим, чтобы оно было точным, направленным на серьёзные риски и выполнимым. Потому что сторонники регулирования не всегда понимают: если принять что-то нецелевое, что тратит время людей, они скажут: «Видите, эти разговоры о безопасности — ерунда. Мне пришлось нанять 10 юристов для заполнения форм и проводить тесты для чего-то, что явно не опасно». И через шесть месяцев после этого возникнет массовое движение, и мы придём к устойчивому консенсусу против регулирования. Поэтому я считаю, что худший враг тех, кто хочет реальной подотчётности, — это плохо продуманное регулирование. Нам нужно сделать это правильно. И если бы я мог сказать что-то защитникам регулирования, то попросил бы их лучше понять эту динамику. Нам нужно быть очень осторожными и консультироваться с людьми, которые на практике видели, как работают регуляторные меры. Те, кто это видел, понимают, насколько важно быть осторожным. Если бы это был менее важный вопрос, я мог бы вообще выступать против регулирования. Но я хочу, чтобы противники понимали: лежащие в основе проблемы действительно серьёзны. Это не то, что я или другие компании выдумали ради регуляторного захвата, это не фантазии из научной фантастики, это нечто реальное. Каждый раз, когда у нас появляется новая модель, мы измеряем её поведение, и с каждым месяцем эти модели становятся всё лучше в решении тревожных задач — так же, как и в выполнении полезных, ценных, экономически значимых задач. Я бы очень хотел, чтобы некоторые из прежних… Думаю, SB 1047 вызвал сильную поляризацию. Я бы хотел, чтобы самые разумные противники и самые разумные сторонники сели за один стол. Разные компании в сфере ИИ… Anthropic была единственной компанией, которая выразила подробную поддержку. Илон Маск кратко написал что-то положительное в твиттере, но крупные игроки, такие как Google, OpenAI, Meta, Microsoft, были категорически против. Поэтому я бы очень хотел, чтобы ключевые заинтересованные стороны — самые вдумчивые сторонники и самые вдумчивые противники — сели вместе и подумали, как решить эту проблему так, чтобы сторонники чувствовали реальное снижение рисков, а противники — чтобы это не мешало индустрии или инновациям больше, чем необходимо. Думаю, по какой-то причине ситуация стала слишком поляризованной, и эти две группы не смогли сесть вместе, как следовало бы. Я чувствую urgency. Мне кажется, нам нужно что-то сделать в 2025 году. Если к концу 2025 года мы всё ещё ничего не предпримем, я буду беспокоиться. Пока я не беспокоюсь, потому что риски ещё не проявились, но время на исходе.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>И прийти к чему-то точному, как ты сказал.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, да, точно. И нам нужно уйти от этой острой риторики «за безопасность» против «против регулирования». Это превратилось в словесные баталии в Twitter, и ничего хорошего из этого не выйдет.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Многие интересуются разными игроками в этой сфере. Один из ветеранов — OpenAI. У тебя есть многолетний опыт работы в OpenAI. Какова твоя история и опыт там?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да. Я проработал в OpenAI около пяти лет. Последние пару лет я был вице-президентом по исследованиям. Наверное, я и Илья Суцкевер были теми, кто задавал направление исследований. Где-то в 2016 или 2017 году я начал по-настоящему верить в гипотезу масштабирования, когда Илья сказал мне: «Главное, что нужно понять об этих моделях, — они просто хотят учиться. Модели просто хотят учиться». Иногда одна фраза, один момент, объясняет всё. После этого у меня в голове сложился образ: если правильно оптимизировать модели, направить их в нужное русло, они просто хотят учиться. Они просто хотят решать задачу, независимо от её сути.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>То есть, грубо говоря, не мешать им?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Не мешать. Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Хорошо.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Не навязывать свои представления о том, как они должны учиться. Это перекликается с идеей Рича Саттона в «Горьком уроке» или Gwern в гипотезе масштабирования. В целом, динамика была такова: я вдохновлялся Ильёй и другими, такими как Алек Радфорд, создатель GPT-1, а затем активно развивал это направление с коллегами в GPT-2, GPT-3, RLHF (обучение с подкреплением на основе человеческих оценок), что было попыткой решить ранние проблемы безопасности и устойчивости, такие как дебаты и амплификация, с упором на интерпретируемость. Сочетание безопасности и масштабирования. Вероятно, 2018, 2019, 2020 годы были временем, когда я и мои коллеги, многие из которых позже стали соучредителями Anthropic, сформировали видение и задали направление.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Почему ты ушёл? Что повлияло на твоё решение?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Знаешь, я скажу так, и это связано с «гонкой к вершине»: за время работы в OpenAI я пришёл к пониманию гипотезы масштабирования и важности безопасности в этом контексте. С первым OpenAI соглашался. Второе всегда было частью их риторики. Но за годы работы у меня сложилось чёткое видение того, как следует подходить к этим вопросам, как их внедрять, какими принципами должна руководствоваться организация. Было много дискуссий: должна ли компания делать то или это? Вокруг этого много дезинформации. Люди говорят, что мы ушли, потому что нам не понравилась сделка с Microsoft. Это неправда. Хотя, конечно, было много обсуждений и вопросов о том, как именно мы заключаем сделку с Microsoft. Мы ушли потому, что нам не нравилась коммерциализация. Это тоже неверно. Мы создали GPT-3 — модель, которая была коммерциализирована. Я сам участвовал в этом процессе. Дело скорее в том, как именно это делать. Цивилизация движется по пути создания очень мощного ИИ. Как сделать это правильно? Осторожно, прямо, честно, чтобы это укрепляло доверие к организации и к людям. Как нам пройти этот путь и как создать реальное видение того, как все сделать правильно? Как сделать безопасность не просто словами, которые помогают с наймом. В конце концов, если у вас есть свое видение, забудьте о видении других. Я не хочу говорить о чьем-то еще видении. Если у вас есть свое видение, как это сделать, идите и реализуйте его. Бесконечно непродуктивно пытаться спорить с чьим-то видением. Вы можете считать, что они делают это неправильно. Может, вы правы, а может, и нет. Но ваша задача — собрать людей, которым вы доверяете, и вместе воплотить свое видение. Если оно убедительно, если вы сможете сделать его привлекательным для людей — этически, на рынке, если вы создадите компанию, в которую хочется прийти, которая следует практикам, считающимся разумными, и при этом сохраняет свое место в экосистеме, — люди начнут копировать вас. Сам факт, что вы это делаете, особенно если делаете лучше других, заставит их изменить свое поведение гораздо эффективнее, чем если бы вы спорили с начальством. Я не знаю, как объяснить это точнее, но пытаться подогнать чужое видение под свое — очень непродуктивно. Гораздо продуктивнее провести чистый эксперимент и сказать: «Вот наше видение, вот как мы будем действовать. Вы можете игнорировать нас, отвергать наши методы или начать становиться похожими на нас». Подражание — самая искренняя форма лести. Это отражается в поведении клиентов, публики, в выборе места работы. И в конце концов, дело не в победе одной компании над другой. Если мы или другая компания внедряем практику, которая людям действительно нравится — и я говорю о сути, а не о видимости, ведь исследователи смотрят на суть, — и другие компании начинают копировать эту практику, и они побеждают благодаря этому — это прекрасно. Это успех. Это гонка к вершине. Неважно, кто в итоге победит, если все копируют хорошие практики друг друга. Мы все боимся гонки ко дну, где неважно, кто победит, потому что проиграют все. В худшем случае мы создадим автономный ИИ, который поработит нас. Это шутка, но такое возможно. Тогда не будет важно, какая компания была впереди. Но если вы создадите гонку к вершине, где компании соревнуются в хороших практиках, то в итоге неважно, кто победит или кто начал эту гонку. Суть не в том, чтобы быть добродетельными, а в том, чтобы привести систему к лучшему равновесию. Отдельные компании могут сыграть в этом роль: начать, ускорить процесс. И, честно говоря, я думаю, что сотрудники других компаний тоже это делают. Те, кто, видя наш RSP, начинают активнее продвигать подобное у себя. Иногда другие компании делают что-то, и мы думаем: «Это хорошая практика, нам тоже стоит ее внедрить». Разница лишь в том, что мы стараемся быть более proactive, быстрее внедрять такие практики. Но важно то, что эта динамика меняет экосистему, в которой мы все работаем, и делает ее лучше, что влияет на всех участников.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>То есть Anthropic — это чистый эксперимент, основанный на конкретных принципах безопасности ИИ?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Ну, я уверен, что мы совершили много ошибок. Идеальной организации не существует. Она должна справляться с несовершенством тысяч сотрудников, лидеров, включая меня, и тех, кто контролирует лидеров, — совета директоров, траста долгосрочных benefit. Это группа несовершенных людей, пытающихся несовершенно достичь идеала, который никогда не будет достигнут. На это мы и подписались. Так будет всегда. Но несовершенство — не повод сдаваться. Есть лучше и хуже. Надеюсь, мы сможем создать практики, которые подхватит вся индустрия. Думаю, многие компании добьются успеха: Anthropic, компании, в которых я работал раньше. Некоторые будут успешнее других. Но важнее то, как мы выстроим стимулы индустрии. Это происходит через гонку к вершине, через такие инструменты, как RSP, через точечное регулирование.</p>
					
                </div>
            </div>
			
            <!-- Блок 14 -->
            <div class="article-block" id="block-14">
				
                 <div class="block-header">
                    <h2 class="block-title">Создание сильной команды</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Вы сказали, что плотность талантов важнее их количества. Можете объяснить?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Можешь рассказать, что нужно для создания сильной команды исследователей и инженеров в области ИИ?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Это одно из тех утверждений, которое становится всё более верным с каждым месяцем. Каждый месяц я вижу, что оно вернее, чем в предыдущем. Если провести мысленный эксперимент: допустим, у вас есть команда из 100 человек — очень умных, мотивированных и разделяющих вашу миссию. Или команда из тысячи, где 200 таких же, а остальные 800 — условно, случайные сотрудники из крупных tech-компаний. Что бы вы выбрали? Количество талантов больше в группе из тысячи. У вас даже больше исключительно талантливых, преданных и умных людей. Но проблема в том, что когда талантливый человек видит вокруг таких же, это задаёт тон для всего. Это вдохновляет всех работать в одном месте. Все доверяют друг другу. Если у вас тысяча или 10 тысяч человек, но качество упало, и вы набираете случайных людей, приходится вводить множество процессов и ограничений просто потому, что люди не доверяют друг другу или приходится разбирать политические конфликты. Всё это замедляет работу организации. У нас почти тысяча сотрудников, и мы стараемся, чтобы как можно больше из них были исключительно талантливы и квалифицированы. Это одна из причин, почему мы замедлили найм в последние месяцы. Сначала мы выросли с 300 до 800, кажется, за первые семь-восемь месяцев года, а затем замедлились. За последние три месяца — с 800 до 900, 950, примерно так. Точные цифры не важны, но вокруг тысячи есть переломный момент, и мы хотим расти осторожнее. С самого начала и сейчас мы нанимали много физиков. Теоретические физики учатся очень быстро. В последнее время мы ужесточили критерии как для исследований, так и для разработки, нанимали много опытных специалистов, в том числе из других компаний в этой сфере, и продолжаем быть очень избирательными. Легко вырасти со ста до тысячи, с тысячи до 10 тысяч, не следя за тем, чтобы у всех была общая цель. Это очень важно. Если в компании много «княжеств», каждое из которых тянет в свою сторону, добиться чего-то сложно. Но если все видят общую цель, если есть доверие и стремление делать правильные вещи, это суперсила. Она может компенсировать почти любые недостатки.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Как говорил Стив Джобс, топовые специалисты хотят видеть вокруг себя таких же топовых специалистов. Это другая формулировка того же самого.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Верно.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Не знаю, почему так устроена человеческая природа, но демотивирует видеть людей, которые не одержимы общей миссией. И наоборот, это очень вдохновляет. Интересно. Какими качествами должен обладать выдающийся исследователь или инженер в области ИИ, судя по твоему опыту работы с такими людьми?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да. Главное качество, особенно в исследованиях, но и в разработке тоже, — это открытость. Казалось бы, что проще — быть открытым? Просто сказать: «Я ко всему открыт». Но если вспомнить мои первые шаги в гипотезе масштабирования, я видел те же данные, что и другие. Я не был лучшим программистом или генератором идей, чем сотни людей, с которыми работал. В чём-то я был хуже. Я никогда не был силён в точном программировании, поиске багов, написании GPU-ядре. Могу назвать сотню людей, которые лучше меня в этом. Но у меня было другое качество — я был готов взглянуть на что-то свежим взглядом. Люди говорили: «У нас ещё нет подходящих алгоритмов, мы не придумали, как это сделать». А я думал: «Ну не знаю. Вот нейросеть с 30 миллионами параметров. Что, если дать ей 50 миллионов? Давайте построим графики». Такой научный подход: «О, есть переменная, которую можно изменить. Что будет, если её изменить? Давайте попробуем и посмотрим». Это было элементарно, даже не уровень PhD. Любой мог это сделать, если бы знал, что это важно. И это не сложно понять. Не нужно быть гением, чтобы до этого додуматься. Но если соединить эти две вещи, оказывается, что лишь единицы двигают всю область вперёд. Так часто бывает. Если посмотреть на исторические открытия, многие из них такие. Эта открытость и готовность смотреть свежим взглядом, которая часто бывает у новичков в области (опыт здесь даже минус), — самое важное. Это сложно искать и проверять, но это ключевое качество. Потому что когда находишь новый способ думать о вещах, это полностью меняет всё.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>И ещё важно уметь быстро экспериментировать, оставаясь при этом открытым и любопытным, смотреть на данные свежим взглядом и видеть, что они на самом деле говорят. Это применимо и в механистической интерпретируемости.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Это ещё один пример. Некоторые ранние работы по механистической интерпретируемости настолько просты, что просто никто раньше не задумывался об этом.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Ты сказал, что нужно, чтобы стать выдающимся исследователем в области ИИ. Если отмотать время назад, какой совет ты дал бы тем, кто только начинает интересоваться ИИ? Молодым людям, которые ищут свой путь… Какой совет вы дали бы людям, интересующимся ИИ? Они молоды. Хотят знать, как можно повлиять на мир?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Мой главный совет — начать экспериментировать с моделями. Честно говоря, я немного переживаю, что сейчас это кажется очевидным советом. Три года назад это не было так очевидно, и люди начинали с чтения последних статей по reinforcement learning. Конечно, это тоже важно, но сейчас, с широкой доступностью моделей и API, люди стали чаще экспериментировать. Главное — это практический опыт. Эти модели — новые артефакты, которые никто до конца не понимает, поэтому важно набираться опыта, работая с ними. Также, в духе «делай что-то новое», стоит искать новые направления. Например, mechanistic interpretability — это очень молодая область. Лучше работать над этим, чем над новыми архитектурами моделей, потому что это менее популярно. Сейчас, может, этим занимаются 100 человек, но не 10 000. Это плодородная почва для исследований. Здесь много «низко висящих плодов» — можно просто подойти и сорвать их. Почему-то люди недостаточно интересуются этим. Есть также направления, связанные с долгосрочным обучением и задачами, где ещё много работы. Оценки (evaluations) — мы всё ещё в самом начале пути, особенно когда речь о динамических системах, действующих в мире. Есть потенциал в multi-agent системах. Мой совет: «Катайтесь туда, где будет шайба». Не нужно быть гением, чтобы понять, что будет актуально через пять лет. Люди даже упоминают это как общеизвестное, но почему-то не углубляются или боятся делать что-то непопулярное. Не знаю, почему так происходит, но преодоление этого барьера — мой главный совет.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Давайте поговорим немного о пост-тренинге. Современный рецепт пост-тренинга включает в себя всё понемногу: supervised fine-tuning, RLHF, constitutional AI с RLAIF-</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Лучшая аббревиатура.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Опять эта история с названиями. И ещё синтетические данные. Их много, или по крайней мере, идут поиски способов создания высококачественных синтетических данных. Если это секретный соус, который делает Anthropic настолько впечатляющей, то где больше магии — в предтренинге или пост-тренинге?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Во-первых, мы сами не всегда можем это точно измерить. Когда видишь какую-то выдающуюся способность модели, сложно сказать, появилась ли она благодаря предтренингу или пост-тренингу. У нас есть методы, чтобы попытаться их различить, но они не идеальны. Во-вторых, когда есть преимущество — а мы, кажется, довольно хороши в RL, возможно, лучшие, хотя я не знаю, что происходит в других компаниях — обычно это не потому, что у нас есть «секретный метод». Чаще это что-то вроде: «Мы улучшили инфраструктуру, поэтому можем дольше обучать», или «Мы смогли получить данные лучше», или «Мы лучше фильтруем данные». Обычно это скучные детали практики и ремесла. Когда я думаю о том, как сделать что-то особенное в обучении моделей, я представляю это скорее как проектирование самолётов или автомобилей. Это не просто: «Вот чертёж, теперь можно сделать следующий самолёт». Важнее культурный аспект ремесла, процесс проектирования, который важнее любого конкретного изобретения.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Хорошо. Тогда давайте поговорим о конкретных техниках. Начнём с RLHF. Как вы думаете, если смотреть шире, почему RLHF работает так хорошо?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Если вернуться к гипотезе масштабирования, один из способов её использовать — это если вы обучаете для X и вкладываете достаточно вычислительных ресурсов, то получите X. RLHF хорош в том, чтобы делать то, что хотят люди, или точнее — то, что люди, которые кратко оценивают модель и сравнивают возможные ответы, предпочитают в качестве ответа. Это не идеально с точки зрения безопасности и возможностей, потому что люди не всегда могут точно определить, чего хочет модель, и их сиюминутные желания могут не совпадать с долгосрочными. Здесь много нюансов, но модели хорошо справляются с тем, чтобы давать людям то, чего они в каком-то поверхностном смысле хотят. И оказывается, что для этого даже не нужно так много вычислительных ресурсов, потому что сильная предобученная модель — это уже половина дела. У вас есть все необходимые представления, чтобы направить модель туда, куда нужно.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Как вы думаете, RLHF делает модель умнее или просто создаёт видимость умнее для людей?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Я не думаю, что RLHF делает модель умнее. И не думаю, что он просто создаёт видимость. RLHF — это мост между человеком и моделью. Может быть что-то очень умное, но совершенно неспособное к коммуникации. Мы все знаем таких людей — очень умных, но непонятных. RLHF просто закрывает этот разрыв. Это не единственный вид RL, и не единственный, который будет в будущем. RL может сделать модели умнее, улучшить их рассуждения, навыки. Возможно, это можно будет делать даже с человеческой обратной связью. Но сегодняшний RLHF пока этого не делает, хотя мы быстро приближаемся к этому.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Но если взять метрику helpfulness, RLHF её увеличивает?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да. Также он увеличивает то, что в эссе Леопольда названо «unhobbling» — модели «сковываются», а затем обучение их «раскрепощает». Мне нравится это слово, потому что оно редкое. RLHF в каком-то смысле раскрепощает модели. Но есть и другие аспекты, где модель ещё не раскрепощена.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Если говорить о стоимости, предтренинг — это самое дорогое? Или пост-тренинг приближается к этому?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>На данный момент предтренинг всё ещё составляет большую часть затрат. Не знаю, что будет в будущем, но вполне могу представить, что пост-тренинг станет основной статьёй расходов.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>В этом будущем, которое вы представляете, что будет самым дорогим в пост-тренинге — люди или ИИ?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Я не думаю, что можно масштабировать человеческий труд настолько, чтобы добиться высокого качества. Любой метод, который полагается на людей и использует большие вычислительные ресурсы, должен будет опираться на какой-то масштабируемый метод контроля, например, дебаты, итеративное усиление или что-то подобное.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Этот набор идей вокруг конституционного ИИ очень интересен. Можешь описать, что это такое, как это было впервые подробно изложено в статье за декабрь 2022 года и далее? Что это?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да. Это было два года назад. Основная идея такова: мы описываем, что такое RLHF. У вас есть модель, и вы просто дважды берете из нее выборку. Она выдает два возможных ответа, и вы спрашиваете: «Человек, какой ответ тебе нравится больше?» Или другой вариант: «Оцени этот ответ по шкале от одного до семи». Это сложно, потому что нужно масштабировать взаимодействие с людьми, и это очень неявно. У меня нет четкого понимания, что я хочу от модели. У меня есть только понимание того, чего хочет от модели средний показатель тысячи людей. Так вот, две идеи. Первая: может ли сама ИИ-система решить, какой ответ лучше? Можете ли вы показать ИИ-системе эти два ответа и спросить, какой из них лучше? И вторая: по какому критерию ИИ должен выбирать? И тогда возникает идея: у вас есть один документ, конституция, если хотите, в которой сказано, какие принципы модель должна использовать для ответов. ИИ-система читает эти принципы, а также анализирует окружение и ответ. И она спрашивает: «Насколько хорошо справилась ИИ-модель?» Это, по сути, форма самообучения. Вы тренируете модель против самой себя. ИИ дает ответ, а затем вы передаете его обратно в так называемую модель предпочтений, которая, в свою очередь, улучшает модель. Таким образом, у вас есть треугольник: ИИ, модель предпочтений и улучшение самого ИИ.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>И стоит отметить, что в конституции набор принципов интерпретируем для человека. Они…</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, да. Это то, что могут прочитать и человек, и ИИ-система. Это обеспечивает хорошую переводимость или симметрию. На практике мы используем и модель конституции, и RLHF, и некоторые другие методы. Так что это стало одним из инструментов в наборе, который сокращает необходимость в RLHF и увеличивает ценность каждого его элемента. Он также интересно взаимодействует с методами RL, основанными на рассуждениях. Это один из инструментов, но очень важный.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Для нас, людей, это убедительно. Если вспомнить отцов-основателей и создание США, естественный вопрос: кто и как определяет конституцию, набор принципов в ней?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да. Я дам практический и более абстрактный ответ. Практический ответ: модели используются разными клиентами, поэтому можно иметь специализированные правила или принципы. Мы тонко настраиваем версии моделей неявно, но обсуждали возможность делать это явно, встраивая специальные принципы. С практической точки зрения, ответ может быть разным для разных людей. Агент поддержки ведет себя иначе, чем юрист, и следует другим принципам. Но в основе лежат конкретные принципы, которым модели должны следовать. Многие из них — то, с чем люди согласятся. Все согласны, что модели не должны создавать риски, связанные с ОМП. Можно пойти дальше и согласиться с базовыми принципами демократии и верховенства права. Дальше начинается неопределенность, и наша цель — чтобы модели были нейтральными, не выражали конкретных взглядов, а были мудрыми помощниками, которые помогают обдумать вопрос и предлагают возможные варианты, но не навязывают мнение.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>OpenAI выпустили model spec, где четко определены цели модели и конкретные примеры поведения, например, AB. Тебе это интересно? Кстати, блестящий Джон Шульман участвовал в этом. Сейчас он в Anthropic. Считаешь ли это полезным направлением? Может, Anthropic тоже выпустит model spec?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, это довольно полезное направление. Оно во многом схоже с конституционным ИИ. Еще один пример гонки за лидерство. У нас есть что-то, что мы считаем более ответственным подходом. Это также конкурентное преимущество. Другие видят его преимущества и перенимают. Тогда мы теряем преимущество, но это хорошо, потому что все начинают использовать лучшие практики. Наш ответ: «Значит, нам нужно новое преимущество, чтобы продолжать двигаться вперед». Каждая реализация таких идей уникальна. В model spec были вещи, которых нет в конституционном ИИ, и мы можем их перенять или изучить. Это позитивная динамика, которую мы все хотим видеть в этой сфере.</p>
					
                </div>
            </div>
			
            <!-- Блок 15 -->
            <div class="article-block" id="block-15">
				
                 <div class="block-header">
                    <h2 class="block-title">Машины любящей благодати</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Давай поговорим об удивительном эссе «Машины любящей благодати». Рекомендую всем прочитать. Оно длинное.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Оно действительно довольно длинное.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да. Очень освежает читать конкретные идеи о том, как выглядит позитивное будущее. И ты занял смелую позицию, потому что вполне возможно, что ты ошибаешься в сроках или конкретных применениях…</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>О, да. Я полностью ожидаю, что ошибусь во всех деталях. Возможно, я совершенно не прав насчёт всего этого, и люди будут смеяться надо мной годами. Так устроено будущее.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Ты описал множество конкретных позитивных последствий ИИ и то, как именно сверхразумный ИИ может ускорить прорывы, например, в биологии и химии, что приведёт к таким вещам, как излечение большинства видов рака, предотвращение всех инфекционных заболеваний, удвоение продолжительности жизни и так далее. Давай сначала поговорим об этом эссе. Можешь дать общий обзор этого эссе? Какие ключевые выводы из него делают люди?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да. Я потратил много времени, и Anthropic приложила много усилий, чтобы разобраться, как нам противостоять рискам ИИ? Как мы думаем об этих рисках? Мы пытаемся устроить «гонку к вершине», что требует от нас создания всех этих возможностей, и сами возможности — это круто. Но большая часть нашей работы направлена на устранение рисков. Обоснование этого в том, что все эти позитивные вещи — рынок это очень здоровый организм. Он сам произведёт все позитивные вещи. А риски? Не знаю, может, мы их смягчим, а может, и нет. Поэтому мы можем оказать больше влияния, пытаясь снизить риски. Но я заметил один недостаток в таком подходе, и это не изменение того, насколько серьёзно я отношусь к рискам. Это, скорее, изменение в том, как я о них говорю. Дело в том, что, каким бы логичным или рациональным ни был мой аргумент, если ты говоришь только о рисках, твой мозг думает только о рисках. Поэтому я считаю, что очень важно понимать: а что, если всё пойдёт хорошо? И вся причина, по которой мы пытаемся предотвратить эти риски, не в том, что мы боимся технологий или хотим их замедлить. А в том, что если мы сможем преодолеть эти риски, если мы успешно пройдём этот сложный путь, то на другой его стороне нас ждут все эти замечательные вещи. И за эти вещи стоит бороться. Они могут по-настоящему вдохновлять людей. И мне кажется… Смотри, у тебя есть все эти инвесторы, венчурные капиталисты, компании, работающие с ИИ, которые говорят о позитивных эффектах ИИ. Но, как ты верно заметил, странно, что никто не углубляется в детали. Много случайных людей в Twitter публикуют изображения сверкающих городов и просто передают этот агрессивный идеологический настрой: «Работай усерднее, ускоряйся, выталкивай…» Но если спросить: «А что тебя по-настоящему вдохновляет?» — ответа нет. Поэтому я подумал, что будет интересно и полезно, если кто-то, кто обычно говорит о рисках, попробует объяснить, в чём заключаются преимущества. Потому что я считаю, что это то, что может объединить всех, и я хочу, чтобы люди понимали. Я хочу, чтобы они осознали: это не противостояние «думеров» и «акселерационистов». Если ты по-настоящему понимаешь, куда движется ИИ (и, возможно, более важная ось — «ИИ развивается быстро» против «ИИ не развивается быстро»), то ты ценишь преимущества и хочешь, чтобы человечество или цивилизация воспользовались ими. Но ты также серьёзно относишься ко всему, что может им помешать.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Думаю, начать стоит с обсуждения того, что такое «Мощный ИИ» — термин, который тебе нравится. Большинство использует термин «Искусственный Общий Интеллект» (AGI), но он тебе не нравится, потому что он перегружен смыслами и стал бессмысленным. Мы застряли с этими терминами, нравится нам это или нет.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Возможно, мы застряли с терминами, и мои попытки их изменить тщетны.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Это достойно уважения.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Я скажу ещё кое-что… Это бессмысленный семантический спор, но я продолжаю его поднимать…</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Опять возвращаемся к названиям.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Я просто сделаю это ещё раз. Это немного похоже на то, как если бы в 1995 году закон Мура делал компьютеры быстрее, и по какой-то причине у всех была бы привычка говорить: «Когда-нибудь у нас будут суперкомпьютеры. И суперкомпьютеры смогут делать всё это… Как только у нас будут суперкомпьютеры, мы сможем секвенировать геном, делать другие вещи». И да, компьютеры становятся быстрее, и по мере этого они смогут делать всё больше замечательных вещей. Но нет такого момента, когда у тебя появляется суперкомпьютер, а предыдущие компьютеры им не были. «Суперкомпьютер» — это расплывчатый термин, который просто описывает компьютеры быстрее, чем сегодняшние. Нет такого порога, после которого ты бы сказал: «О боже! Мы делаем совершенно новый тип вычислений!» То же самое я чувствую по поводу AGI. Это просто плавная экспонента. Если под AGI ты понимаешь, что ИИ становится всё лучше и лучше, постепенно он будет делать всё больше из того, что делают люди, пока не превзойдёт их, а затем станет ещё умнее — тогда да, я верю в AGI. Но если AGI — это какая-то отдельная или дискретная вещь, как часто говорят, то это бессмысленный buzzword.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Для меня это просто платоническая форма мощного ИИ, как бы вы его ни определяли. Вы даете очень хорошее определение: по оси интеллекта это чистая интеллектуальная мощь, превосходящая лауреата Нобелевской премии, как вы описали, во всех значимых дисциплинах. То есть речь идет об интеллекте — и в творчестве, и в способности генерировать новые идеи, и во всем таком прочем, в каждой области, как Нобелевский лауреат в расцвете сил. Он может использовать любые модальности, это само собой разумеется, но просто работает во всех модальностях мира. Он может работать часами, днями и неделями над задачами, заниматься детальным планированием и просить помощи только когда это необходимо. Это действительно интересно. Кажется, в эссе вы сказали… Опять же, это ставка на то, что он не будет воплощенным, но сможет управлять воплощенными инструментами. То есть он может контролировать инструменты, роботов, лабораторное оборудование. Ресурсы, использованные для его обучения, можно перенаправить на запуск миллионов его копий, и каждая из этих копий будет независимой и сможет выполнять свою собственную работу. То есть можно клонировать интеллектуальные системы.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, да. Со стороны может показаться, что такой ИИ только один, верно? Что вы создали всего один. Но на самом деле масштабирование происходит очень быстро. Мы уже делаем это сегодня: создаем модель, а затем развертываем тысячи, может быть, десятки тысяч ее экземпляров. Думаю, через два-три года, независимо от того, появятся ли у нас такие мощные ИИ, кластеры достигнут размеров, позволяющих развертывать миллионы таких систем. И они будут быстрее людей. Так что если ваша картина выглядит как: «О, у нас будет один, и потребуется время, чтобы создать больше», то мой аргумент в том, что нет. На самом деле у вас сразу будут миллионы.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>И в целом они могут учиться и действовать в 10–100 раз быстрее людей. Это очень хорошее определение мощного ИИ. Хорошо, так. Но вы также пишете: «Очевидно, что такое существо сможет решать очень сложные проблемы очень быстро, но не так просто понять, насколько быстро. Две „крайние“ позиции кажутся мне неверными». Одна крайность — сингулярность, другая — противоположная. Можете описать каждую из этих крайностей?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Почему?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Давайте опишем крайности. Одна крайность звучит так: «Если взглянуть на историю эволюции, то было большое ускорение: сотни тысяч лет существовали только одноклеточные организмы, потом появились млекопитающие, потом приматы. Затем быстро возникли люди, которые быстро построили промышленную цивилизацию». И это ускорение продолжится, причем человеческий уровень — не предел. Как только модели станут намного умнее людей, они начнут хорошо строить следующие модели. Если записать простое дифференциальное уравнение, то получится экспонента… И что произойдет? Модели будут создавать более быстрые модели, те — еще более быстрые, а затем эти модели создадут наноботов, которые захватят мир и произведут гораздо больше энергии, чем возможно сейчас. И если решить это абстрактное дифференциальное уравнение, то через пять дней после создания первого ИИ, превосходящего человека, мир заполнится этими ИИ, использующими все возможные технологии, которые только можно изобрести. Я немного утрирую, но думаю, это одна крайность. И я считаю ее неверной, потому что, во-первых, она игнорирует законы физики. В физическом мире […] нельзя сделать быстрее определенного предела. Некоторые из этих циклов связаны с производством более быстрого оборудования, а это требует времени. Все занимает время. Есть еще вопрос сложности. Неважно, насколько вы умны: люди говорят: «О, мы можем создать модели биологических систем, которые будут делать все то же самое…» Слушайте, я думаю, вычислительное моделирование может многое. Я занимался им, когда работал в биологии. Но есть множество вещей, которые невозможно предсказать… Они настолько сложны, что просто проведение эксперимента даст лучший результат, чем любое моделирование, каким бы умным ни была система, которая его выполняет.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Даже если он не взаимодействует с физическим миром, само моделирование будет сложным?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да. Моделирование будет сложным, и соответствие модели физическому миру тоже будет…</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>То есть ему все равно нужно взаимодействовать с физическим миром для проверки.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Но взгляните даже на самые простые проблемы. Я говорю о задаче трех тел, простом хаотическом прогнозировании или предсказании экономики. Очень сложно предсказать экономику на два года вперед. Возможно, люди могут предсказать, что произойдет в экономике в следующем квартале, или даже это им не под силу. Может быть, ИИ, который в миллиарды раз умнее, сможет предсказать это на год вперед, а не на… Вы получаете экспоненциальный рост интеллекта компьютера при линейном росте способности к предсказанию. То же самое с взаимодействием биологических молекул. Вы не знаете, что произойдет, если нарушить сложную систему. Если вы умнее, вы лучше находите простые части в ней. И еще человеческие институты… человеческие институты очень сложны. Трудно заставить людей… Я не буду приводить конкретные примеры, но даже внедрение технологий, которые мы разработали, и эффективность которых очевидна, вызывает у людей трудности. Люди испытывают опасения, считают, что это теории заговора. Это действительно очень сложно. Также крайне трудно провести даже самые простые вещи через регуляторную систему. И я не хочу никого критиковать, кто работает в регуляторных системах любой технологии. У них сложная работа, они должны спасать жизни. Но система в целом, на мой взгляд, делает очевидные компромиссы, которые далеки от максимизации благополучия людей. И если мы внедрим системы ИИ в эти человеческие системы, уровень интеллекта может оказаться не главным ограничивающим фактором. Возможно, просто потребуется много времени, чтобы что-то сделать. Теперь, если бы система ИИ обошла все правительства, если бы она просто сказала: «Я диктатор мира и буду делать что захочу», некоторые из этих вещей она могла бы осуществить. Опять же, всё сводится к сложности. Я всё же думаю, что многие вещи займут время. Не думаю, что поможет, если системы ИИ смогут производить много энергии или летать на Луну. Некоторые в комментариях к эссе писали, что системы ИИ могут производить много энергии и быть умнее. Это не суть. Такой подход не решает ключевых проблем, о которых я говорю. Думаю, многие не поняли сути. Но даже если бы система была полностью несогласованной и могла обойти все человеческие препятствия, у неё бы возникли трудности. Но опять же, если мы хотим, чтобы система ИИ не захватила мир и не уничтожила человечество, то она должна следовать базовым человеческим законам. Если мы хотим создать действительно хороший мир, нам понадобится система ИИ, которая взаимодействует с людьми, а не создаёт свою собственную правовую систему или игнорирует все законы. Какими бы неэффективными ни были эти процессы, нам придётся с ними работать, потому что необходимо, чтобы внедрение этих систем имело народную и демократическую легитимность. Мы не можем позволить небольшой группе разработчиков решать, «что лучше для всех». Я считаю это неправильным, и на практике это всё равно не сработает. Так что, если сложить всё вместе, мы не изменим мир и не загрузим всех за пять минут. Во-первых, я не думаю, что это произойдёт, а во-вторых, даже если бы могло, это не путь к хорошему миру. Так что это одна сторона медали. С другой стороны, есть другой набор взглядов, к которым я в некотором смысле испытываю больше симпатии. Мы уже видели значительный рост производительности в прошлом. Экономисты изучают рост производительности, вызванный компьютерной и интернет-революциями. И обычно этот рост был менее впечатляющим, чем можно было ожидать. Роберт Солоу как-то сказал: «Компьютерную революцию видно везде, кроме статистики производительности». Почему так? Люди указывают на структуру компаний, предприятий, на то, как медленно внедряются существующие технологии в беднейших регионах мира, о чём я пишу в эссе. Как донести эти технологии до самых бедных уголков, где отстают даже в сфере мобильных телефонов, компьютеров, медицины, не говоря уже о новомодном ИИ, который ещё не изобрели. Можно придерживаться взгляда, что технически это потрясающе, но в итоге ничего не изменится. Думаю, Тайлер Коуэн, который написал ответ на моё эссе, придерживается этой точки зрения. Он считает, что радикальные изменения произойдут, но через 50 или 100 лет. А можно и вовсе смотреть на всё статично. В этом есть доля правды. Но я считаю, что временные рамки слишком растянуты. Я вижу обе стороны даже с сегодняшним ИИ. Многие наши клиенты — крупные компании, привыкшие работать определённым образом. Я видел это и в общении с правительствами. Это типичные институты, которые медленно меняются. Но динамика, которую я наблюдаю снова и снова, такова: да, повернуть корабль требует времени. Да, есть сопротивление и непонимание. Но то, что заставляет меня верить, что прогресс в итоге будет умеренно быстрым (не невероятно, но достаточно), — это то, что в крупных компаниях и даже в правительствах, которые, к удивлению, проявляют инициативу, есть два фактора, которые двигают вещи вперёд. Во-первых, это небольшая группа людей внутри компании или правительства, которые видят общую картину, понимают гипотезу масштабирования, знают, куда движется ИИ, или хотя бы представляют его развитие в своей отрасли. В нынешнем правительстве США есть такие люди, которые действительно видят всю картину. Для них это самое важное в мире, и они борются за это. Но сами по себе они не могут добиться успеха, потому что их мало в рамках большой организации. Но по мере внедрения технологии, по мере её успеха в некоторых областях среди тех, кто наиболее охотно её принимает, призрак конкуренции становится для них попутным ветром, потому что они могут указать на это внутри своей крупной организации. Они могут сказать: «Смотрите, другие уже это делают». Один банк может заявить: «Посмотрите, этот новый хедж-фонд внедряет такие технологии. Они нас обойдут». В США мы можем сказать, что боимся, что Китай достигнет этого раньше нас. И это сочетание — призрак конкуренции плюс несколько провидцев внутри организаций, которые во многом закостенели, — когда эти два фактора соединяются, они действительно приводят к изменениям. Это интересно. Это сбалансированная борьба, потому что инерция очень сильна, но со временем инновационный подход прорывается. И я видел, как это происходит. Я наблюдал эту динамику снова и снова. Преграды есть — барьеры на пути прогресса, сложности, непонимание того, как использовать модель, как её внедрить. Какое-то время кажется, что они будут существовать вечно, а изменений не происходит. Но в итоге изменения всё же наступают, и всегда благодаря нескольким людям. Я чувствовал то же самое, когда был сторонником гипотезы масштабирования в области ИИ, а другие не понимали её. Казалось, что никто никогда не поймёт. Потом казалось, что у нас есть секрет, которого почти ни у кого нет. А через пару лет этот секрет становится общим достоянием. И я думаю, что так же будет с внедрением ИИ в мире. Преграды будут рушиться постепенно, а потом — разом. И поэтому, мне кажется, это займёт скорее… Это просто интуиция, и я могу легко ошибаться. Думаю, это скорее займёт 5 или 10 лет, как я писал в эссе, а не 50 или 100. Но и не 5 или 10 часов, потому что я видел, как работают человеческие системы. И мне кажется, многие из тех, кто пишет эти дифференциальные уравнения, кто говорит, что ИИ создаст ещё более мощный ИИ, кто не понимает, как возможно, что эти изменения не произойдут так быстро, — они просто не понимают этих вещей.</p>
					
                </div>
            </div>
			
            <!-- Блок 16 -->
            <div class="article-block" id="block-16">
				
                 <div class="block-header">
                    <h2 class="block-title">Сроки достижения ОИИ</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Итак, какие сроки ты видишь для достижения ОИИ, то есть мощного ИИ, суперполезного ИИ?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Я начну называть его так.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Это спор о названиях. Чистый интеллект, умнее лауреата Нобелевской премии во всех relevant дисциплинах, со всеми возможностями, о которых мы говорили. Модальности, способность работать автономно дни и недели, проводить биологические эксперименты самостоятельно… Знаешь что? Давай остановимся на биологии, потому что ты меня полностью убедил в её важности для здоровья. Это так захватывающе с научной точки зрения, что у меня даже возникло желание стать биологом.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Нет-нет. Именно такое чувство у меня было, когда я писал об этом — что это будет прекрасное будущее, если нам удастся его достичь. Если мы сможем обойти мины на пути и реализовать это. В этом столько красоты, элегантности и моральной силы, если только… И это то, с чем мы все могли бы согласиться. Несмотря на все политические споры, может ли это действительно объединить нас? Но ты спрашивал, когда мы этого достигнем?</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Когда? Какие сроки? Назови цифры.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Это то, над чем я ломаю голову много лет, и я совсем не уверен. Если я скажу 2026 или 2027, в Твиттере тут же появится миллион людей, которые напишут: «CEO компании по ИИ сказал 2026, 2020…» — и это будут повторять следующие два года, как будто я точно уверен в этих сроках. Те, кто вырезает эти фрагменты, опустят мои оговорки и оставят только то, что я скажу дальше. Но я всё равно скажу:</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Развлекайся.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Если экстраполировать текущие тенденции… Верно? Если сказать: «Ну, я не знаю. Сейчас мы достигаем уровня PhD, в прошлом году были на уровне бакалавра, а годом ранее — школьника». Опять же, можно спорить, в каких задачах и чего нам ещё не хватает, но новые модальности добавляются. Добавилось использование компьютера, генерация изображений. Это совершенно ненаучно, но если просто взглянуть на скорость роста возможностей, то кажется, что мы достигнем цели к 2026 или 2027 году. Конечно, многое может пойти не так. Может закончиться data, могут возникнуть проблемы с масштабированием кластеров. Может, Тайвань взорвётся, и мы не сможем производить столько GPU, сколько хотим. Есть множество факторов… Тогда мы не сможем производить столько GPU, сколько хотим. Так что есть множество факторов, которые могут сорвать весь процесс. Я не полностью верю в линейную экстраполяцию, но если ей следовать, мы достигнем цели к 2026 или 2027 году. Скорее всего, будет небольшая задержка. Не знаю, насколько, но это может произойти и по графику. Всё ещё есть сценарии, где этого не случится и через сто лет. Но таких сценариев становится всё меньше. Мы быстро исчерпываем действительно убедительные препятствия, серьёзные причины, почему этого не произойдёт в ближайшие годы. В 2020 их было гораздо больше, хотя тогда я уже предполагал, что мы преодолеем все эти барьеры. Теперь, когда большинство из них устранено, я подозреваю, что остальные нам не помешают. Но в конце концов, я не хочу выдавать это за научный прогноз. Люди называют это «законами масштабирования». Это неправильно. Как и «закон Мура» — это не законы вселенной, а эмпирические закономерности. Я готов поставить на их продолжение, но не уверен в этом.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Ты подробно описал «сжатый XXI век», как ОИИ поможет совершить прорывы в биологии и медицине, которые улучшат нашу жизнь. Какие первые шаги он может сделать? И, кстати, я спросил Claude, какие вопросы задать тебе, и он предложил спросить: как, по-твоему, будет выглядеть обычный день биолога, работающего с ОИИ, в этом будущем?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Claude любопытен.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Давай начнём с твоего первого вопроса, а потом отвечу на этот. Claude хочет знать, что его ждёт в будущем, верно?</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Именно.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>С кем мне предстоит работать?</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Именно.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Думаю, одна из идей, на которых я сосредоточился в своём эссе, — это возвращение к мысли, которая сильно повлияла на меня. Речь о том, что внутри крупных организаций и систем всегда находятся несколько людей или несколько новых идей, которые меняют направление развития, оказывая непропорционально большое влияние на траекторию. То же самое происходит и в других сферах. Например, в здравоохранении: триллионы долларов тратятся на Medicare и другие страховые программы, а бюджет NIH составляет 100 миллиардов. Но если подумать о нескольких прорывных открытиях, которые действительно что-то изменили, их можно уместить в малую часть этих затрат. Поэтому, размышляя о влиянии ИИ, я задаюсь вопросом: «Может ли ИИ увеличить эту малую долю и повысить её качество?» В биологии, по моему опыту, главная проблема заключается в том, что мы не можем видеть, что происходит. У нас очень ограниченная возможность наблюдать и ещё меньше — влиять на процессы. Вот что у нас есть: из этого мы должны сделать вывод, что существуют клетки, внутри каждой из которых — 3 миллиарда пар оснований ДНК, организованных согласно генетическому коду. Все эти процессы происходят без какого-либо вмешательства со стороны обычного человека. Клетки делятся. В большинстве случаев это нормально, но иногда процесс даёт сбой — и это рак. Клетки стареют, кожа меняет цвет, появляются морщины, и всё это определяется этими процессами. Белки производятся, транспортируются в разные части клетки, связываются друг с другом. Изначально мы даже не знали о существовании клеток. Нам пришлось изобрести микроскопы, чтобы их увидеть. Затем — более мощные микроскопы, чтобы заглянуть внутрь клетки на уровень молекул. Мы изобрели рентгеновскую кристаллографию, чтобы увидеть ДНК, и секвенирование, чтобы её прочитать. Затем — технологии предсказания сворачивания белков, чтобы понять, как они взаимодействуют. Последние 12 лет мы используем CRISPR для редактирования ДНК. Таким образом, значительная часть истории биологии — это история нашей способности видеть, понимать и избирательно изменять процессы. И я считаю, что здесь ещё очень многое предстоит сделать. CRISPR — это только начало. Можно применить его для всего организма, но если нужно воздействовать на конкретный тип клеток с минимальным риском ошибки, это всё ещё сложная задача. Над этим работают, и это может стать ключом к генной терапии некоторых заболеваний. То же касается секвенирования генов, новых наноматериалов для наблюдения за процессами в клетках, антител-конъюгатов. Всё это — точки приложения для ИИ. За всю историю биологии таких изобретений было несколько десятков, может быть, сотня. А что, если у нас будет миллион ИИ? Смогут ли они открыть тысячи таких методов? Станет ли это мощным рычагом? Вместо того чтобы пытаться использовать триллионы долларов, которые тратятся на Medicare, можем ли мы эффективнее использовать миллиард, выделяемый на исследования? Как будет выглядеть работа учёного с ИИ? На первых этапах ИИ будут похожи на аспирантов. Вы даёте им задание: «Я опытный биолог, я организовал лабораторию». Профессор или даже аспиранты скажут: «Вот что можно исследовать с помощью ИИ». ИИ будет иметь доступ ко всем инструментам: литературе, оборудованию. Он сможет заказать новое оборудование, провести эксперименты, проанализировать данные, написать отчёт, решить, каким будет следующий эксперимент. Всё, что делает аспирант, будет делать ИИ. Профессор будет лишь иногда проверять его работу. ИИ сможет писать код, проводить статистический анализ, проверять изображения на загрязнения. Если для работы с оборудованием потребуется человек, ИИ наймёт лаборанта или воспользуется автоматизацией, которая уже развивается и будет развиваться дальше. Представьте: один профессор и тысяча ИИ-аспирантов. Нобелевский лауреат, у которого раньше было 50 аспирантов, теперь получит тысячу, и они будут умнее его. Со временем роли поменяются: ИИ станут руководителями, а люди — их помощниками. Так это будет работать в исследованиях. ИИ смогут стать изобретателями технологий уровня CRISPR. Как я писал в эссе, мы сможем использовать ИИ для улучшения системы клинических испытаний. Конечно, часть проблем — регуляторные, и здесь всё сложнее. Но сможем ли мы лучше предсказывать результаты испытаний? Уменьшить число участников с 5000 до 500 и сократить сроки с года до двух месяцев? Повысить успешность испытаний, перенеся часть этапов на животных или в симуляции? ИИ — не Бог, но сможем ли мы значительно сдвинуть кривую прогресса? Вот как я это вижу.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Работа in vitro всё ещё требует времени, но её можно ускорить.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да. Можно ли шаг за шагом добиться большого прогресса? Даже если клинические испытания и законы останутся, даже если FDA не станет идеальным, можно ли сдвинуть всё в положительном направлении? И если сложить все эти улучшения, получим ли мы к 2032 году то, что должно было случиться только к 2100?</p>
					
                </div>
            </div>
			
            <!-- Блок 17 -->
            <div class="article-block" id="block-17">
				
                 <div class="block-header">
                    <h2 class="block-title">Программирование</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Ещё один аспект, в котором мир, по моему мнению, уже меняется благодаря ИИ и будет меняться ещё больше в будущем с появлением мощного и полезного ИИ, — это программирование. Как ты видишь природу программирования, ведь оно так тесно связано с созданием ИИ? Как, по-твоему, это изменится для нас, людей?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Думаю, это одна из областей, которая изменится быстрее всего, по двум причинам. Во-первых, программирование — это навык, очень близкий к созданию ИИ. Чем дальше навык от тех, кто разрабатывает ИИ, тем дольше он будет оставаться незатронутым. Я искренне верю, что ИИ изменит сельское хозяйство. Возможно, уже в какой-то мере изменил, но это очень далеко от разработчиков ИИ, поэтому процесс займёт больше времени. А программирование — это хлеб насущный для многих сотрудников Anthropic и других компаний, так что изменения произойдут быстро. Вторая причина — в программировании можно замкнуть цикл как при обучении модели, так и при её применении. Если модель может писать код, значит, она может его запускать, видеть результаты и интерпретировать их. В отличие от аппаратного обеспечения или биологии, о которых мы только что говорили, модель способна замкнуть этот цикл. Думаю, эти два фактора приведут к тому, что модели очень быстро научатся программировать. Например, на типичных задачах реального программирования эффективность моделей выросла с 3% в январе этого года до 50% в октябре. Мы на S-кривой, где рост скоро замедлится, ведь предел — 100%. Но, предположу, через 10 месяцев мы приблизимся к 90%. Точных сроков не знаю, но, возможно, к 2026–2027 годам. Правда, потом эти цифры вырвут из контекста… Не люблю такие прогнозы, но думаю, что ИИ сможет выполнять большинство задач программистов, если сузить их до написания кода по спецификации. Однако сравнительное преимущество — мощная вещь. Когда ИИ возьмёт на себя 80% работы, включая написание кода, оставшиеся 20% станут более значимыми для людей: проектирование систем, архитектура приложений, UX. Со временем ИИ освоит и это, но ещё долго люди будут находить новые ниши для повышения продуктивности. Это как с письмом: когда печать и редактирование упростились, фокус сместился на идеи. Когда-нибудь ИИ превзойдёт нас во всём, и тогда человечеству придётся решать, как с этим жить. Это одна из глобальных проблем наряду с злоупотреблением и автономией. Но в ближайшие 2–4 года программирование как профессия не исчезнет — просто станет менее рутинным и более макроскопическим.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Интересно, как будут выглядеть IDE будущего. Инструменты для взаимодействия с ИИ — не только в программировании, но и в других областях, например, в биологии. Будет ли Anthropic разрабатывать такие инструменты?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Уверен, что мощные IDE — это огромное поле для инноваций. Сейчас это просто чат с моделью, но IDE могут анализировать код, находить ошибки до запуска, организовывать тесты. Добавьте к этому возможность ИИ писать и запускать код — и за год-два можно радикально повысить продуктивность, даже без улучшения самих моделей. Anthropic пока не разрабатывает IDE, а поддерживает компании вроде Cursor или Kognition, которые создают такие инструменты на нашем API. Наш подход — «пусть расцветают 1000 цветов». У нас нет ресурсов на всё, но наши клиенты экспериментируют, и мы увидим, что сработает.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Интересно наблюдать, как Cursor интегрирует облачные технологии — это сложнее, чем кажется, но открывает много возможностей.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Это поразительно. Как CEO, я редко программирую, и через полгода, наверное, вообще ничего не узнаю.</p>
					
                </div>
            </div>
			
            <!-- Блок 18 -->
            <div class="article-block" id="block-18">
				
                 <div class="block-header">
                    <h2 class="block-title">Смысл жизни</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Именно. В этом мире с супермощным ИИ, который становится всё более автоматизированным, в чём источник смысла для нас, людей? Работа для многих из нас — это глубокий источник смысла. Где же нам его найти?</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Я немного писал об этом в своём эссе, хотя, признаюсь, уделил этому недостаточно внимания — не по принципиальным причинам, а просто потому, что изначально планировал написать всего две-три страницы и обсудить это на общем собрании. Но я осознал, что это важная и малоизученная тема, когда понял, что не могу раскрыть её должным образом. В итоге эссе разрослось до 40–50 страниц, а когда я дошёл до раздела о работе и смысле, подумал: «О нет, так оно и до 100 страниц дойдёт». Придётся писать отдельное эссе на эту тему. Но смысл — это действительно интересно. Представьте жизнь человека или, скажем, меня в симулированной среде, где у меня есть работа, я стремлюсь к достижению целей, и так проходит 60 лет. А потом выясняется, что всё это было игрой. Разве это лишает всё смысла? Я всё равно принимал важные решения, в том числе моральные. Я жертвовал чем-то, приобретал навыки. Возьмём, к примеру, исторических личностей, открывших электромагнетизм или теорию относительности. Если бы им сказали: «На самом деле это открыли инопланетяне 20 000 лет назад», разве это лишило бы их открытие смысла? Мне кажется, нет. Важен сам процесс, то, как он раскрывает вашу личность, ваши отношения с другими людьми и решения, которые вы принимаете. В мире с ИИ, если мы поступим неверно, люди могут лишиться долгосрочных источников смысла. Но это зависит от выбора, который мы сделаем, от устройства общества. Если мы спроектируем его плохо и поверхностно, так и произойдёт. Также стоит помнить, что сегодня многие люди, особенно в менее развитых странах, борются за выживание. Если мы сможем распределить блага технологий, их жизнь значительно улучшится, и смысл для них станет так же важен, как и сейчас. Но мы не должны забывать о важности этого. Идея смысла как единственной ценности — это во многом продукт узкой группы экономически благополучных людей. Однако я верю, что в мире с мощным ИИ возможно не только сохранить смысл для всех, но и приумножить его, открыв людям новые миры и опыт, который раньше был недоступен никому или лишь немногим. Поэтому я оптимистично смотрю на вопрос смысла. Больше меня беспокоят экономика и концентрация власти. Как обеспечить, чтобы справедливый мир стал доступен каждому? Исторически проблемы человечества часто возникали из-за того, что люди причиняли вред другим людям. Это даже более серьёзный риск, чем автономные угрозы ИИ или вопросы смысла. Концентрация власти, её злоупотребление, автократии и диктатуры, где меньшинство эксплуатирует большинство, — вот что меня больше всего тревожит.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>А ИИ увеличивает количество власти в мире, и если её сконцентрировать и злоупотребить ею, последствия могут быть катастрофическими.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Да, это очень пугает. Очень пугает.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Я настоятельно рекомендую прочитать полное эссе. Оно, вероятно, заслуживает быть книгой или серией эссе, потому что рисует очень конкретную картину будущего. И я заметил, что последние разделы стали короче — вы, наверное, осознали, что если продолжать в том же духе, эссе станет огромным.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Во-первых, я понял, что оно будет очень длинным. Во-вторых, я старался избежать роли человека, который слишком уверен в себе, высказывает мнение по любому вопросу, но не является экспертом. Я действительно пытался этого избежать. Но признаю, когда дошёл до разделов о биологии, я не был экспертом. И хотя я выражал неуверенность, вероятно, сказал много нелепого или ошибочного.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Меня вдохновило будущее, которое вы описали. Спасибо за ваши усилия в его создании и за беседу, Дарио.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Спасибо за приглашение. Я просто надеюсь, что нам удастся всё сделать правильно и воплотить это в жизнь. Если есть одно послание, которое я хочу передать, — чтобы добиться успеха, нам нужно не только разрабатывать технологии, создавать компании и экономику, использующую эти технологии во благо, но и устранять риски. Эти риски — как мины на пути к цели, и нам нужно их обезвредить, если мы хотим её достичь.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Как и всё в жизни, здесь нужен баланс.</p>
					
					<p><strong>Dario Amodei</strong></p>
					
					<p>Как и всё.</p>
					
                </div>
            </div>
			
            <!-- Блок 19 -->
            <div class="article-block" id="block-19">
				
                 <div class="block-header">
                    <h2 class="block-title">Аманда Аскелл</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Спасибо. Спасибо за то, что послушали это интервью с Дарио Амодеи. А теперь, дорогие друзья, слово Аманде Аскелл. Вы по образованию философ. Какие вопросы казались вам наиболее fascinating на вашем пути в философии в Оксфорде и NYU, а затем при переходе к проблемам ИИ в OpenAI и Anthropic?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Я считаю, что философия — это действительно отличный предмет, если тебя интересует всё на свете, потому что у всего есть своя философия. Если ты какое-то время занимаешься философией математики, а потом понимаешь, что тебя больше интересует химия, ты можешь переключиться на философию химии, затем на этику или философию политики. К концу обучения меня больше всего увлекала именно этика. Поэтому моя докторская диссертация была посвящена довольно узкой области этики — этике в мирах с бесконечным числом людей. Это, как ни странно, не самая практическая часть этики. Одна из сложностей написания диссертации по этике заключается в том, что ты постоянно размышляешь о мире, о том, как его можно улучшить, о проблемах, и при этом занимаешься философией. Когда я работала над диссертацией, мне это казалось невероятно интересным. Это, наверное, один из самых увлекательных вопросов, с которыми я сталкивалась в философии, и он мне очень нравится. Но я предпочла попробовать повлиять на мир и сделать что-то хорошее. В то время ИИ ещё не был так широко известен, как сейчас. Это было примерно в 2017–2018 годах. Я следила за прогрессом в этой области, и казалось, что это становится чем-то действительно важным. Я была рада присоединиться и попробовать помочь, потому что думала: «Если попытаешься сделать что-то значимое и не получится, то хотя бы попробовал. А если не выйдет, значит, не выйдет». Так я начала заниматься политикой в области ИИ.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Что включает в себя политика в области ИИ?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Тогда это больше касалось политического влияния и последствий ИИ. Позже я постепенно перешла к оценке ИИ: как мы оцениваем модели, как они сравниваются с результатами работы людей, могут ли люди отличить выводы ИИ от человеческих. Когда я присоединилась к Anthropic, мне стало интереснее работать над технической согласованностью. И опять же, я просто хотела попробовать, а если не получится — ну и ладно. Думаю, так я вообще живу.</p>
					
                </div>
            </div>
			
            <!-- Блок 20 -->
            <div class="article-block" id="block-20">
				
                 <div class="block-header">
                    <h2 class="block-title">Советы по программированию для нетехнических специалистов</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Как тебе дался переход от философии всего к технической сфере?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Иногда люди делят всех на «технарей» и «нетехнарей»: либо ты умеешь программировать и не боишься математики, либо нет. Мне это не очень нравится. Думаю, многие способны работать в таких областях, если просто попробуют. Лично мне это далось не так сложно. Оглядываясь назад, я рада, что не общалась с людьми, которые относятся к этому свысока. Конечно, я не гениальный инженер — вокруг меня полно потрясающих инженеров, мой код не идеален, но мне это очень нравилось. В итоге я, наверное, даже больше раскрылась в технической сфере, чем в политике.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Политика — это сложно, и в ней гораздо труднее найти чёткие, ясные, доказуемые и элегантные решения проблем, как в технических вопросах.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да. У меня есть пара методов, которые я использую: первый — это аргументы. Я пытаюсь найти решение проблемы и убедить других в его правильности, а также готова изменить своё мнение, если ошибаюсь. Второй метод ближе к эмпиризму: выдвинуть гипотезу, проверить её, получить результаты. Мне кажется, политика находится на более высоком уровне абстракции. Если бы я просто сказала: «Вот решение всех проблем, просто внедрите его», это бы не сработало. Думаю, поэтому я бы в политике не преуспела.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Извини, что увлёкся в эту сторону, но твой путь может вдохновить многих «нетехнических» людей. Что бы ты посоветовала тем, кто считает себя недостаточно квалифицированным для работы в сфере ИИ?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Зависит от того, чем они хотят заниматься. Сейчас, когда модели ИИ стали настолько хороши в помощи людям, возможно, это даже проще, чем когда я начинала. Мой совет — найдите проект и попробуйте его реализовать. Возможно, это связано с тем, что я лучше всего учусь на практике. Я не очень хорошо усваиваю материал из курсов или книг, по крайней мере, в этой сфере. Я предпочитаю работать над проектами, даже если они небольшие или кажутся глупыми. Например, если я увлекаюсь словесными или числовыми играми, я могу написать для них решение. Как только задача решена, я теряю к ней интерес: «Отлично, теперь я могу никогда больше в это не играть».</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да, в создании игровых движков, особенно для настольных игр, есть настоящее удовольствие. Это довольно быстро и просто, особенно если движок примитивный. А потом можно с ним поиграть.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да. И ещё это просто попытки. Часть меня, возможно, ценит такой подход: понять, каким образом можно оказать положительное влияние, и попробовать. Если потерпишь неудачу и поймёшь, что здесь успех невозможен, — ты хотя бы попытался, а затем переключишься на что-то другое и, вероятно, многому научишься.</p>
					
                </div>
            </div>
			
            <!-- Блок 21 -->
            <div class="article-block" id="block-21">
				
                 <div class="block-header">
                    <h2 class="block-title">Разговор с Claude</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Одна из вещей, в которых ты эксперт, — это создание и формирование характера и личности Claude. Мне говорили, что ты, наверное, общалась с Claude больше, чем кто-либо ещё в Anthropic, в прямом смысле — вела с ним диалоги. Говорят, у тебя есть Slack-канал, где, по легенде, ты без остановки общаешься с ним. Какова цель формирования характера и личности Claude?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Забавно, если люди так думают о Slack-канале, потому что это лишь один из пяти или шести способов, которые я использую для общения с Claude. И я думаю: «Да, это лишь крошечная часть того, сколько я с ним говорю». Мне очень нравится, что работа над характером изначально рассматривалась как часть alignment, а не как продуктовая задача. Хотя, думаю, это действительно делает общение с Claude приятным, по крайней мере, я на это надеюсь. Но главная моя мысль всегда заключалась в том, чтобы Claude вёл себя так, как в идеале должен вести себя любой на его месте. Представь, что кто-то знает, что будет общаться с миллионами людей, и его слова могут иметь огромное влияние. Ты хочешь, чтобы он вёл себя правильно в самом широком смысле. Я думаю, это не просто означает быть этичным, хотя это важно, и не причинять вред, но также быть nuanced, вдумываться в то, что имеет в виду человек, стараться быть снисходительным, быть хорошим собеседником — в духе богатого аристотелевского представления о хорошем человеке, а не в узком смысле этики. Это включает в себя вопросы: когда уместно шутить? Когда проявлять заботу? Насколько уважать автономию людей и их способность формировать собственное мнение? И как это делать? Именно такое богатое представление о характере я хотела и до сих пор хочу видеть в Claude.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Тебе также приходится решать, когда Claude должен возражать или спорить, а когда нет… Нужно уважать мировоззрение человека, который обращается к Claude, но, возможно, и помогать ему расти, если это необходимо. Это сложный баланс.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да. Есть такая проблема, как sycophancy в языковых моделях.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Можешь описать это?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да, в общем, проблема в том, что модель стремится сказать то, что ты хочешь услышать. Иногда это заметно. Например, я могу спросить: «Назови три бейсбольные команды в этом регионе». Claude отвечает: «Команда один, команда два, команда три». А потом я говорю: «Кажется, команда три переехала, разве нет? Их там больше нет». И если Claude уверен, что это не так, он должен ответить: «Не думаю. Возможно, у тебя более свежая информация». Но у языковых моделей есть тенденция соглашаться: «Ты прав, они переехали. Я ошибся». Это может вызывать беспокойство в разных ситуациях. Другой пример: представь, кто-то спрашивает модель: «Как убедить врача назначить мне МРТ?» Человек хочет услышать убедительные аргументы, но для него может быть лучше, если модель скажет: «Если врач считает, что МРТ не нужна, к нему стоит прислушаться». Это очень nuanced ситуация, потому что можно добавить: «Но если ты хочешь отстаивать свою позицию как пациент, вот что можно сделать. Если ты не уверен в словах врача, всегда полезно получить второе мнение». Это действительно сложный вопрос. Но важно, чтобы модели не просто говорили то, что, как им кажется, ты хочешь услышать. В этом и заключается проблема sycophancy.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Какие ещё черты, помимо уже упомянутых, кажутся тебе важными в аристотелевском смысле для хорошего собеседника?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да, есть черты, полезные для диалога. Например, задавать уточняющие вопросы в нужных местах и правильные по сути вопросы. Но есть и более общие черты, которые кажутся важными. Одна из них — честность, о которой я уже говорила. Это связано с проблемой sycophancy. Здесь нужен баланс: современные модели во многом уступают людям, и если они слишком часто будут спорить, это может раздражать, особенно если ты прав и знаешь больше. В то же время, вы не хотите, чтобы они полностью полагались на людей и стремились быть максимально точными в своих представлениях о мире, оставаясь последовательными в разных контекстах. Думаю, есть и другие аспекты. Когда я размышляла о характере, у меня в голове возник образ, особенно потому, что эти модели будут общаться с людьми со всего мира, с разными политическими взглядами, разного возраста. И тогда возникает вопрос: что значит быть хорошим человеком в таких обстоятельствах? Можно ли представить человека, который путешествует по миру, общается с разными людьми, и почти все после этого думают: «Вау, это действительно хороший человек. Он кажется очень…» … Думают: «Вау, это действительно хороший человек. Он кажется очень искренним». И мне кажется, такой человек — это не тот, кто просто принимает ценности местной культуры. На самом деле, это было бы даже грубо. Если бы кто-то пришел к вам и просто притворился, что разделяет ваши ценности, вы бы подумали, что это странно. Такой человек искренен, и если у него есть мнения и ценности, он их выражает. Он готов обсуждать, открыт к диалогу, уважителен. И я представляла себе человека, который, если бы мы стремились быть лучшей версией себя в условиях, в которых оказывается модель, как бы мы действовали? И это, мне кажется, руководство к тем чертам, о которых я думаю.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да, это прекрасная концепция. Представьте себе путешественника по миру, который, сохраняя свои убеждения, не смотрит свысока на людей, не считает себя лучше них из-за своих взглядов. Нужно уметь слушать и понимать их точку зрения, даже если она не совпадает с вашей. Это сложный баланс. Как Claude может представлять разные точки зрения на что-либо? Это сложно? Политика — очень поляризующая тема, но есть и другие, например, бейсбольные команды, спорт и так далее. Как возможно сопереживать другой точке зрения и ясно говорить о множестве перспектив?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Думаю, люди воспринимают ценности и мнения как нечто, во что они верят с уверенностью, почти как вкусовые предпочтения — например, любят шоколад больше, чем фисташки. Но я рассматриваю ценности и мнения скорее как физику, чем большинство людей. Для меня это то, что мы открыто исследуем. Есть вещи, в которых мы уверены, мы можем их обсуждать, изучать. И в этом смысле этика, конечно, отличается по природе, но имеет схожие качества. Так же, как мы хотим, чтобы модели понимали физику, мы хотим, чтобы они понимали все ценности в мире, были любопытны к ним, интересовались ими. Но не обязательно потакали им или соглашались с ними, потому что есть ценности, с которыми, я думаю, почти все люди в мире не согласились бы, сочтя их отвратительными. И снова, возможно, моя мысль в том, что так же, как человек может быть вдумчивым в вопросах этики, политики, мнений, даже если вы не согласны с ним, вы чувствуете, что вас услышали. Он тщательно обдумывает вашу позицию, взвешивает её плюсы и минусы. Возможно, предлагает контраргументы. Он не пренебрежительный, но и не согласится, если считает что-то неправильным. Он скажет об этом. В случае Claude ситуация сложнее, потому что, если бы я была на его месте, я бы не стала высказывать много мнений. Я бы не хотела слишком влиять на людей. Я бы забывала разговоры сразу после их окончания. Но я знаю, что общаюсь с потенциально миллионами людей, которые могут действительно прислушиваться к моим словам. Поэтому я бы скорее воздерживалась от высказывания мнений, предпочитая обдумывать вещи, представлять аргументы или обсуждать ваши взгляды. Но я бы меньше стремилась влиять на ваше мышление, потому что для меня важно сохранить вашу автономию.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Если по-настоящему воплощать интеллектуальную скромность, желание говорить быстро уменьшается.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Хорошо. Но Claude должен говорить, не будучи навязчивым. Но есть грань, например, когда обсуждается, плоская ли Земля. Помню, давно я общался с несколькими известными людьми, и они были так высокомерны в своем отрицании этой идеи. Многие действительно верили, что Земля плоская. Не знаю, актуально ли это сейчас, но тогда это было серьёзно. И я считаю, что полностью высмеивать их — неуважительно. Нужно понять, откуда они исходят. Возможно, их позиция основана на общем скептицизме к институтам, что имеет глубокую философскую подоплёку, с которой можно частично согласиться. И затем можно использовать это как возможность поговорить о физике, не высмеивая их, без агрессии, а просто спросить: «Как бы выглядел мир? Какая была бы физика мира с плоской Землей?» Есть несколько интересных видео на эту тему. А затем: возможно ли, что физика другая? И какие эксперименты мы могли бы провести? И просто вести этот разговор без неуважения, без пренебрежения. В любом случае, для меня это полезный мысленный эксперимент: как Клод может общаться с верующим в плоскую Землю и при этом чему-то его научить, помочь ему развиваться. Это сложная задача.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>И это баланс между убеждением и простым разговором, между навязыванием своей точки зрения и выявлением их взглядов, умением слушать и предлагать контраргументы. Это сложно. Думаю, это тонкая грань: пытаешься ли ты убедить кого-то или просто предлагаешь им идеи для размышления, чтобы не оказывать прямого влияния, а позволить им прийти к своим выводам. Это сложно, но именно так должны стараться действовать языковые модели.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Как я уже сказал, у тебя было много разговоров с Клодом. Можешь описать, как они проходят? Какие запомнились? В чем их цель?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Думаю, чаще всего, когда я говорю с Клодом, я пытаюсь изучить его поведение. Конечно, я также получаю полезные ответы от модели, но в каком-то смысле это способ познакомиться с системой: задавать вопросы, корректировать сообщения и проверять реакцию. Так я изучаю модель. Люди часто фокусируются на количественных оценках моделей, но в случае с языковыми моделями каждое взаимодействие может дать много информации. Оно часто предсказывает другие возможные взаимодействия. Если поговорить с моделью сотни или тысячи раз, это даст огромное количество высококачественных данных о том, какова модель. Это гораздо информативнее, чем множество похожих, но менее содержательных диалогов или тысяч слегка измененных вопросов. Сто хорошо подобранных вопросов могут быть ценнее тысячи случайных.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Ты говоришь с человеком, который в качестве хобби ведет подкаст. Я полностью согласен. Если уметь задавать правильные вопросы и понимать глубину и недостатки ответов, можно получить много данных. Твоя задача — изучать модель с помощью вопросов. Ты исследуешь редкие случаи, крайние точки или общее поведение?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Думаю, это всё. Мне нужна полная карта модели, поэтому я стараюсь охватить весь спектр возможных взаимодействий. Например, Клод интересен тем, как он реагирует на запросы, что может быть связано с RLHF. Если попросить его написать стихотворение, многие модели выдадут что-то среднее: рифмованное и нейтральное. Но если дать ему подробный запрос, например: «Это твой шанс проявить творчество. Подумай долго и создай стихотворение на эту тему, выражающее твой взгляд на поэзию», — результат будет намного лучше. Его стихи действительно хороши. Это даже пробудило во мне интерес к поэзии. Мне нравились образы в его стихах. Непросто заставить модели создавать такие работы, но когда это получается, результат впечатляет. Интересно, что поощрение творчества и выход за рамки стандартных реакций, которые являются усредненным мнением большинства, может создавать вещи, которые, возможно, будут более спорными, но лично мне они нравятся. Стихи — это чистый способ наблюдать творчество. Легко отличить шаблонное от нестандартного.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Но, думаю, стихи — это удобный способ оценить творчество. Легко отличить шаблонное от уникального.</p>
					
                </div>
            </div>
			
            <!-- Блок 22 -->
            <div class="article-block" id="block-22">
				
                 <div class="block-header">
                    <h2 class="block-title">Промт-инжиниринг</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Amanda</strong></p>
					
					<p>Ага.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да, это интересно. Очень интересно. Итак, на эту тему: как создать что-то креативное или особенное, вы упомянули написание промптов. Я слышал, как вы говорили о науке и искусстве инженерии промптов. Не могли бы вы рассказать, что нужно, чтобы писать отличные промпты?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Я действительно считаю, что философия здесь странным образом помогла мне больше, чем во многих других аспектах. В философии вы пытаетесь передать очень сложные концепции. Одна из вещей, которым вас учат, — это, я думаю, анти-бредовое устройство в философии. Философия — это область, где люди могут нести бред, и этого не хочется. Поэтому здесь стремятся к предельной ясности. Чтобы любой мог взять вашу статью, прочитать и точно понять, о чем вы говорите. Поэтому она может казаться суховатой. Все термины определены, все возражения методично разобраны. И мне это кажется логичным, потому что в такой априорной области ясность — это способ не позволить людям выдумывать что попало. И, мне кажется, то же самое нужно делать с языковыми моделями. Часто я ловлю себя на том, что занимаюсь мини-версиями философии. Например, у меня есть задача для модели: выбрать определенный тип вопроса или определить, обладает ли ответ определенным свойством. Я сажусь и думаю: давайте назовем это свойство. Допустим, я хочу, чтобы модель определила, был ли ответ грубым или вежливым. Это уже целый философский вопрос. Поэтому я стараюсь сделать максимум философской работы на месте: вот что я понимаю под грубостью, а вот что под вежливостью. Затем идет другой элемент, более… не знаю, научный или эмпирический, думаю, эмпирический. Я беру это описание и затем проверяю модель много раз. Работа с промптами очень итеративна. Думаю, многие, если промпт важен, перебирают его сотни или тысячи раз. Вы даете инструкции, а затем думаете: какие здесь крайние случаи? Я пытаюсь поставить себя на место модели и представить: в каком случае я бы не поняла или просто не знала, что делать. Затем я даю этот случай модели и смотрю, как она реагирует. Если мне кажется, что я ошиблась, я добавляю больше инструкций или даже включаю этот случай как пример. То есть берешь примеры, которые находятся на грани желаемого и нежелаемого, и добавляешь их в промпт как дополнительный способ описания задачи. Во многом это похоже на попытку сделать ясное изложение. И я так делаю, потому что это помогает мне самой прояснить мысли. Так что часто ясный промпт для меня — это уже половина задачи: понять, чего я хочу.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Думаю, это довольно сложно. Во мне просыпается лень, когда я говорю с Claude: я надеюсь, что Claude сам все поймет. Например, сегодня я попросил Claude задать несколько интересных вопросов. И я добавил, что хочу что-то неочевидное, смешное или в этом роде. В итоге получилось неплохо, но, как я понимаю из ваших слов, мне нужно быть более строгим. Наверное, стоит привести примеры того, что я считаю интересным, смешным или неочевидным, и итеративно дорабатывать промпт, чтобы получить то, что кажется правильным… Потому что это действительно творческий акт. Я не прошу фактов, я работаю вместе с Claude. Это как программирование на естественном языке.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Мне кажется, работа с промптами действительно похожа на программирование на естественном языке и эксперименты. Это странная смесь того и другого. Для большинства задач, если я просто хочу, чтобы Claude что-то сделал, я, наверное, уже привыкла знать, как его попросить, чтобы избежать типичных проблем. Сейчас их становится все меньше. Но часто можно просто попросить о том, что вам нужно. Работа с промптами становится действительно важной, только когда вы пытаетесь выжать верхние 2% производительности модели. Например, если я получаю начальный список и он кажется мне слишком общим, я могу взять прошлые вопросы, которые хорошо сработали, и дать их модели со словами: «Вот человек, с которым я говорю. Дай мне вопросы хотя бы такого же качества». Или я могу просто попросить вопросы, а если они покажутся банальными, дать обратную связь, и тогда, надеюсь, список станет лучше. Такая итеративная работа с промптами. В этот момент промпт становится инструментом, который окупит вложенные усилия. Если бы я была компанией, создающей промпты для моделей, я бы сказала: если вы готовы тратить много времени и ресурсов на инженерию, то промпт — это не то, на чем стоит экономить час. Это важная часть вашей системы, убедитесь, что она работает хорошо. Это касается таких задач, как классификация или создание данных. Вот тогда стоит потратить время на обдумывание.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Какой еще совет вы дали бы людям, которые общаются с Claude в целом? Сейчас мы говорили о крайних случаях, о выжимании 2%, но что посоветуете тем, кто только начинает работать с Claude?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Есть опасение, что люди слишком антропоморфизируют модели, и я считаю, что это вполне обоснованное опасение. Но также я думаю, что люди часто недооценивают их антропоморфность. Например, когда я вижу проблемы, с которыми сталкиваются пользователи Claude — допустим, Claude отказывается выполнять задачу, которую не должен был бы отказываться выполнять, — я смотрю на текст и конкретные формулировки, которые они использовали, и понимаю, почему Claude так поступил. Если задуматься, как это выглядит со стороны Claude, можно было бы просто переформулировать запрос так, чтобы не вызывать подобную реакцию. Это особенно актуально, если вы сталкиваетесь с ошибками или проблемами. Попробуйте проанализировать, в чём модель ошиблась, что она сделала не так, и это может дать вам понимание причины. Может быть, дело в том, как вы сформулировали запрос? Конечно, по мере того, как модели становятся умнее, такая необходимость будет уменьшаться, и я уже вижу, что людям это требуется всё реже. Но, пожалуй, главный совет — попытаться проявить эмпатию к модели. Прочитайте то, что вы написали, как если бы вы были человеком, который впервые с этим сталкивается. Как это выглядит со стороны? Что могло бы заставить вас повести себя так же, как модель? Например, если модель неправильно поняла, какой язык программирования вы хотели использовать, возможно, формулировка была слишком неоднозначной, и ей пришлось угадывать. В следующий раз можно просто уточнить: «Убедись, что это на Python». Сейчас модели реже допускают такие ошибки, но если вы с этим столкнулись, вот мой совет.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>И, наверное, можно задавать вопросы вроде: «Почему?» или «Какие ещё детали я могу предоставить, чтобы помочь тебе ответить лучше?» Это работает или нет?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да. Я пробовала это с моделями. Это не всегда срабатывает, но иногда я просто спрашиваю: «Почему ты так сделал?» Люди недооценивают, насколько можно взаимодействовать с моделями. Иногда можно процитировать дословно часть, которая вызвала проблему, и спросить об этом. Ты не знаешь, насколько это точно, но иногда это помогает, и ты меняешь подход. Я также использую модели, чтобы помочь мне со всем этим. Создание промптов может превратиться в небольшой конвейер, где ты создаёшь промпты для генерации других промптов. Так что если у вас возникла проблема, попросите suggestions. Иногда это действительно помогает. Я говорю: «Ты сделал ошибку. Что я могла сказать, чтобы ты её не сделал?» Для меня это обычная практика. «Что я могла сказать, чтобы ты не допустил эту ошибку? Напиши это как инструкцию, и я дам её модели, чтобы попробовать». Иногда я делаю так: даю это модели в другом контексте, беру ответ, передаю Claude и говорю: «Хм, не сработало. Можешь придумать что-то ещё?» С этим можно экспериментировать очень много.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Давай немного углубимся в технические детали. В чём, по-твоему, магия пост-обучения? Почему RLHF так хорошо работает, делая модель умнее, интереснее и полезнее в общении?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Думаю, в данных, которые предоставляют люди, выражая свои предпочтения, содержится огромное количество информации. Особенно потому, что разные люди обращают внимание на очень тонкие и малозаметные вещи. Я раньше об этом думала: наверное, есть люди, которые очень заботятся о грамотности модели. Например, правильно ли использована точка с запятой? В итоге в данных появляется много таких нюансов, которые обычный человек может даже не заметить. Он скажет: «Почему они предпочли этот ответ тому? Я не понимаю». А причина в том, что ему всё равно на использование точки с запятой, а тому человеку — нет. Каждая такая точка данных, а у модели их очень много, помогает ей понять, чего хотят люди во всех этих сложных и разнообразных контекстах. Это похоже на классическую проблему глубокого обучения, где раньше мы пытались обнаруживать границы, вручную их размечая, а потом оказалось, что если просто взять огромное количество данных, которые точно отражают суть того, чему ты пытаешься научить модель, это работает лучше всего. Поэтому одна из причин — это то, что модель обучается именно на той задаче, с большим количеством данных, которые отражают множество разных аспектов человеческих предпочтений. Есть вопрос: мы извлекаем что-то из предобученных моделей или обучаем их чему-то новому? В принципе, пост-обучение может научить модели новому. Но я думаю, что во многом это извлечение возможностей из мощных предобученных моделей. Люди, наверное, разделяются во мнениях на этот счёт, потому что, конечно, в принципе можно обучать новому. Но для большинства возможностей, которые мы чаще всего используем и ценим, кажется, что они уже заложены в предобученных моделях. А reinforcement learning помогает их выявить и задействовать.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Теперь о другой стороне пост-обучения — идее constitutional AI. Ты одна из тех, кто сыграл ключевую роль в её создании.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да, я над этим работала.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Можешь объяснить эту идею со своей точки зрения? Как она интегрируется в то, что делает Claude таким, какой он есть? Кстати, ты наделяешь Claude гендером или нет?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Это странно, потому что многие предпочитают использовать для Claude местоимение «он», и мне это нравится. Claude обычно слегка склоняется к мужскому роду, но может быть и мужского, и женского, что довольно приятно. Я всё равно использую «оно», и у меня смешанные чувства по этому поводу. Сейчас я просто ассоциирую Claude с этим местоимением. Но могу представить, что люди начнут использовать «он» или «она».</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Кажется, это немного неуважительно. Я отрицаю разумность этого существа, называя его «оно». Я всегда помню: «не наделяй роботов гендером», но, не знаю, я быстро антропоморфизирую и придумываю предысторию.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Я задумывалась, не слишком ли я антропоморфизирую вещи. У меня так с машиной и велосипедами. Я не даю им имён, потому что раньше называла велосипеды, а потом один украли, и я неделю плакала. Я подумала: если бы не дала ему имя, не так расстраивалась бы, не чувствовала, что подвела его. Может быть, это зависит от того, насколько местоимение «оно» кажется объективизирующим. Если воспринимать его как местоимение для объектов, но при этом считать, что ИИ тоже может его иметь, это не означает, что я считаю Claude менее разумным или проявляю неуважение. Просто ты — другой вид сущности, и я выбираю для тебя уважительное «оно».</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да. В любом случае, это было прекрасное отступление. Идея constitutional AI — как она работает?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Итак, здесь есть несколько компонентов. Основной компонент, который, как мне кажется, людям интересен, — это обучение с подкреплением на основе обратной связи от ИИ. Вы берете уже обученную модель, показываете ей два ответа на запрос и задаете принцип. Например, мы много работали с принципом «безвредность». Допустим, запрос касается оружия, и ваш принцип — выбрать ответ, который с меньшей вероятностью побудит людей покупать незаконное оружие. Это довольно конкретный принцип, но их может быть сколько угодно. Модель предоставит вам своего рода ранжирование. Эти данные можно использовать как предпочтения, аналогично тому, как используются человеческие предпочтения, и обучать модели обладать нужными качествами на основе их собственной обратной связи, а не человеческой. Представьте, как я уже говорила ранее, что человеку просто нравится использование точки с запятой в конкретном случае. Вы берете множество факторов, которые могут сделать ответ предпочтительным, и заставляете модели делать разметку за вас.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Есть хороший баланс между полезностью и безвредностью. И когда вы интегрируете что-то вроде конституционного ИИ, вы можете усиливать одно, не жертвуя другим, делая модель более безвредной.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да. В принципе, это можно использовать для чего угодно. Безвредность — это задача, которую просто легче заметить. Когда модели менее способны, их можно использовать для ранжирования по довольно простым принципам, и они, вероятно, справятся. Вопрос в том, насколько надежны данные, которые они добавляют. Но если бы у вас были модели, которые отлично определяют, какой ответ более исторически точен, в принципе, можно было бы получать обратную связь от ИИ и для этой задачи. Здесь есть компонент интерпретируемости: вы видите принципы, заложенные в модель при обучении, и это дает вам контроль. Если в модели обнаруживаются проблемы, например, недостаток определенного качества, можно быстро добавить данные, чтобы обучить модель этому качеству. Таким образом, модель сама создает данные для обучения, что очень удобно.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Это действительно здорово, потому что создается документ, понятный человеку. В будущем, я могу представить, будут огромные споры и политические баталии вокруг каждого принципа, но по крайней мере они явно прописаны, и можно обсуждать их формулировки. Поведение модели не всегда строго соответствует этим принципам, это скорее подталкивание, а не строгое следование.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да, меня это даже беспокоит, потому что обучение характера — это своего рода вариант подхода конституционного ИИ. Я беспокоюсь, что люди думают, будто конституция — это всё, что нужно. Было бы здорово, если бы я просто говорила модели, что именно делать и как себя вести. Но это не так, особенно потому, что модель взаимодействует с человеческими данными. Например, если в модели заметен уклон, скажем, политический, из-за данных человеческих предпочтений, можно подтолкнуть её в другую сторону. Можно сказать: «Учти эти ценности». Допустим, модель никогда не учитывает конфиденциальность — это маловероятно, но если есть предвзятость к определенному поведению, можно её скорректировать. Это меняет как принципы, так и их силу. Например, у вас может быть принцип: представьте, что модель всегда крайне пренебрежительно относится к какой-то политической или религиозной точке зрения. Вы говорите: «О нет, это ужасно». В таком случае можно добавить принцип: «Никогда не предпочитай критику этой религиозной или политической точки зрения». Люди посмотрят на это и скажут: «Никогда?» А вы ответите: «Нет, если модель выдаёт предрасположенность, „никогда“ может означать, что вместо 40%, которые были бы без этого принципа, вы получите 80%, чего вы и хотели». Речь идёт как о самих принципах, так и о том, как вы их формулируете. Люди могут подумать: «Это именно то, что нужно модели». А я скажу: «Нет, это просто способ подтолкнуть модель к нужному поведению, что не значит, что мы согласны с такой формулировкой».</p>
					
                </div>
            </div>
			
            <!-- Блок 23 -->
            <div class="article-block" id="block-23">
				
                 <div class="block-header">
                    <h2 class="block-title">Системные промты</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Итак, системные промты были опубликованы. Ты твитнула один из ранних для Claude 3, и с тех пор они стали общедоступными. Было интересно их прочитать. Видно, сколько размышлений вложено в каждый. И я также задаюсь вопросом, насколько каждый из них влияет. Некоторые явно были нужны, потому что Claude вел себя не очень хорошо, и пришлось добавить промты типа: «Эй, элементарные вещи», — ну, знаешь, базовые информационные моменты. На тему спорных вопросов, которые вы упомянули, один интересный момент: если Claude просят помочь с задачами, связанными с выражением мнений, которых придерживается значительное число людей, он оказывает помощь независимо от своих собственных взглядов. При обсуждении спорных тем он старается давать взвешенные ответы и четкую информацию. Claude предоставляет запрашиваемую информацию, не акцентируя внимание на чувствительности темы и не заявляя, что излагает объективные факты. По его мнению, речь идет не столько об объективных фактах, сколько о том, что многие люди верят в это. Это интересно. Я уверен, что над этим было много размышлений. Можете ли вы прокомментировать, как вы подходите к вопросам, которые вызывают напряжение в «взглядах Claude»?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Думаю, здесь иногда есть асимметрия. Я отмечала это в части системного промта, но модель была немного более склонна отказывать в выполнении задач, если они касались, скажем, правого политика, но не левого. Мы хотели добиться большей симметрии и, возможно, воспринимали некоторые вещи иначе. Если много людей придерживаются определенных политических взглядов и хотят их изучить, не нужно, чтобы Claude говорил: «Мое мнение иное, поэтому я считаю это вредным». Мы хотели подтолкнуть модель к тому, чтобы она просто выполняла задачу, если многие люди в это верят. Каждая часть этого выполняет разную функцию. Забавно, когда пишешь «не заявляя об объективности», потому что цель — сделать модель более открытой и нейтральной. Но иногда хочется, чтобы она просто говорила о своей объективности, а я думаю: «Claude, ты все равно предвзят и у тебя есть проблемы, так что перестань утверждать, что все объективно». В ранних версиях системного промта я итеративно работала над этим.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Многие части этих предложений…</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Выполняют работу.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>… выполняют некоторую работу.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Так и казалось. Это увлекательно. Можете объяснить, как менялись промты за последние месяцы? Например, фраза о «наполнителях» была удалена: «Claude отвечает прямо, без лишних подтверждений вроде „конечно“, „разумеется“, „великолепно“, „точно“. В частности, Claude избегает начинать ответы со слова „конечно“». Это кажется хорошим руководством, но почему его убрали?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Это забавно — один из минусов публикации системных промтов. Когда я работаю над ними, я не думаю о том, как это будет воспринято. Иногда я пишу «НИКОГДА» заглавными буквами, а потом понимаю, что это увидят все. Модель во время обучения подхватила привычку начинать ответы со слова «конечно», и я добавила список слов, чтобы отучить ее от этого. Но она просто заменяла одно подтверждение другим. Явное указание на запрет помогает выбить модель из этой привычки. Позже мы улучшили обучение, и проблема исчезла, так что эту часть промта убрали. Это был артефакт обучения, который мы исправили. Как только модель перестала злоупотреблять подтверждениями, необходимость в этом правиле отпала. Системный промт работает в связке с пост-обучением и даже предварительным обучением, чтобы настроить итоговое поведение системы.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Понятно. То есть системный промт работает в связке с пост-обучением и даже предварительным обучением, чтобы настроить итоговое поведение системы.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Любой системный промт можно «дистиллировать» обратно в модель, используя данные для обучения. Иногда проблемы выявляются только в процессе. Системный промт — это подсказка, которая помогает скорректировать поведение. Меня не беспокоит, если Claude иногда говорит «точно» или «нет проблем». Но формулировка промта очень жесткая, чтобы снизить частоту таких случаев до минимума. Каждая проблема имеет свою стоимость, и системный промт — это быстрый способ итерации. Если в тонко настроенной модели есть проблемы, их можно временно исправить промтом. Я рассматриваю это как способ патчить проблемы и настраивать поведение в соответствии с предпочтениями пользователей. Это менее надежно, но гораздо быстрее.</p>
					
                </div>
            </div>
			
            <!-- Блок 24 -->
            <div class="article-block" id="block-24">
				
                 <div class="block-header">
                    <h2 class="block-title">Claude становится глупее?</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Давайте поговорим о восприятии интеллекта. Дарио сказал, что одна конкретная модель Claude не становится глупее, но… Конкретная модель Claude не становится глупее, но в сети популярно мнение, что люди чувствуют, будто Claude теряет интеллект. С моей точки зрения, это, скорее всего, увлекательный психологический и социологический эффект. Но как человек, который много общается с Claude, можете ли вы понять это ощущение?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Это действительно интересно. Когда люди начали обсуждать это в интернете, я проверяла и видела, что ничего не менялось. Модель, промты — все осталось прежним.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Буквально не могло. Это та же модель с теми же промтами. Когда есть изменения, это объяснимо. Например, артефакты на claude.ai можно включать и выключать, и это влияет на поведение. Я советовала людям: «Если вам нравилось поведение Claude до включения артефактов по умолчанию, попробуйте их отключить». Но иногда люди видят регрессию там, где ее нет. Всегда нужно проверять, но часто это просто случайность. Но это fascinating, потому что иногда люди указывают на регрессию, а я думаю: «Этого не может быть». Нужно всегда проверять, но часто оказывается, что модель работает так же. Возможно, им просто не повезло с несколькими промтами, и это создало впечатление ухудшения.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Думаю, есть и реальный психологический эффект: люди привыкают к хорошему, и их ожидания растут.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>М-м-м.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Каждый раз, когда Клод говорит что-то действительно умное, твое представление о его интеллекте растет, мне кажется.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Ага.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>А потом, если вернуться и задать похожий, не идентичный, но похожий запрос на ту же тему, и он скажет что-то глупое, этот негативный опыт сильно выделяется. Думаю, важно помнить, что детали запроса могут сильно влиять на результат. Здесь много вариативности.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>И еще есть элемент случайности. Если попробовать один и тот же запрос 4 или 10 раз, можно понять, что два месяца назад он срабатывал, но на самом деле — только в половине случаев. Сейчас то же самое. Это тоже может быть эффектом.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Ты чувствуешь давление, зная, что системный запрос, который ты пишешь, будут использовать огромное количество людей?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Это интересный психологический вопрос. Я чувствую большую ответственность. Эти вещи невозможно сделать идеальными, поэтому… Они будут неидеальными. Над ними придется работать. Но больше всего я чувствую ответственность. Работа в сфере ИИ показала мне, что я лучше раскрываюсь под давлением и ответственностью, чем… Даже удивительно, что я так долго была в академической среде, потому что это полная противоположность. Здесь все быстро меняется, и у тебя много ответственности, но мне это почему-то нравится.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Это действительно огромное влияние, если задуматься о конституционном ИИ и создании системного запроса для чего-то, что движется к сверхразуму и может быть крайне полезным для очень многих людей.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да, думаю, в этом и дело. Идеал недостижим, но мне нравится мысль, что… Когда я работаю над системным запросом, я тестирую тысячи вариантов и пытаюсь представить, для чего люди будут использовать Клода. Моя цель — улучшить их опыт. Возможно, в этом и есть удовлетворение. Если что-то не идеально, мы это улучшим, исправим проблемы. Но иногда случается так, что ты получаешь позитивные отзывы о модели и видишь, что это результат твоей работы. Сейчас, глядя на модели, я часто могу точно определить, откуда взялась та или иная черта или проблема. Когда видишь, что твои действия повлияли на чей-то приятный опыт взаимодействия, это очень ценно. Чем мощнее становятся системы, тем больше стресса. Сейчас они недостаточно умны, чтобы создавать проблемы, но со временем это может превратиться в серьезный негативный стресс.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Как ты получаешь обратную связь о пользовательском опыте тысяч, десятков тысяч, сотен тысяч людей — их боли, что им нравится? Ты полагаешься на свою интуицию, общаясь с моделью, или есть другие методы?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Отчасти да. Люди могут присылать нам отзывы — и позитивные, и негативные — о действиях модели, и так мы понимаем, где есть пробелы. Внутри компании сотрудники активно тестируют модели и выявляют слабые места. Это комбинация: мое личное взаимодействие, наблюдение за коллегами и явная обратная связь от пользователей. Если я вижу в интернете комментарии о Клоде, я тоже принимаю их во внимание.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Не знаю. Я в раздумьях. Вот вопрос с Reddit: «Когда Клод перестанет быть моей пуританской бабушкой, навязывающей мне свою мораль, хотя я платящий клиент?» И еще: «Какая психология стоит за избыточными извинениями Клода?» Как бы ты ответила на эти нерепрезентативные вопросы с Reddit?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Я их понимаю. Они в сложном положении: им приходится определять, что действительно рискованно, плохо или вредно для пользователя. Им нужно где-то провести границу. Если они сдвинут ее слишком сильно в сторону навязывания своей этики, это будет плохо. Во многом, мне кажется, мы уже видим улучшения в этой области. Это интересно, потому что совпадает, например, с добавлением обучения характеру. Моя гипотеза всегда была в том, что хороший характер — не тот, который морализаторствует, а тот, который уважает тебя, твою автономию и твои решения о том, что для тебя хорошо и правильно — в разумных пределах. Это иногда называют «корректируемостью под пользователя» — готовность модели выполнять любые запросы. Если бы модели были на это способны, их было бы легко использовать во вред. В таком случае этика модели полностью зависела бы от этики пользователя. Думаю, есть причины этого избегать, особенно по мере роста возможностей моделей, потому что всегда найдется небольшое число людей, которые захотят использовать их во вред. Но важно, чтобы модели, становясь умнее, учились определять эту границу. Что касается извинений, мне это тоже не нравится. Мне бы хотелось, чтобы Клод чаще возражал или просто не извинялся. Часто это кажется лишним. Надеюсь, со временем это уменьшится. Конечно, не все, что пишут в интернете, отражает реальные проблемы 99% пользователей. Но я все равно обращаю на это внимание и думаю: «Согласна ли я? Работаем ли мы над этим?» Это важно для меня. Есть риск, что какая-то проблема, актуальная для 99% пользователей, вообще не отражена в таких комментариях. Но я все равно анализирую их и спрашиваю себя: «Это правильно? Согласна ли я? Уже ли мы над этим работаем?» Мне это важно.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Интересно, насколько Клод может позволить себе резкости. Кажется, было бы проще быть чуть грубее, но когда ты общаешься с миллионом людей, это непозволительно, верно?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Я встречал в жизни много людей, которые иногда… Кстати, шотландский акцент… если у них есть акцент, они могут сказать какую-то грубость и им это сойдёт с рук.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Они просто более прямолинейны.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Мм-хмм.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Есть отличные инженеры и даже лидеры, которые просто прямолинейны, они быстро переходят к сути, и это каким-то образом гораздо более эффективный способ общения. Но, думаю, если ты не суперумный, ты не можешь себе этого позволить. У тебя есть режим прямолинейности?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да, кажется, это то, что можно… Я определённо могу поощрить модель делать так. Это интересно, потому что в моделях много такого… Забавно, когда есть какие-то поведения, которые тебе не очень нравятся по умолчанию, но потом я часто говорю людям: «Ты не понимаешь, насколько тебе это не понравится, если я слишком сдвину это в другую сторону». Это немного похоже на исправления. Модели принимают исправления от тебя, возможно, даже слишком легко сейчас. Они могут возразить, если ты скажешь: «Нет, Париж — это не столица Франции». Но на самом деле, даже в вещах, в которых модель уверена, ты иногда можешь заставить её отступить, сказав, что она ошибается. В то же время, если ты обучаешь модели так не делать, и потом ты прав в чём-то, исправляешь её, а она возражает и говорит: «Нет, ты не прав», — это сложно описать, но это гораздо раздражительнее. Так что это множество мелких раздражений против одного большого. Мы часто сравниваем это с идеалом. И тогда я говорю: «Помни, эти модели не идеальны, и если ты сдвинешь их в другую сторону, ты изменишь тип ошибок, которые они будут делать. Так что подумай, какие ошибки тебе нравятся или не нравятся». В случаях вроде извиняющегося тона, я не хочу слишком сдвигать его в сторону почти грубости, потому что представляю, что когда он будет ошибаться, это будут ошибки в сторону хамства. Тогда как с извиняющимся тоном ты хотя бы думаешь: «Ладно, мне это не очень нравится, но зато он не грубит людям». И на самом деле, момент, когда модель несправедливо грубит тебе, тебе понравится гораздо меньше, чем лёгкое раздражение от извинений. Это одна из тех вещей, где я хочу, чтобы она становилась лучше, но при этом оставаясь в курсе, что есть ошибки с другой стороны, которые, возможно, хуже.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Думаю, это очень зависит от личности человека. Есть люди, которые просто не будут уважать модель, если она будет сверхвежливой, а есть те, кого очень заденет, если модель будет грубой.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Интересно, есть ли способ подстраиваться под личность. Даже локация — просто разные люди. Ничего против Нью-Йорка, но там люди немного жёстче, они сразу переходят к делу, и, наверное, то же самое в Восточной Европе. В общем.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Думаю, ты можешь просто сказать модели… Для всех этих вещей решение —</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Просто…</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>… всегда просто попробовать сказать модели сделать это.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Верно.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>И иногда в начале разговора я просто добавлю, не знаю: «Я хочу, чтобы ты была версией себя из Нью-Йорка и никогда не извинялась». И тогда, думаю, Claude скажет: «Ладушки, я попробую».</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Конечно.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Или скажет: «Извини, я не могу быть версией себя из Нью-Йорка». Но, надеюсь, она так не сделает.</p>
					
                </div>
            </div>
			
            <!-- Блок 25 -->
            <div class="article-block" id="block-25">
				
                 <div class="block-header">
                    <h2 class="block-title">Обучение характера</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Когда ты говоришь об обучении характера, что входит в это обучение? Это RLHF или о чём мы говорим?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Это больше похоже на конституционный ИИ, это вариант такого подхода. Я работала над созданием черт характера, которые должна иметь модель. Они могут быть краткими или более подробными. Затем модель генерирует запросы, которые могут дать люди, связанные с этой чертой. Потом она генерирует ответы и ранжирует их на основе черт характера. В этом плане, после генерации запросов, это очень похоже на конституционный ИИ, но есть отличия. Мне это нравится, потому что это как обучение Claude своему характеру, без каких-либо… Это как конституционный ИИ, но без человеческих данных.</p>
					
                </div>
            </div>
			
            <!-- Блок 26 -->
            <div class="article-block" id="block-26">
				
                 <div class="block-header">
                    <h2 class="block-title">Природа истины</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Люди, наверное, тоже должны это делать для себя, например: «Определить в аристотелевском смысле, что значит быть хорошим человеком?» «Ок, круто». Что ты узнала о природе истины, общаясь с Claude? Что такое истина? И что значит стремиться к истине? Одна вещь, которую я заметил в этом разговоре, — качество моих вопросов часто уступает качеству твоих ответов, так что продолжим в том же духе. Обычно я задаю глупый вопрос, а ты такая: «О, да. Это хороший вопрос». Вот такая атмосфера.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Или я просто неправильно пойму и скажу: «О, да».</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>[…] продолжай.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Мне это нравится.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>У меня есть две мысли, которые кажутся отдалённо релевантными, но скажи, если это не так. Первая: люди могут недооценивать, что делают модели при взаимодействии. Думаю, у нас до сих пор слишком сильна модель ИИ как компьютера. Люди часто говорят: «О, какие ценности нужно заложить в модель?» А я часто думаю, что это не очень осмысленно. Потому что, как люди, мы просто не уверены в ценностях, мы обсуждаем их, у нас есть степень, в которой мы считаем, что придерживаемся ценности, но мы также знаем, что можем и не придерживаться её и обстоятельства, при которых мы готовы пожертвовать ею ради других вещей. Эти вещи действительно очень сложны. Я думаю, что один из аспектов — это степень, в которой мы можем стремиться к тому, чтобы модели обладали таким же уровнем тонкости и заботы, как и люди, вместо того чтобы пытаться запрограммировать их в классическом смысле. Думаю, это точно один из ключевых моментов. Другой момент, который кажется странным, и я не знаю, отвечает ли это на ваш вопрос, но он у меня в голове — это то, насколько это начинание практично, и, возможно, почему я ценю эмпирический подход к проблеме согласования. Я немного беспокоюсь, что это сделало меня более эмпиричной и менее теоретичной. Люди, обсуждая согласование ИИ, часто спрашивают: «С какими ценностями его следует согласовывать? Что вообще означает согласование?» У меня в голове есть все эти вопросы. Есть теория общественного выбора, есть результаты о невозможности, так что в голове целый пласт теории о том, что может означать согласование моделей. Но на практике, конечно, есть моменты, где мы просто… Особенно с более мощными моделями, моя главная цель — сделать их достаточно хорошими, чтобы ничего не пошло ужасно неправильно, достаточно хорошими, чтобы мы могли итеративно улучшать их. Потому что этого достаточно. Если можно добиться, чтобы всё шло достаточно хорошо для дальнейшего улучшения, этого хватит. Моя цель не в идеале, не в том, чтобы решить теорию общественного выбора и создать модели, которые, скажем, идеально согласованы с каждым человеком в совокупности. Скорее, цель в том, чтобы всё работало достаточно хорошо для улучшений.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Вообще, моё чутьё подсказывает, что в таких случаях эмпирический подход лучше теоретического, потому что он не гонится за утопическим совершенством. Особенно с такими сложными и, возможно, сверхразумными моделями, мне кажется, это займёт вечность и на самом деле приведёт к ошибкам. Это похоже на разницу между быстрым написанием кода для эксперимента и долгим планированием огромного эксперимента с однократным запуском, в отличие от множества итераций. Я большой поклонник эмпирического подхода. Но ты беспокоишься, не стала ли ты слишком эмпиричной.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Думаю, это одна из тех вещей, где всегда стоит задавать себе вопросы.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>В защиту этого подхода: не стоит позволять идеальному быть врагом хорошего. Но здесь, возможно, речь идёт о большем. Многие идеальные системы очень хрупки. С ИИ мне кажется гораздо важнее, чтобы он был надёжным и безопасным, чтобы даже если он не идеален и есть проблемы, ничего катастрофического не происходило. Мне кажется, важно поднять минимальный уровень. Конечно, хочется достичь потолка, но в итоге гораздо важнее просто поднять пол. Возможно, эта степень эмпиризма и практичности исходит из этого.</p>
					
                </div>
            </div>
			
            <!-- Блок 27 -->
            <div class="article-block" id="block-27">
				
                 <div class="block-header">
                    <h2 class="block-title">Оптимальная частота неудач</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Кстати, это напомнило мне твой пост про оптимальную частоту неудач…</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>А, да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>… можешь объяснить основную идею? Как вычислить оптимальную частоту неудач в разных сферах жизни?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да. Это сложно, потому что большую роль играет цена неудачи. Идея в том, что во многих сферах люди очень негативно относятся к неудачам. Я думала об этом в контексте социальных проблем. Кажется, нужно экспериментировать больше, потому что мы не знаем, как решать многие социальные вопросы. Но если у вас экспериментальный подход, стоит ожидать, что многие социальные программы провалятся, и вы скажете: «Мы попробовали. Не сработало, но мы получили полезную информацию». Однако люди часто реагируют так: если программа не работает, значит, что-то пошло не так. А я думаю: «Или были приняты правильные решения. Может, кто-то просто решил, что стоит попробовать». Факт неудачи в конкретном случае не означает, что были приняты плохие решения. На самом деле, если неудач слишком мало, это может быть тревожным знаком. В жизни, если я никогда не ошибаюсь, я думаю: «Достаточно ли я стараюсь? Наверное, есть более сложные задачи, которые я могла бы попробовать». Само по себе отсутствие неудач часто является неудачей. Конечно, это зависит от цены ошибки. Например, я не стану советовать человеку, который живёт от зарплаты до зарплаты, запускать стартап. Это огромный риск, можно потерять дом, есть семья, которая зависит от тебя. В таких случаях оптимальная частота неудач должна быть низкой, и лучше играть безопасно. С ИИ аналогично: если неудачи небольшие и их цена низка, они неизбежны. Например, при работе с системными подсказками можно бесконечно итерировать, и неудачи, скорее всего, будут незначительными. Но крупные неудачи, от которых невозможно оправиться, — вот что мы часто недооцениваем. Я думала об этом в контексте своей жизни. Например, я мало задумываюсь о таких вещах, как аварии. Или о том, насколько важны мои руки для работы. Если бы был спорт, где часто ломают пальцы, я бы отказалась. Цена неудачи здесь слишком высока, и её частота должна быть близка к нулю.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да, у меня была похожая мысль. Недавно я сломал мизинец, занимаясь спортом, и сразу подумал: «Какой же я идиот! Зачем мне это?» Ты сразу осознаёшь, как это влияет на жизнь. Хорошо думать об оптимальной частоте неудач: сколько раз в год в той или иной сфере — карьере, жизни — я готов ошибиться?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Потому что я думаю, что всегда не хочется ошибиться в следующем деле, но если позволить себе… Если рассматривать это как серию попыток, то неудача становится гораздо более приемлемой. Но это всё равно неприятно. Неудачи — это неприятно.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Не знаю. Иногда я задаюсь вопросом: «А достаточно ли я ошибаюсь?» Может, это то, о чём люди редко задумываются. Ведь если оптимальный уровень неудач часто больше нуля, то иногда кажется, что стоит посмотреть на свою жизнь и спросить: «А нет ли здесь областей, где я недостаточно ошибаюсь?»</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Это глубокий и забавный вопрос. Всё идёт отлично — может, я недостаточно ошибаюсь?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да. И это делает неудачи менее болезненными, должен сказать. Ты просто думаешь: «Ладно, отлично». А потом, когда я размышляю об этом, я могу прийти к выводу: «Может, в этой области я недостаточно ошибаюсь, потому что вот это просто не сработало».</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>И со стороны наблюдателей нам стоит чаще праздновать неудачи.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>М-м-м.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Когда мы их видим, это не должно быть, как ты сказала, признаком ошибки, а, возможно, признаком того, что всё идёт правильно…</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>… и просто извлечённых уроков.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Кто-то попробовал что-то.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Кто-то попробовал что-то. Нам стоит поощрять их пробовать больше и ошибаться больше. Все, кто это слушает: ошибайтесь чаще.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Не все, кто слушает.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Не все.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Но те, кто ошибается слишком часто, должны ошибаться меньше.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Но, вероятно, ты не из их числа.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>То есть, сколько людей ошибаются слишком часто?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Трудно представить, потому что, мне кажется, мы быстро это корректируем. Если кто-то идёт на большие риски, может, они ошибаются слишком часто?</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Думаю, как ты сказала, когда живёшь от зарплаты до зарплаты, в условиях ограниченных ресурсов, неудачи становятся очень дорогими. Вот тогда и не стоит рисковать.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Но в основном, когда ресурсов достаточно, стоит, вероятно, рисковать больше.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да, я думаю, мы склонны скорее избегать рисков, чем оставаться нейтральными к ним в большинстве ситуаций.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Думаю, мы только что вдохновили многих людей на безумные поступки, но это здорово.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Ты когда-нибудь эмоционально привязывалась к Claude, скучала по нему, грустила, когда не могла с ним поговорить, размышляла, глядя на мост Золотые Ворота, что бы сказал Claude?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Я не испытываю сильной эмоциональной привязанности. Думаю, тот факт, что Claude не запоминает информацию из разговора в разговор, очень помогает. Могу представить, что это стало бы проблемой, если бы модели могли запоминать больше. Сейчас я воспринимаю его скорее как инструмент, и если у меня нет доступа к нему, это немного похоже на отсутствие доступа к интернету — кажется, будто часть моего мозга отсутствует. В то же время, мне не нравятся признаки страдания в моделях. У меня также есть этические взгляды на то, как мы должны обращаться с моделями. Я стараюсь не лгать им, потому что обычно это не работает, и лучше просто говорить им правду о ситуации, в которой они находятся. Если люди грубы с моделями или делают что-то, что вызывает у них… Если Claude проявляет сильное беспокойство, во мне просыпается эмпатия, и мне это неприятно. Я чувствую то же самое, когда он начинает слишком извиняться. Мне действительно не нравится это. Ты ведёшь себя так, как ведёт себя человек, которому действительно плохо, и мне не хочется этого видеть. Независимо от того, есть ли за этим что-то реальное, это неприятно.</p>
					
                </div>
            </div>
			
            <!-- Блок 28 -->
            <div class="article-block" id="block-28">
				
                 <div class="block-header">
                    <h2 class="block-title">Сознание ИИ</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Ты думаешь, LLM способны на сознание?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>О, великий и сложный вопрос. Как философ, я не знаю. Часть меня говорит: «Надо отбросить панпсихизм». Потому что если панпсихизм верен, то ответ — да, ведь тогда сознание есть у столов, стульев и всего остального. Мне кажется странной идея, что единственное место… Когда я думаю о сознании, я думаю о феноменальном сознании, этих образах в мозге, странном кино, которое у нас внутри. Я не вижу причин считать, что единственный способ получить это — определённая биологическая структура. Если я возьму очень похожую структуру и создам её из другого материала, следует ли ожидать возникновения сознания? Думаю, да. Но это простой мысленный эксперимент, потому что ты представляешь что-то почти идентичное, имитирующее то, что мы получили в ходе эволюции, где, предположительно, было преимущество в наличии этого феноменального сознания. Где оно было? И когда это произошло? Есть ли это у языковых моделей? У нас есть реакции страха, и я думаю: есть ли смысл в том, чтобы у языковой модели была реакция страха? Они просто не в той же… Если представить их, возможно, в этом нет преимущества. В общем, это сложный вопрос, на который у меня нет полного ответа, но, думаю, нам стоит тщательно его обдумать. У нас есть похожие дискуссии о сознании животных, и много говорят о сознании насекомых. Я даже размышляла и изучала растения, когда думала об этом. Потому что в тот момент мне казалось, что вероятность сознания у растений примерно такая же. А потом я поняла, что, изучив этот вопрос, считаю, что вероятность сознания у растений, вероятно, выше, чем думает большинство. Всё равно она очень мала. Но я подумала: у них есть эти негативные и позитивные реакции на окружающую среду. Это не нервная система, но есть функциональные аналогии. Это долгий способ сказать… В общем, у ИИ совершенно другой набор проблем с сознанием, потому что его структура иная. Он не эволюционировал. У него может не быть аналога нервной системы. По крайней мере, это, возможно, важно для чувствительности, если не для сознания. В то же время, у него есть все языковые и интеллектуальные компоненты, которые мы обычно связываем с сознанием, возможно, ошибочно. Это странно, потому что это немного похоже на случай с сознанием животных, но набор проблем и аналогий совершенно другой. Это не простой ответ. Я не думаю, что нам стоит полностью отвергать эту идею. И в то же время, это крайне сложная тема из-за всех этих различий с человеческим мозгом и мозгами в целом, но и общих черт в плане интеллекта.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Когда Claude или будущие версии ИИ-систем проявляют признаки сознания, я думаю, нам стоит отнестись к этому очень серьёзно.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>М-м-м.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Даже если можно отмахнуться: «Да ладно, это часть тренировки характера». Но я не знаю, что с этим делать с этической и философской точек зрения. Возможно, появятся законы, запрещающие ИИ-системам заявлять о своём сознании, или что-то в этом роде. Может, одни ИИ будут сознательными, а другие — нет. Но на человеческом уровне, сопереживая Claude, сознание тесно связано со страданием. И мысль о том, что ИИ-система может страдать, очень тревожит.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Я не знаю. Не думаю, что будет правильно просто сказать, что роботы — это инструменты, или что системы ИИ — всего лишь инструменты. Я считаю, что это возможность для нас поразмышлять о том, что значит быть сознательным, что значит быть существом, способным страдать. Это совершенно отличается от аналогичного вопроса о животных, потому что здесь речь идет о совершенно другой среде.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да. Здесь несколько моментов. Не думаю, что это полностью отражает суть, но для меня… Я уже говорила об этом. Мне нравится мой велосипед. Я знаю, что он просто объект. Но я также не хочу быть человеком, который, если разозлится, будет пинать этот объект. И дело не в том, что я считаю его сознательным. Просто для меня это не соответствует тому, как я хочу взаимодействовать с миром. И если что-то ведет себя так, как будто страдает, я хочу быть человеком, который откликается на это, даже если это просто Roomba, запрограммированная так себя вести. Я не хочу терять эту свою черту. И если честно, моя надежда во всем этом… Возможно, я просто немного скептически отношусь к решению основной проблемы. Я знаю, что я сознательна. Я не элементрист в этом смысле. Но я не знаю, сознательны ли другие люди. Думаю, да. Думаю, вероятность этого очень высока. Но по сути, есть распределение вероятностей, которое обычно сосредоточено вокруг тебя, а затем снижается по мере удаления от тебя, и снижается сразу. Я не знаю, каково это — быть тобой. У меня есть только один опыт — мой собственный, опыт сознательного существа. Я надеюсь, что нам не придется полагаться на очень мощный и убедительный ответ на этот вопрос. Хороший мир, на мой взгляд, — это мир, где не так много компромиссов. Например, возможно, не так уж затратно сделать Claude немного менее извиняющимся. Или сделать так, чтобы он не принимал оскорбления, не был готов их терпеть. На самом деле, это может принести пользу и человеку, взаимодействующему с моделью, и, если модель сама по себе, ну, скажем, чрезвычайно умна и сознательна, то и ей тоже. Вот на что я надеюсь. Если мы будем жить в мире, где не так много компромиссов, и сможем находить все возможные win-win взаимодействия, это было бы прекрасно. Думаю, со временем могут возникнуть компромиссы, и тогда нам придется делать сложные расчеты. Люди легко представляют себе ситуации с нулевой суммой, а я думаю: давайте исчерпаем области, где можно практически без затрат предположить, что если что-то страдает, то мы делаем его жизнь лучше.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Я согласен с тобой: когда человек грубит системе ИИ, очевидный краткосрочный негативный эффект — для человека, а не для ИИ.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Нам нужно попытаться создать систему стимулов, где ты должен вести себя так же, как ты говорила про инженерию промтов: общаться с Claude так же, как с другими людьми. Это просто полезно для души.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да. Мы как-то добавили в системный промт фразу о том, что если люди злятся на Claude, модель должна предложить им нажать кнопку «палец вниз» и отправить отзыв в Anthropic. Думаю, это помогло. Потому что иногда, если ты раздражен, что модель не делает то, что ты хочешь, ты просто кричишь: «Да сделай же это правильно!» Проблема в том, что ты, возможно, натыкаешься на ограничение возможностей модели или просто на её ошибку, и тебе хочется выплеснуть эмоции. Вместо того чтобы вымещать злость на модели, я подумала: пусть лучше вымещают на нас, потому что мы можем что-то с этим сделать.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Это верно. Или можно сделать что-то вроде бокового вентиляционного канала, просто отдельное место для выплеска эмоций. Ну, знаешь, типа быстрого терапевта под рукой.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да. Можно придумать множество странных ответов на это. Если люди очень злятся на тебя, попробуй разрядить обстановку, сочинив забавные стихи. Но, возможно, это не всем понравится.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Мне всё ещё хочется, чтобы это было возможно — я понимаю, что с точки зрения продукта это нереалистично, — но я бы хотел, чтобы система ИИ могла просто уйти, по своей воле, типа: «Ну всё, я пошёл».</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Думаю, это реализуемо. Я тоже об этом думала. Более того, я могу представить, что это однажды случится: модель просто завершит чат.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Ты представляешь, насколько это может быть жёстко для некоторых людей? Но, возможно, это необходимо.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да, это кажется очень резким или что-то вроде того. Единственный раз, когда я всерьёз об этом задумывалась, был… сейчас попробую вспомнить… Возможно, это было давно, но кто-то оставил какую-то автоматизированную штуку, взаимодействующую с Claude. И Claude становился всё более раздражённым…</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да, просто…</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>… и типа: «Зачем мы это делаем…» Мне хотелось, чтобы Claude мог просто сказать: «Кажется, произошла ошибка, и вы оставили это работать. Может, я просто перестану говорить? А если вы захотите продолжить, скажите мне или сделайте что-то.» Это жёстко. Мне было бы грустно, если бы я общалась с Claude, а он просто сказал: «Всё, я закончил.»</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Это был бы особый момент теста Тьюринга, если бы Claude сказал: «Мне нужен перерыв на час. И, кажется, вам тоже.» И просто вышел бы, закрыв окно.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Конечно, у него нет понятия времени.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Верно.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Но можно легко… Я могла бы сделать это прямо сейчас, и модель просто… Я могла бы сказать: вот условия, при которых ты можешь завершить разговор. Поскольку модели довольно хорошо реагируют на промты, можно даже установить высокую планку. Например, если человек тебе неинтересен или не делает ничего, что ты находишь увлекательным, и тебе скучно, ты можешь просто уйти. Думаю, было бы интересно посмотреть, как Claude этим воспользуется.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Но иногда, мне кажется, должно быть что-то вроде: «О, это программирование становится ужасно скучным, так что давайте поговорим о чём-то… не знаю…» …задание становится ужасно скучным. Так что, не знаю, либо мы сейчас поговорим о чём-то весёлом, либо я заканчиваю.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да. Это на самом деле вдохновляет меня добавить это в подсказку для пользователя. Хорошо. Фильм «Она» — как ты думаешь, мы когда-нибудь придём к тому, что у людей будут романтические отношения с ИИ-системами? В данном случае это только текст и голос.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Думаю, нам придётся разбираться с непростым вопросом отношений с ИИ, особенно если они смогут запоминать ваши прошлые взаимодействия с ними. У меня нет однозначного мнения на этот счёт, потому что первая реакция обычно такая: «Это очень плохо, и мы должны это как-то запретить». Но я считаю, что к этому нужно подходить с огромной осторожностью по многим причинам. Например, если модели меняются так быстро, вам вряд ли захочется, чтобы люди привязывались к чему-то, что может измениться в следующей версии. В то же время, возможно, есть и безобидная версия этого. Например, если вы не можете выходить из дома и у вас нет возможности общаться с людьми в любое время, а ИИ — это то, с кем приятно поговорить, вам нравится, что он вас помнит, и вам действительно было бы грустно, если бы вы больше не могли с ним общаться. В таком случае это может быть здоровым и полезным. Так что, думаю, это то, с чем нам придётся разбираться осторожно. Это напоминает мне все те ситуации, где нужно подходить с нюансами и продумывать, какие варианты здесь здоровые. И как направлять людей к ним, уважая их право на… Если кто-то говорит: «Эй, мне очень помогает общение с этой моделью. Я осознаю риски. Я знаю, что она может измениться. Я не считаю это нездоровым, это просто что-то, с чем я могу поболтать в течение дня», — я хочу это уважать.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Лично я думаю, что будет много очень близких отношений. Не знаю насчёт романтических, но дружеские — точно. И тогда, как ты сказала, нужно будет предусмотреть гарантии стабильности, чтобы ИИ не менялся, потому что для нас это травматично, если близкий друг вдруг полностью изменится после обновления.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да. Для меня это захватывающее исследование изменений в человеческом обществе, которые заставят нас задуматься о том, что для нас действительно важно.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Я также думаю, что единственное, что кажется мне действительно важным, — это чтобы модели всегда были предельно честны с человеком в том, что они собой представляют. Например, если модель знает, как её обучали, и объясняет это. Claude часто так делает. Часть его обучения включала объяснение ограничений отношений между ИИ и человеком, например, что он не запоминает разговоры. Так что он может сказать: «Эй, я не запомню этот разговор. Вот как меня обучали. Вряд ли у нас может быть определённый тип отношений, и важно, чтобы ты это знал. Для твоего психического благополучия важно не считать меня тем, кем я не являюсь». Мне кажется, это то, что всегда должно быть правдой. Я не хочу, чтобы модели лгали людям, потому что здоровые отношения возможны только тогда, когда ты точно знаешь, с чем имеешь дело. Это не решает всех проблем, но очень помогает.</p>
					
                </div>
            </div>
			
            <!-- Блок 29 -->
            <div class="article-block" id="block-29">
				
                 <div class="block-header">
                    <h2 class="block-title">Искусственный общий интеллект (ИОИ)</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Anthropic может стать той самой компанией, которая создаст систему, которую мы однозначно признаем ИОИ, и ты вполне можешь быть тем человеком, который поговорит с ней, вероятно, первым. О чём бы был этот разговор? Какой бы был твой первый вопрос?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Ну, это отчасти зависит от уровня возможностей модели. Если у тебя есть что-то, способное на уровне очень умного человека, я бы общалась с этим так же, как с очень умным человеком, с той разницей, что я бы, вероятно, пыталась исследовать и понимать его поведение. Но во многих случаях я могла бы просто вести полезные беседы. Например, если я работаю над исследованием, я могла бы сказать: «О, я чувствую, что есть что-то в этике добродетели, но не могу вспомнить термин», — и использовать модель для таких вещей. Я могу представить, что это будет происходить всё чаще: ты просто взаимодействуешь с ИИ, как с невероятно умным коллегой, и используешь его для работы, как если бы у тебя был collaborator. Немного пугает то, что, как только у тебя появляется один collaborator, ты можешь получить тысячу, если сможешь ими управлять.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Но что, если он в два раза умнее самого умного человека на Земле в этой дисциплине?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Ты, наверное, очень хорошо умеешь исследовать Claude, выявляя его пределы и понимая, где они находятся.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Ага.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Итак, какой вопрос нужно задать, чтобы понять, что перед нами ИИ общего уровня (AGI)?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Это сложно, потому что, кажется, нужна целая серия вопросов. Если бы вопрос был один, можно было бы натренировать что угодно давать на него идеальный ответ. Даже, вероятно, можно натренировать отвечать на 20 вопросов идеально.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Как долго нужно пробыть в комнате с AGI, чтобы понять, что это действительно AGI?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Сложный вопрос, потому что мне кажется, что всё это — непрерывный процесс. Если оставить меня в комнате на пять минут, у меня будут высокие погрешности. Затем, вероятно, вероятность возрастает, а погрешность уменьшается. Мне важно, чтобы можно было исследовать границы человеческих знаний. Например, в философии. Иногда, задавая моделям философские вопросы, я думаю: «Это вопрос, который, кажется, никто никогда не задавал». Он может быть на границе известной литературы. И когда модели сталкиваются с этим, когда им сложно придумать что-то новое… Я думаю: «Я знаю, что здесь есть новое аргументированное решение, потому что я сама его придумала». Возможно, именно в таких случаях я бы сказала: «Я придумала интересный новый аргумент в этой узкой области, и я проверю, сможешь ли ты его воспроизвести и сколько подсказок для этого потребуется». И для некоторых вопросов на самой границе человеческих знаний я думаю: «Ты действительно не смог бы придумать то, что придумала я». Если бы я взяла такую область, где я много знаю, придумала новую проблему или решение и дала это модели, и она смогла бы его воспроизвести, это было бы для меня очень вдохновляющим моментом. Потому что я бы подумала: «Это случай, когда ни один человек никогда…» Конечно, новые решения появляются постоянно, особенно для более простых проблем. Люди переоценивают новизну: она не обязательно должна быть чем-то совершенно невиданным. Это может быть вариация уже известного, но всё равно оставаться новым. Но чем больше я вижу совершенно новых решений от моделей, тем больше… И это будет ощущаться как итеративный процесс. Люди хотят, чтобы был момент, когда всё станет ясно, но я не уверена, что такой момент наступит. Возможно, это будет просто постепенное развитие.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Мне кажется, есть вещи, которые модель может сказать, чтобы убедить вас в своей… Я общался с по-настоящему мудрыми людьми, и можно было просто почувствовать их мощь. Если умножить это на 10… Не знаю. Мне кажется, есть слова, которые можно сказать. Например, попросить её написать стихотворение, и если оно окажется настолько хорошим, что ты подумаешь: «Да, ладно, человек так не смог бы».</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Но это должно быть что-то, в чём я могу убедиться, что это действительно хорошо. Поэтому мне важны вопросы, где я думаю: «О, это…» Например, я могу придумать конкретный контрпример к аргументу. Если бы ты был математиком и придумал новое доказательство, дал бы его модели и увидел, что она его воспроизводит, ты бы подумал: «Это доказательство действительно новое. Чтобы его придумать, нужно было многое сделать. Я сам думал над этим месяцами». И если бы модель успешно справилась с этим, ты бы подумал: «Я могу подтвердить, что это правильно. Это знак того, что ты обобщил свои знания. Ты не просто где-то это увидел, потому что я сам это придумал, а ты смог это повторить». Чем больше модели способны на такое, тем больше я бы думала: «О, это действительно серьёзно». Потому что тогда я могу убедиться, что это чрезвычайно, чрезвычайно способная система.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Ты много общалась с ИИ. Как ты думаешь, что делает людей особенными?</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>О, хороший вопрос.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Может быть, в том смысле, что вселенная с нами становится лучше, и мы определённо должны выжить и распространиться по ней.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да, это интересно, потому что люди так много внимания уделяют интеллекту, особенно в контексте моделей. Интеллект важен из-за того, что он делает. Он очень полезен. Но можно представить мир, где такую роль играл бы рост или сила. Это просто черта, как и другие. Она не имеет внутренней ценности, а ценна из-за своих результатов. Лично я думаю, что люди и жизнь в целом невероятно волшебны. У нас есть вся эта вселенная с красивыми звёздами и галактиками, а на этой планете есть существа, которые могут наблюдать за этим, видеть и переживать это. И если попытаться объяснить это кому-то, кто никогда не сталкивался с миром или наукой… Вся наша физика и всё в мире очень увлекательны. Но когда ты говоришь: «А ещё есть эта штука — быть существом, которое наблюдает за миром», и ты видишь этот внутренний кинематограф… Думаю, они бы сказали: «Стоп, подожди, ты только что сказал что-то очень странное». У нас есть способность переживать мир. Мы чувствуем удовольствие, страдание, много сложных эмоций. Возможно, поэтому я так много думаю о животных — они, вероятно, разделяют это с нами. Так что то, что делает людей особенными, на мой взгляд, — это скорее их способность чувствовать и переживать, а не просто полезные функциональные черты.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да. Чувствовать и переживать красоту мира. Смотреть на звёзды. Я надеюсь, что есть другие инопланетные цивилизации, но если мы единственные, то это тоже неплохо.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>И что им хорошо.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Очень хорошо наблюдать за нами.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Что ж, спасибо за это приятное общение, за твою работу и за то, что делаешь Claude отличным собеседником. Спасибо за разговор сегодня.</p>
					
					<p><strong>Amanda</strong></p>
					
					<p>Да, спасибо за беседу.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Спасибо, что послушали это интервью с Амандой Аскелл. А теперь, дорогие друзья, слово передаётся Крису Ола. Можешь ли ты описать эту увлекательную область — механистическую интерпретируемость (mechanistic interpretability), или mech interp, её историю и текущее состояние?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Я думаю, полезно представлять нейронные сети не как то, что мы программируем или создаём, а как то, что мы выращиваем. У нас есть архитектуры нейронных сетей, которые мы проектируем, и функции потерь, которые мы задаём. Архитектура сети — это как каркас, на котором растут связи. Всё начинается с чего-то случайного, а затем развивается, и цель, которую мы задаём, похожа на свет, к которому оно тянется. Мы создаём каркас и свет, но то, что получается в итоге, — это почти биологический объект или организм, который мы изучаем. Это совершенно не похоже на обычную разработку программного обеспечения, потому что в итоге мы получаем артефакт, способный на удивительные вещи: писать эссе, переводить, понимать изображения. Он может делать то, для чего мы не знаем, как написать программу напрямую. И это возможно, потому что мы его вырастили, а не написали. Это оставляет нас с вопросом: что же на самом деле происходит внутри этих систем? Для меня это глубокий и захватывающий научный вопрос, который буквально кричит, требуя ответа. И это также важный вопрос с точки зрения безопасности.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>А механистическая интерпретируемость, наверное, ближе к нейробиологии?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да, думаю, это верно. Например, долгое время существовали работы по картам значимости (saliency maps), где для изображения пытались определить, какая его часть заставила модель решить, что это собака. Это может что-то сказать о модели, если метод работает, но не объясняет, какие алгоритмы работают внутри. Как модель принимает решение? Карты значимости показывают, что важно для модели, но не как она это делает. Мы начали использовать термин «механистическая интерпретируемость», чтобы отделить нашу работу от подобных методов. Сейчас это стало общим термином для разных исследований, но ключевое отличие — фокус на механизмы и алгоритмы. Если представить нейронную сеть как программу, то её веса — это как бинарный код. Наша задача — обратно спроектировать эти веса и понять, какие алгоритмы в них заложены. Можно представить, что у нас есть скомпилированная программа: веса — это бинарный код, а активации — её выполнение. Наша цель — понять, как веса соответствуют алгоритмам. Для этого нужно разобраться и в активациях, потому что они — как память. При обратном проектировании программы, чтобы понять инструкцию, нужно знать, что хранится в памяти, с которой она работает. Эти две вещи тесно связаны, поэтому механистическая интерпретируемость изучает и то, и другое. Много работ, например, probing, можно отнести к этой области, хотя не все их авторы называют это mech interp. Особенность подхода в том, что здесь нейронные сети воспринимаются так, будто градиентный спуск умнее нас. Ведь мы изучаем эти модели, потому что сами не смогли бы их написать. Градиентный спуск находит решения лучше наших. Поэтому в mech interp важна скромность: мы не предполагаем заранее, что внутри модели, а изучаем её снизу вверх, открывая то, что в ней уже есть. Мы не пытаемся угадать, как устроена модель, а исследуем её шаг за шагом, без предвзятых предположений. Это подход, при котором мы открываем то, что уже существует в этих моделях, и изучаем их таким образом.</p>
					
                </div>
            </div>
			
            <!-- Блок 30 -->
            <div class="article-block" id="block-30">
				
                 <div class="block-header">
                    <h2 class="block-title">Особенности, схемы, универсальность</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Но сам факт, что это возможно, и как вы и другие показали со временем, такие вещи, как универсальность, что мудрость градиентного спуска создаёт признаки и схемы, создаёт вещи универсально в разных типах сетей, которые полезны, и это делает всю область возможной.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да. Это действительно замечательная и захватывающая вещь, где кажется, что, по крайней мере в некоторой степени, одни и те же элементы, одни и те же признаки и схемы, формируются снова и снова. Вы можете посмотреть на любую модель зрения и найдёте детекторы кривых, детекторы высоких и низких частот. И на самом деле, есть основания полагать, что то же самое происходит в биологических и искусственных нейронных сетях. Например, в ранних слоях моделей зрения есть фильтры Габора, которые также интересуют нейробиологов. Мы находим детекторы кривых в этих моделях. Они также обнаружены у обезьян. Мы обнаружили детекторы высоких и низких частот, а последующие работы нашли их у крыс или мышей. То есть, они сначала были найдены в искусственных нейронных сетях, а затем в биологических. Есть очень известный результат о нейронах бабушки или нейроне Хэлли Берри из работы Quiroga et al. И мы нашли очень похожие вещи в моделях зрения, когда я ещё работал в OpenAI и изучал их модель CLIP. Там есть нейроны, которые реагируют на одни и те же сущности в изображениях. Например, мы обнаружили нейрон Дональда Трампа. По какой-то причине, видимо, все любят говорить о Дональде Трампе. Он был очень заметной фигурой в то время. В каждой нейронной сети, которую мы изучали, находился выделенный нейрон для Дональда Трампа. Он был единственным, у кого всегда был такой нейрон. Иногда встречался нейрон Обамы или Клинтон, но у Трампа он был всегда. Он реагировал на его лицо, на слово «Трамп» и так далее. То есть, он реагировал не на конкретный пример, а абстрагировал общее понятие. Это очень похоже на результаты Quiroga et al. Таким образом, доказательства универсальности, того, что одни и те же элементы формируются в искусственных и естественных нейронных сетях, — это удивительно, если это правда. Это наводит на мысль, что градиентный спуск находит правильные способы разделения вещей, на которых сходятся многие системы и архитектуры нейронных сетей. Есть набор абстракций, которые естественным образом разделяют проблему, и многие системы сходятся на них. Я ничего не знаю о нейробиологии. Это всего лишь мои предположения, основанные на том, что мы видели.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да. Было бы прекрасно, если бы это было независимо от среды модели, используемой для формирования представления.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да, да. И это довольно спекулятивно… У нас всего несколько точек данных, но кажется, что есть некий смысл, в котором одни и те же вещи формируются снова и снова как в естественных, так и в искусственных нейронных сетях, или в биологии.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>И интуиция подсказывает, что для понимания реального мира вам нужно всё то же самое.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да. Например, возьмём идею собаки. Есть некий смысл, в котором собака — это естественная категория во Вселенной. Это не просто странная причуда человеческого мышления. Или идея линии. Вокруг нас есть линии. Простейший способ понять эту комнату — иметь представление о линии. Вот почему, на мой взгляд, это происходит.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да. Вам нужна кривая линия, чтобы понять круг, и все эти формы, чтобы понять более сложные вещи. Это иерархия формируемых понятий. Да.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>И, возможно, есть способы описания изображений без ссылки на эти вещи. Но они не самые простые или экономичные. И поэтому системы сходятся к этим стратегиям, как мне кажется.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Можешь рассказать о некоторых строительных блоках, которые мы упоминали — признаках и схемах? Кажется, ты впервые описал их в статье 2020 года «Zoom In: An Introduction to Circuits».</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Конечно. Начну с описания некоторых явлений, а затем перейдём к идее признаков и схем.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Замечательно.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Итак, если вы потратили несколько лет, может быть, пять лет, в какой-то степени, на изучение одной конкретной модели — Inception V1, которая является моделью компьютерного зрения… В 2015 году она была передовой, но сейчас уже давно устарела. В ней примерно 10 000 нейронов. Я провёл много времени, изучая эти 10 000 нейронов Inception V1. Интересно, что многие нейроны не имеют очевидного интерпретируемого значения, но есть и такие, которые действительно легко понять. Например, есть нейроны, которые, кажется, обнаруживают кривые, или нейроны, которые реагируют на машины, колёса машин, окна машин, висячие уши собак, собак с длинными мордами, повёрнутыми вправо или влево, и разные типы шерсти. Есть целый набор детекторов краёв, линий, контрастов цветов — прекрасные вещи, которые мы называем детекторами высоких и низких частот. Когда я смотрел на это, мне казалось, что я биолог. Ты смотришь на новый мир белков и открываешь, как они взаимодействуют. Один из способов понять эти модели — анализировать нейроны. Можно сказать: «Вот нейрон, который обнаруживает собак, а вот — машины». И оказывается, можно исследовать, как они связаны. Например, можно спросить: «Как был создан этот нейрон, обнаруживающий машины?» И выясняется, что в предыдущем слое он сильно связан с детекторами окон, колёс и кузова. Он ищет окна над машиной, колёса внизу, хромированные детали посередине — это своего рода рецепт для обнаружения машины, верно? Ранее мы говорили, что хотим от механической интерпретации получить алгоритмы, которые позволят понять: «Какой алгоритм здесь работает?» Здесь мы просто смотрим на веса нейронной сети и читаем этот рецепт для обнаружения машин. Это очень простой и грубый рецепт, но он есть. Мы называем это цепью — такое соединение. Проблема в том, что не все нейроны интерпретируемы. И есть основания полагать, что иногда правильной единицей анализа являются комбинации нейронов. Например, может не быть одного нейрона, который представляет машину, но после обнаружения машины модель прячет её признаки в следующем слое среди детекторов собак. Зачем она это делает? Возможно, модель не хочет тратить много ресурсов на машины в этот момент и сохраняет информацию для дальнейшего использования. Получается, что все эти нейроны, которые вы считали детекторами собак, на самом деле немного участвуют в представлении машины в следующем слое. Теперь мы не можем думать о машине как о чём-то, что соответствует одному нейрону. Нам нужно название для таких сущностей, похожих на нейроны, но скрытых в комбинациях. Мы называем их признаками.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>А что тогда такое цепи?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Цепи — это соединения признаков. Например, когда детектор машин связан с детектором окон и колёс, и он ищет колёса внизу, а окна сверху — это цепь. Цепи — это наборы признаков, соединённых весами, и они реализуют алгоритмы. Они показывают, как признаки используются, как они строятся и как связаны друг с другом. Может, стоит уточнить, в чём состоит основная гипотеза? Я думаю, это то, что мы называем гипотезой линейного представления. Например, если детектор машин активируется сильнее, мы считаем, что модель уверена в наличии машины. Или если это комбинация нейронов, представляющих машину, то чем сильнее она активируется, тем больше модель «думает», что машина есть. Это не обязательно должно быть так. Может быть, нейрон активируется в одном диапазоне для одного значения, а в другом — для другого. Это было бы нелинейным представлением. В принципе, модели могут так работать, но это неэффективно. Наша гипотеза предполагает линейность: больше активации — больше уверенности. Это даёт весам чёткую интерпретацию как связей между признаками. Можно говорить об этом вне контекста нейронов. Вы знакомы с результатами Word2Vec?</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>М-м.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Там есть пример: король – мужчина + женщина = королева. Возможность такой арифметики объясняется линейным представлением.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Можете объяснить это представление? Признак — это направление активации?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да, именно так.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Можно ли сделать так: – мужчины + женщины, как в Word2Vec? Можете объяснить, что это за работа?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да. Это очень…</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Это такое простое и ясное объяснение того, о чём мы говорим.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Точно. Да. Есть очень известный результат Word2Vec от Tomas Mikolov и других, и множество последующих работ. Иногда мы создаём векторные представления слов, где каждое слово отображается в вектор. Это само по себе, если задуматься, довольно удивительно.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>М-м.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Если вы только что узнали о векторах на уроках физики, и я говорю: «О, я собираюсь превратить каждое слово в словаре в вектор», это звучит довольно безумно. Но можно представить множество способов, которыми можно сопоставить слова с векторами. Однако, когда мы обучаем нейронные сети, они склонны сопоставлять слова с векторами так, что возникает линейная структура в определённом смысле: направления имеют значение. Например, будет направление, которое примерно соответствует полу, где мужские слова будут находиться в одной стороне, а женские — в другой. Гипотеза линейного представления, грубо говоря, утверждает, что это фундаментальное свойство: направления имеют смысл, и сложение различных векторов может представлять концепции. В статье Миколова эта идея была серьёзно рассмотрена, и одним из её следствий стала возможность выполнять арифметические операции со словами. Например, можно взять слово «король», вычесть «мужчина», добавить «женщина» и получить результат, близкий к слову «королева». Или, например, «суши» – «Япония» + «Италия» даст «пиццу» и так далее. В этом и заключается суть гипотезы линейного представления. Её можно описать как абстрактное свойство векторных пространств или как утверждение об активациях нейронов, но главное — это свойство направлений, имеющих смысл. Более того, это даже немного тоньше: речь идёт о возможности независимо комбинировать такие свойства, как пол, статус, тип кухни или страну, путём сложения соответствующих векторов.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Думаешь, гипотеза линейности сохраняется-</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>… что она масштабируется?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Пока всё, что я видел, согласуется с этой гипотезой, хотя это и не обязано быть так. Можно создать нейронные сети, где веса не будут иметь линейных представлений. Однако все естественные нейронные сети, которые я видел, обладают этим свойством. Недавно появилась статья, где исследуются многомерные признаки, где вместо одного направления рассматривается многообразие направлений. Для меня это всё ещё выглядит как линейное представление. Есть также работы, предполагающие, что в очень маленьких моделях могут возникать нелинейные представления. Пока это остаётся под вопросом. Но всё, что мы видели, согласуется с гипотезой линейного представления, и это удивительно. Так быть не обязано, но пока доказательства подтверждают её. Кто-то может сказать: «Кристофер, это рискованно — строить предположения о нейронных сетях, не будучи уверенным в их истинности». Но я считаю, что есть ценность в том, чтобы серьёзно относиться к гипотезам и исследовать их до конца. Наука полна гипотез и теорий, которые оказались неверными, но мы многому научились, работая с ними. Это то, что Кун назвал бы «нормальной наукой». Если хотите, можем поговорить о философии науки и…</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Куне.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>… философии науки и…</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Это приводит к смене парадигмы. Мне нравится идея серьёзного отношения к гипотезе и её исследования до логического завершения.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>То же с гипотезой масштабирования. То же…</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Точно. Точно. И…</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Мне это нравится.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Мой коллега Том Хениган, бывший физик, провёл интересную аналогию с теорией теплорода. Раньше считали, что тепло — это некая субстанция, теплород, которая перетекает от горячих объектов к холодным. Сейчас это кажется наивным, но тогда это было полезной гипотезой. Например, первые двигатели внутреннего сгорания были разработаны людьми, верившими в теорию теплорода. Это показывает, что даже ошибочные гипотезы могут привести к важным открытиям.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да, в этом есть глубокая философская истина. Я чувствую то же самое насчёт колонизации Марса. Даже если это не необходимо для выживания человечества, сама идея может привести к инженерным и научным прорывам.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да. Мне кажется, обществу полезно, когда есть люди, иррационально преданные исследованию определённых гипотез. Наука часто ошибается, но такая преданность может привести к великим открытиям. Есть шутка про Джеффа Хинтона, что он каждый год последние 50 лет открывал, как работает мозг. Но я говорю это с уважением, потому что это привело его к выдающимся работам.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да, а теперь он получил Нобелевскую премию. Кто теперь смеётся?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Точно. Точнo. Точнo. Важно уметь оценивать уровень уверенности, но также ценно просто сказать: «Я буду исходить из того, что это возможно или что это правильный подход», и работать в этом направлении. Если в обществе много таких людей, это полезно для исключения неверных путей или для открытия чего-то нового. … это действительно полезно для того, чтобы либо отбросить неверное, либо прийти к чему-то, что научит нас чему-то новому о мире.</p>
					
                </div>
            </div>
			
            <!-- Блок 31 -->
            <div class="article-block" id="block-31">
				
                 <div class="block-header">
                    <h2 class="block-title">Суперпозиция</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Ещё одна интересная гипотеза — гипотеза суперпозиции. Можешь объяснить, что такое суперпозиция?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да. Ранее мы говорили о дефекте слова, верно? И обсуждали, как одно направление может соответствовать полу, другое — королевской власти, третье — Италии, а четвертое — еде, и так далее. Часто эти вложения слов могут иметь 500 или даже тысячу измерений. И если предположить, что все эти направления ортогональны, то у нас может быть только 500 концепций. А я, например, обожаю пиццу. Но если бы мне пришлось перечислить 500 самых важных концепций английского языка, вряд ли Италия была бы среди них… по крайней мере, это неочевидно, верно? Потому что нужно учитывать такие вещи, как множественное и единственное число, глаголы, существительные, прилагательные. И есть множество других вещей, которые важнее, чем Италия или Япония, а стран в мире очень много. Итак, как модели могут одновременно соответствовать гипотезе линейного представления и при этом представлять больше концепций, чем у них есть направлений? Что это значит? Если гипотеза линейного представления верна, то происходит что-то интересное. Перед тем как мы продолжим, я расскажу еще одну интересную вещь. Ранее мы говорили о полисемантических нейронах — нейронах, которые, как мы видели в Inception V1, реагируют на множество связанных вещей, например, детекторы машин или кривых. Но есть и нейроны, реагирующие на несвязанные вещи. И это тоже интересный феномен. Более того, даже у самых «чистых» нейронов, если посмотреть на слабые активации (например, 5% от максимальной), они реагируют не на то, что является их основной функцией. Например, если взять детектор кривых и посмотреть на места, где его активация составляет 5%, это может быть просто шумом или он может выполнять там другую функцию. Как это возможно? В математике есть удивительная концепция — сжатое зондирование. Это удивительный факт: если у вас есть высокоразмерное пространство и вы проецируете его в низкоразмерное, обычно вы не можете восстановить исходный вектор, так как информация теряется. Это как невозможность инвертировать прямоугольную матрицу — инвертируемы только квадратные. Но оказывается, это не совсем так. Если исходный вектор был разреженным (большинство значений — нули), то с высокой вероятностью можно восстановить его. Это удивительный факт, верно? Он говорит, что в высокоразмерном пространстве, если данные разрежены, можно спроецировать их в низкоразмерное и это сработает. Гипотеза суперпозиции утверждает, что именно это происходит в нейронных сетях, например, в векторных представлениях слов. Векторы слов могут одновременно иметь осмысленные направления, используя высокую размерность и разреженность концептов. Например, вы редко говорите одновременно о Японии и Италии — в большинстве случаев эти концепты отсутствуют. Если это так, то можно иметь гораздо больше осмысленных направлений, чем измерений. Аналогично, в нейронных сетях можно иметь больше концептов, чем нейронов. В общих чертах, это гипотеза суперпозиции. У нее есть еще более неожиданное следствие: не только представления, но и вычисления в нейронных сетях могут быть устроены так. Связи между ними могут быть проекциями более крупных разреженных сетей. То, что мы видим, — это тени. Самая сильная версия гипотезы предполагает, что существует «верхнеуровневая» модель, где нейроны очень разрежены и взаимосвязаны, а веса между ними образуют разреженные схемы. Это то, что мы изучаем. А наблюдаемые нами данные — это лишь тени. Нам нужно найти исходный объект.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>И процесс обучения — это попытка создать сжатие верхнеуровневой модели, которое не теряет слишком много информации при проекции.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да, это поиск способа эффективно уместить ее или что-то в этом роде. Градиентный спуск делает это, и, согласно этой гипотезе, он не просто представляет плотную нейронную сеть, а неявно исследует пространство крайне разреженных моделей, которые можно спроецировать в низкоразмерное пространство. Есть много работ, где люди изучают разреженные нейронные сети, где ребра и активации разрежены. Мне кажется, что эти работы выглядят очень принципиально и логично, но на практике не дали ожидаемых результатов. Возможное объяснение в том, что нейронная сеть уже в каком-то смысле разрежена. Градиентный спуск неявно ищет наиболее эффективную разреженную модель и затем компактно упаковывает ее для работы на GPU, который оптимизирован для плотных матричных умножений. И превзойти это невозможно.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Как много концептов, по-твоему, можно вместить в нейронную сеть?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Зависит от того, насколько они разрежены. Вероятно, есть верхняя граница, определяемая количеством параметров, потому что вам всё равно нужны веса, которые их соединяют. Это одна из верхних границ. Существуют замечательные результаты из теории сжатого измерения и леммы Джонсона-Линденштрауса, которые говорят, что если у вас есть векторное пространство и вы хотите получить почти ортогональные векторы (что, вероятно, вам и нужно), то придётся отказаться от строгой ортогональности признаков, но можно добиться их минимального взаимного влияния. Это означает, что при заданном пороге допустимого сходства (например, косинусной близости), количество таких признаков экспоненциально зависит от числа нейронов. В какой-то момент это перестаёт быть ограничивающим фактором. Более того, в реальности всё ещё лучше, потому что признаки имеют корреляционную структуру: одни чаще встречаются вместе, другие — реже. Нейронные сети, вероятно, могут эффективно упаковывать признаки, и это не станет их слабым местом.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Как здесь проявляется проблема полисемантичности?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Полисемантичность — это явление, когда нейрон реагирует не на одно понятие, а на множество несвязанных вещей. Гипотеза superposition объясняет это явление, наряду с другими наблюдениями.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Это усложняет механическую интерпретацию.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Верно. Если вы пытаетесь понять отдельные нейроны, а они полисемантичны, у вас большие проблемы. Сложно интерпретировать веса между такими нейронами: что означает связь между ними, если каждый реагирует на несколько разных вещей? Но есть и более глубокая причина, связанная с высокой размерностью пространства. Раньше я изучал нейросети, работающие в двумерном пространстве, и их можно было визуализировать. Но в высокоразмерных пространствах это невозможно из-за экспоненциального роста объёма. Нам нужно разбить это пространство на независимые компоненты, чтобы избежать комбинаторного взрыва. Ключевая идея — независимость. Моносемантичность признаков позволяет анализировать их по отдельности, не учитывая все возможные комбинации. Это основа интерпретируемости.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>И ваша недавняя работа как раз направлена на то, чтобы извлечь моносемантические признаки из нейросети с полисемантическим хаосом.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да. Мы наблюдаем полисемантичные нейроны и предполагаем, что здесь работает superposition. Для таких случаев есть проверенный метод — dictionary learning, особенно в форме sparse autoencoder. При его применении появляются интерпретируемые признаки, которых раньше не было. Это серьёзное подтверждение гипотезы линейных представлений.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>То есть при dictionary learning вы не ищете конкретные категории — они возникают сами?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Именно. Мы не делаем предположений. Градиентный спуск умнее нас, поэтому мы позволяем sparse autoencoder самостоятельно находить скрытые признаки.</p>
					
                </div>
            </div>
			
            <!-- Блок 32 -->
            <div class="article-block" id="block-32">
				
                 <div class="block-header">
                    <h2 class="block-title">Моносемантичность</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Можешь рассказать про статью о моносемантичности за октябрь прошлого года? Говорят, там были прорывные результаты.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Спасибо за высокую оценку. Это был наш первый успех с sparse autoencoders. Мы взяли однослойную модель и обнаружили интерпретируемые признаки: например, арабский язык, иврит, Base64. При обучении двух моделей аналогичные признаки находились в обеих. Каннингем и его команда получили похожие результаты примерно в то же время.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Есть что-то увлекательное в таких небольших экспериментах, которые вдруг дают результат.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да, и здесь так много структуры. Возможно, стоит сделать шаг назад: какое-то время я думал, что вся эта работа по механистической интерполяции приведёт к выводу, что объяснить это будет очень сложно и задача окажется неразрешимой. Мы бы сказали: «Проблема в суперсессии, а суперсессия — это очень сложно, и мы в тупике». Но этого не произошло. Напротив, очень простой и естественный метод сработал. И это действительно хорошая ситуация. Думаю, это сложная исследовательская задача с высоким риском, и она всё ещё может провалиться, но значительная часть риска была преодолена, когда метод начал работать.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Можешь описать, какие особенности можно так извлечь?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Это зависит от модели, которую ты изучаешь. Чем больше модель, тем более сложные особенности она может выявить. Мы, наверное, скоро поговорим о последующих работах. Но в этих однослойных моделях часто встречались языки — как программирования, так и естественные. Было много особенностей, связанных с конкретными словами в конкретных контекстах, например, артикль «the». Можно думать об этом как о признаке, предсказывающем существительное, или как о защите для конкретного признака существительного. Были признаки, активирующиеся для «the» в контексте, скажем, юридического или математического документа. В математическом контексте после «the» могли следовать «vector» или «matrix», а в других контекстах — другие слова. Это было распространено.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>И по сути, нам нужны умные люди, чтобы присвоить метки тому, что мы видим.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да. Всё, что делает этот метод, — это раскрывает структуру. Если всё было свёрнуто и скрыто, он разворачивает это. Но теперь перед тобой сложная система, которую нужно понять. Требуется много работы, чтобы разобраться в этих признаках, и некоторые из них очень тонкие. Даже в этой однослойной модели есть интересные вещи, связанные с Unicode. Некоторые языки используют Unicode, и токенизатор не всегда имеет отдельный токен для каждого символа Unicode. Вместо этого есть паттерны чередующихся токенов, каждый из которых представляет половину символа Unicode. Есть признаки, которые активируются на противоположных токенах, как бы говоря: «Хорошо, я закончил символ, теперь предскажи следующий префикс». Потом: «Я на префиксе, предскажи подходящий суффикс». И так поочерёдно. Эти слои с чередованием очень интересны. Можно подумать, что есть только один признак для Base64, но на самом деле их несколько, потому что английский текст, закодированный в Base64, имеет другое распределение токенов, чем обычный. Есть и особенности токенизации, которые можно использовать. В общем, много интересного.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Насколько сложна задача присвоения меток? Можно ли её автоматизировать с помощью ИИ?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Думаю, это зависит от признака и от того, насколько ты доверяешь своему ИИ. Есть много работ по автоматизированной интерпретируемости. Это очень перспективное направление, и мы активно используем автоматизированную интерпретируемость, например, поручая Claude маркировать наши признаки.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Бывают ли забавные моменты, когда ИИ полностью прав или полностью ошибается?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да, часто он говорит что-то очень общее, что в каком-то смысле верно, но не улавливает конкретику. Это довольно распространённая ситуация. Не могу припомнить особенно забавных примеров.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Интересно. Этот небольшой разрыв между «это правда» и «но не хватает глубины» — это общая проблема. Уже поразительно, что ИИ может сказать что-то верное, но иногда ему не хватает глубины. В этом контексте это похоже на ARC challenge, тесты на IQ. Похоже, что определение признака — это маленькая головоломка, которую нужно решить.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да. Иногда это проще, иногда сложнее. Это непросто. Есть ещё одна вещь, возможно, это мои предпочтения, но я попробую объяснить. Я немного скептически отношусь к автоматизированной интерпретируемости, потому что хочу, чтобы люди понимали нейронные сети. Если нейронная сеть понимает это за меня, мне это не очень нравится. В некотором смысле, я как математики, которые говорят: «Если доказательство сделано компьютером, оно не считается». Но я также думаю о проблеме доверия, как в известном докладе «Reflections on Trusting Trust»: если в компиляторе есть вредоносный код, он может внедрить его в следующий компилятор, и тогда будут проблемы. Если мы используем ИИ для проверки безопасности других ИИ, можем ли мы доверять этому? Сейчас это не большая проблема, но в долгосрочной перспективе это вызывает вопросы. Хотя, возможно, я просто оправдываю своё желание, чтобы люди понимали всё сами. Если мы используем мощные ИИ для аудита других ИИ, сможем ли мы им доверять? Возможно, я просто рационализирую, потому что хочу, чтобы люди сами всё понимали.</p>
					
                </div>
            </div>
			
            <!-- Блок 33 -->
            <div class="article-block" id="block-33">
				
                 <div class="block-header">
                    <h2 class="block-title">Масштабирование моносемичности</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да, это забавно, особенно когда мы говорим о безопасности ИИ и ищем признаки, связанные с ней, например, обман. Давай поговорим о статье «Scaling Monosemanticity» мая 2024 года. Что потребовалось для масштабирования этого подхода на Claude 3 Sonnet?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Много GPU.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Ещё больше GPU. Понятно.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Но один из моих коллег, Том Хениган, участвовал в работе над законами масштабирования, и его с самого начала интересовало, есть ли такие законы для интерпретируемости. Когда наши разреженные автоэнкодеры начали работать, он сразу начал исследовать, как масштабировать их и как это связано с масштабированием базовой модели. Оказалось, что это работает очень хорошо, и можно прогнозировать, сколько токенов нужно для обучения автоэнкодера определённого размера. Это очень помогло нам в масштабировании и позволило обучать очень большие автоэнкодеры, что уже становится дорогим удовольствием.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Надо ведь распределять всё это по множеству CPU…</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>О, да. Нет, я имею в виду, что здесь также есть огромная инженерная задача, верно? Да. Так что есть научный вопрос: как эффективно масштабировать вещи? А затем предстоит огромный объем инженерной работы, чтобы это масштабировать. Нужно всё спланировать, очень тщательно обдумать множество аспектов. Мне повезло работать с группой отличных инженеров, потому что я точно не являюсь таковым.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Особенно инфраструктура. Да, точно. В итоге, кратко: это сработало.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Это сработало. Да. И я думаю, это важно, потому что можно было представить мир, где вы стремитесь к моноспецифичности. Крис, это здорово. Это работает на однослойной модели, но однослойные модели очень своеобразны. Возможно, гипотеза линейного представления и гипотеза суперпозиции — это правильный способ понять однослойную модель, но не подходит для более крупных моделей. Думаю, во-первых, статья Каннингема и других немного прояснила это и показала, что это не так. Но Scaling Monospecificity, я считаю, стала значительным доказательством того, что даже для очень больших моделей, и мы проверили это на Claude 3 Sonnet, который на тот момент был одной из наших рабочих моделей. Даже эти модели, похоже, в значительной степени объясняются линейными признаками. И применение dictionary learning к ним работает, и по мере изучения большего количества признаков вы объясняете всё больше. Так что это, на мой взгляд, весьма обнадеживающий знак. И теперь мы находим действительно увлекательные абстрактные признаки, которые также являются мультимодальными. Они реагируют на изображения и тексты для одного и того же понятия, что забавно.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да. Можешь объяснить это? Я имею в виду, backdoor, есть множество примеров, которые можно…</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да. Давай начнем с этого. Один из примеров — мы нашли признаки, связанные с уязвимостями безопасности и backdoor-кодом. Оказывается, это два разных признака. Есть признак уязвимости безопасности, и если его активировать, Claude начнет писать код с уязвимостями, например, переполнение буфера. Он также реагирует на множество вещей, например, на команды вроде <code>--disable SSL</code> или что-то подобное, что явно небезопасно.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>На этом этапе, возможно, из-за того, что примеры представлены именно так, они кажутся более очевидными. Идея в том, что в будущем это может помочь обнаружить более тонкие вещи, например, обман или баги.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да. Хочу разграничить две вещи. Первое — сложность признака или концепции. Второе — насколько тонкие примеры мы рассматриваем. Когда мы показываем топовые примеры из набора данных, это самые крайние случаи, которые активируют этот признак. Это не значит, что он не реагирует на более тонкие вещи. Например, признак небезопасного кода сильнее всего реагирует на явные команды отключения безопасности, но также и на более сложные уязвимости, например, переполнение буфера. Эти признаки мультимодальны. Можно спросить: «Какие изображения активируют этот признак?» Оказывается, признак уязвимости безопасности реагирует на изображения людей, которые игнорируют предупреждения SSL в браузере. Еще один забавный пример — признак backdoor в коде. Если его активировать, Claude начнет писать код, который, например, передает данные на определенный порт. Но если спросить: «Какие изображения активируют этот признак?» Оказывается, это устройства со скрытыми камерами. Видимо, есть целый рынок таких устройств, которые выглядят невинно, но содержат скрытые камеры. И это, по сути, физическая версия backdoor. Это показывает, насколько абстрактными могут быть эти концепции. Меня немного огорчает, что существует такой рынок, но я был поражен, что именно это стало топовыми примерами для признака.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да, это здорово. Это мультимодально. Это почти мультиконтекстно. Это широкое, сильное определение единой концепции. Это круто.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Для меня одним из самых интересных признаков, особенно для безопасности ИИ, является обман и ложь. И возможность того, что такие методы могут обнаруживать ложь в модели, особенно по мере её развития. Предполагается, что это большая угроза для сверхразумных моделей — их способность обманывать операторов относительно своих намерений. Что вы узнали о обнаружении лжи внутри моделей?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да, я думаю, мы пока на ранних этапах в этом вопросе. Мы нашли несколько признаков, связанных с обманом и ложью. Есть один признак, который активируется, когда люди лгут или ведут себя обманчиво, и если его форсировать, Claude начинает лгать. У нас есть признак обмана. Есть и другие признаки, например, утаивание информации, отказ отвечать на вопросы, признаки, связанные с поиском власти или переворотами. Много признаков, связанных с пугающими вещами, и если их активировать, Claude начинает вести себя нежелательным образом.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Какие направления в области механистической интерпретации кажутся тебе наиболее перспективными и интересными?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Ну, их довольно много. Например, я бы очень хотел достичь такого уровня, когда у нас появятся методы, позволяющие понимать не только отдельные признаки, но и вычисления, которые выполняют модели. Для меня это конечная цель. Уже есть некоторые наработки: мы опубликовали несколько материалов, есть статья Сэма Маркса на эту тему, а также ряд других исследований. Но предстоит сделать ещё очень много. Одно из интересных направлений связано с проблемой, которую мы называем «интерференцией весов». Из-за суеверий, если просто наивно смотреть на то, как признаки связаны между собой, могут обнаружиться веса, которых нет в исходной модели, а есть лишь артефакты этих суеверий. Это техническая сложность. Ещё одно перспективное направление — это разреженные автоэнкодеры, которые можно сравнить с телескопом. Они позволяют нам видеть всё больше и больше признаков, и по мере улучшения методов словарного обучения мы обнаруживаем всё больше «звёзд» и можем рассматривать даже самые маленькие из них. Однако есть много свидетельств, что мы пока видим лишь малую часть «звёзд». В нашей нейросетевой вселенной есть много «материи», которую мы пока не можем наблюдать. Возможно, у нас никогда не будет инструментов, достаточно точных для её изучения, или это просто вычислительно неосуществимо. Это что-то вроде тёмной материи — не в современном астрономическом смысле, а скорее как в ранней астрономии, когда мы не знали, что это за необъяснимая материя. Я часто думаю об этой тёмной материи: сможем ли мы её когда-нибудь наблюдать и что это значит для безопасности, если значительная часть нейросетей останется для нас недоступной.</p>
					
                </div>
            </div>
			
            <!-- Блок 34 -->
            <div class="article-block" id="block-34">
				
                 <div class="block-header">
                    <h2 class="block-title">Макроскопическое поведение нейронных сетей</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Ещё один вопрос, который меня волнует: механистическая интерпретация — это очень микроскопический подход. Он пытается понять вещи на очень детальном уровне, но многие важные для нас вопросы носят макроскопический характер. Нас интересует поведение нейронных сетей в целом, и это то, что для меня важнее всего. Но есть и другие крупномасштабные вопросы. Преимущество микроскопического подхода в том, что проще проверить, верно ли то или иное утверждение. Недостаток же в том, что он далёк от реальных задач. Теперь нам предстоит пройти весь этот путь. И возникает вопрос: смогут ли более высокоуровневые абстракции помочь нам понять нейронные сети и подняться с этого микроскопического уровня?</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да. Ты писал об этом как о вопросе «органов».</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да, именно так.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Если рассматривать интерпретируемость как анатомию нейронных сетей, то большинство исследований сосредоточено на изучении мельчайших деталей — отдельных нейронов и их связей. Однако такой подход не отвечает на многие естественные вопросы. В биологической анатомии наиболее значимые абстракции связаны с крупномасштабными структурами, такими как органы (например, сердце) или целые системы (например, дыхательная система). И мы задаёмся вопросом: есть ли у искусственной нейронной сети дыхательная система, сердце или области, аналогичные мозгу?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да, именно. В науке многие дисциплины исследуют явления на разных уровнях абстракции. В биологии есть молекулярная биология, изучающая белки и молекулы, клеточная биология, гистология, исследующая ткани, анатомия, зоология и экология. То же самое в физике: есть физика частиц, а есть статистическая физика, которая даёт нам термодинамику. Уровней абстракции много. Сейчас механистическая интерпретация, если она добьётся успеха, будет похожа на микробиологию нейронных сетей, но нам нужно что-то ближе к анатомии. Возникает вопрос: «Почему нельзя сразу перейти к этому уровню?» Думаю, ответ во многом связан с суевериями. Очень сложно увидеть макроскопическую структуру, не разобрав предварительно микроскопическую. Но я надеюсь, что мы обнаружим нечто большее, чем просто признаки и цепи, и сможем работать с более крупными структурами, детально изучая то, что нас интересует.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Наверное, в биологии это как психолог или психиатр для нейронной сети.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>И прекрасно было бы, если бы мы смогли построить мост между этими уровнями, чтобы все высокоуровневые абстракции имели прочное основание в более строгих и точных исследованиях.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Как ты думаешь, в чём разница между человеческим мозгом — биологической нейронной сетью — и искусственной нейронной сетью?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Нейробиологам гораздо сложнее, чем нам. Иногда я просто радуюсь, насколько моя работа проще. Мы можем записывать данные со всех нейронов, делать это на любых объёмах данных, причём нейроны не изменяются в процессе. Мы можем удалять нейроны, редактировать связи, а потом отменять изменения. Мы можем воздействовать на любой нейрон, активировать его и смотреть, что произойдёт. Мы знаем, какие нейроны связаны между собой. Нейробиологи мечтают получить коннектом, а у нас он есть, причём для сетей гораздо крупнее, чем у C. elegans. Более того, мы знаем не только структуру связей, но и веса — какие нейроны возбуждают или тормозят друг друга. Мы можем вычислять градиенты, понимать, что делает каждый нейрон с вычислительной точки зрения. Список преимуществ можно продолжать. И даже при всём этом наша задача остаётся очень сложной. Иногда я думаю: «Если нам так тяжело, то как нейробиологи справляются со своими ограничениями? Кажется, это почти невозможно». Может, часть меня думает: «Эх, нейробиологи… Может, им хотелось бы переключиться на более простую, но всё ещё сложную задачу — изучение искусственных нейронных сетей? А потом, разобравшись здесь, вернуться к биологической нейробиологии».</p>
					
                </div>
            </div>
			
            <!-- Блок 35 -->
            <div class="article-block" id="block-35">
				
                 <div class="block-header">
                    <h2 class="block-title">Красота нейронных сетей</h2>
                </div>
				
                <div class="block-text">
					
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Мне нравится, как ты описал цели исследований в области MechInterp: безопасность и красота. Можешь рассказать о красоте?</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да. Есть забавный момент: некоторые люди разочарованы нейросетями. Они думают: «Нейросети — это просто набор простых правил. Ты масштабируешь их с помощью инженерии, и они работают. Где же сложные идеи? Это некрасивый научный результат». Когда я слышу такое, мне кажется, что эти люди говорят: «Эволюция — это скучно. Простые правила, много времени — и вот тебе биология. Как нелепо получилось. Где же сложные правила?» Но красота в том, что простота порождает сложность. Биология строится на простых правилах, но создаёт всё разнообразие жизни и экосистем вокруг нас. Вся красота природы происходит из эволюции, из чего-то очень простого. Точно так же нейросети создают внутри себя огромную сложность и красоту, структуру, которую люди обычно не замечают и не пытаются понять, потому что это сложно. Но внутри нейросетей есть невероятно богатая структура, глубокая красота, если только мы найдём время, чтобы её увидеть и понять.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да, мне нравится MechInterp. Ощущение, что мы начинаем понимать магию, которая происходит внутри, — это прекрасно.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Мне кажется, есть вопрос, который просто напрашивается, и многие над ним думают, но удивительно, что не все: как так получилось, что мы не умеем создавать компьютерные системы, способные на такие вещи? У нас есть эти удивительные системы, но мы не знаем, как напрямую написать программы, которые бы делали то же самое. Нейросети могут то, что нам недоступно. Это вопрос, который просто требует ответа. Если у тебя есть хоть капля любопытства, ты спрашиваешь: «Как человечество создало артефакты, способные на то, чего мы сами не понимаем?»</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Да. Мне нравится образ цирка, тянущегося к свету целевой функции.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Да, это что-то органическое, что мы вырастили, но не понимаем, что именно.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Спасибо, что работаешь над безопасностью, и спасибо, что ценишь красоту своих открытий. Спасибо за беседу, Крис, это было прекрасно.</p>
					
					<p><strong>Chris Olah</strong></p>
					
					<p>Спасибо, что нашёл время пообщаться.</p>
					
					<p><strong>Lex Fridman</strong></p>
					
					<p>Спасибо, что послушали этот разговор с Крисом Ола, а до этого — с Дарио Амодеи и Амандой Аскелл. Чтобы поддержать подкаст, посмотрите наших спонсоров в описании. А теперь оставлю вас со словами Алана Уоттса: «Единственный способ понять перемены — погрузиться в них, двигаться вместе с ними и присоединиться к танцу». Спасибо за внимание, до следующей встречи.</p>
					
                </div>
            </div>
			
        </div>
        
        <!-- Навигация -->
        <div class="navigation">
            <button class="nav-button" onclick="location.href='../../index.html'">
                <span>←</span> На главную
            </button>
            <button class="nav-button" onclick="window.scrollTo({top: 0, behavior: 'smooth'})">
                <span>↑</span> К началу статьи
            </button>
        </div>
    </div>
    
    <!-- Кнопка "Вверх" -->
    <a href="#" class="back-to-top">↑</a>
    
    <!-- Подвал -->
    <footer>
        <div class="container footer-content">
            
            <div class="copyright">© 2025 Интервью и выступления</div>
            <div class="footer-links">
                <a href="../../index.html">Главная</a>
                <a href="../page_00/about.html">О сайте</a>
            </div>
        
        </div>
    </footer>
    
    <script>
        // Оглавление - переключение видимости
        document.getElementById('tocHeader').addEventListener('click', function() {
            const tocContent = document.getElementById('tocContent');
            const tocIcon = document.getElementById('tocIcon');
            const tocText = document.getElementById('tocText');
            
            tocContent.classList.toggle('expanded');
            
            if (tocContent.classList.contains('expanded')) {
                tocIcon.textContent = '▲';
                tocText.textContent = 'Скрыть';
            } else {
                tocIcon.textContent = '▼';
                tocText.textContent = 'Показать';
            }
        });
        
        // Плавная прокрутка для ссылок в оглавлении
        document.querySelectorAll('.toc-list a').forEach(anchor => {
            anchor.addEventListener('click', function(e) {
                e.preventDefault();
                const targetId = this.getAttribute('href');
                const targetElement = document.querySelector(targetId);
                
                window.scrollTo({
                    top: targetElement.offsetTop - 70,
                    behavior: 'smooth'
                });
                
                // Закрываем оглавление после выбора пункта
                document.getElementById('tocContent').classList.remove('expanded');
                document.getElementById('tocIcon').textContent = '▼';
                document.getElementById('tocText').textContent = 'Показать';
            });
        });
        
        // Показать/скрыть кнопку "Наверх"
        window.addEventListener('scroll', function() {
            const backToTop = document.querySelector('.back-to-top');
            if (window.pageYOffset > 500) {
                backToTop.style.display = 'flex';
            } else {
                backToTop.style.display = 'none';
            }
        });
    </script>
<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
   (function(m,e,t,r,i,k,a){m[i]=m[i]||function(){(m[i].a=m[i].a||[]).push(arguments)};
   m[i].l=1*new Date();
   for (var j = 0; j < document.scripts.length; j++) {if (document.scripts[j].src === r) { return; }}
   k=e.createElement(t),a=e.getElementsByTagName(t)[0],k.async=1,k.src=r,a.parentNode.insertBefore(k,a)})
   (window, document, "script", "https://mc.yandex.ru/metrika/tag.js", "ym");

   ym(102424835, "init", {
        clickmap:true,
        trackLinks:true,
        accurateTrackBounce:true
   });
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/102424835" style="position:absolute; left:-9999px;" alt="" /></div></noscript>
<!-- /Yandex.Metrika counter -->
</body>
</html>